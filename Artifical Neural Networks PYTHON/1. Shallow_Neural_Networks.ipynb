{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron neural network\n",
    "\n",
    " * A perceptron is the simplest form of a neural network, specifically a single-layer neural network. \n",
    " * It was introduced by Frank Rosenblatt in 1957. \n",
    " * The perceptron is a binary classifier that takes multiple binary inputs and produces a single binary output.  \n",
    " * It's a fundamental building block for more complex neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "# Extract petal length and petal width features from the dataset\n",
    "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
    "# Create binary classification labels: 1 if Iris setosa, 0 otherwise\n",
    "y = (iris.target == 0).astype(int)\n",
    "\n",
    "# Create a Perceptron classifier with specified parameters\n",
    "## max_iter is the maximum number of training iterations.\n",
    "## tol is the tolerance, stopping training when the improvement is small.\n",
    "## random_state ensures reproducibility by setting a random seed.\n",
    "per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
    "# Train the Perceptron on the dataset\n",
    "per_clf.fit(X, y)\n",
    "# Make a prediction for a new input [2, 0.5]\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an image classifire using the Sequential API in Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version is  2.10.0\n",
      "Keras version is  2.10.0\n",
      "The training set contains 60000 grayscale images, each 28*28 pixels\n",
      "The first image in the training set is a Coat\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.7190 - accuracy: 0.7645 - val_loss: 0.4898 - val_accuracy: 0.8344\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4878 - accuracy: 0.8291 - val_loss: 0.4685 - val_accuracy: 0.8408\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4439 - accuracy: 0.8444 - val_loss: 0.4209 - val_accuracy: 0.8582\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4169 - accuracy: 0.8532 - val_loss: 0.4106 - val_accuracy: 0.8586\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3957 - accuracy: 0.8601 - val_loss: 0.3827 - val_accuracy: 0.8700\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3808 - accuracy: 0.8654 - val_loss: 0.4178 - val_accuracy: 0.8486\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3682 - accuracy: 0.8715 - val_loss: 0.3814 - val_accuracy: 0.8656\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3565 - accuracy: 0.8738 - val_loss: 0.3615 - val_accuracy: 0.8720\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3462 - accuracy: 0.8773 - val_loss: 0.3442 - val_accuracy: 0.8774\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3351 - accuracy: 0.8811 - val_loss: 0.3428 - val_accuracy: 0.8786\n",
      "{'verbose': 1, 'epochs': 10, 'steps': 1719}\n",
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3782 - accuracy: 0.8637\n",
      "1/1 [==============================] - 0s 57ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26000\\158248012.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;31m# Predict the class with the highest probability for each instance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_proba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The predicted classes are \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# First, let's import TensorFlow and Keras.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print TensorFlow and Keras versions.\n",
    "print('TensorFlow version is ', tf.__version__)\n",
    "print('Keras version is ', keras.__version__)\n",
    "\n",
    "# Loading the fashion MNIST dataset.\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Get the shape of the training set.\n",
    "(m, n1, n2) = X_train_full.shape\n",
    "print(f\"The training set contains {m} grayscale images, each {n1}*{n2} pixels\")\n",
    "\n",
    "# Split the training set into validation and training sets, and normalize pixel values.\n",
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Normalize pixel values in the test set.\n",
    "X_test = X_test / 255.\n",
    "\n",
    "# # plot an image using Matplotlib's `imshow()` function, with a 'binary' color map\n",
    "# plt.imshow(X_train[0], cmap=\"binary\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# Defining the corresponding class names\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "# Printing the class name of the first image in the training set\n",
    "print(f\"The first image in the training set is a {class_names[y_train[0]]}\")\n",
    "\n",
    "# Creating a Sequential model\n",
    "model = keras.models.Sequential()\n",
    "# Adding a Flatten layer to reshape the input images into a 1D array\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "# Adding a Dense layer with 300 neurons and ReLU activation function\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "# Adding another Dense layer with 100 neurons and ReLU activation function\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "# Adding the output layer with 10 neurons (for each class) and softmax activation function\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# # Instead of adding the layers one by one as we just did, you can pass a list\n",
    "# # of layers when creating the Sequential model\n",
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.Flatten(input_shape=[28, 28]),\n",
    "#     keras.layers.Dense(300, activation=\"relu\"),\n",
    "#     keras.layers.Dense(100, activation=\"relu\"),\n",
    "#     keras.layers.Dense(10, activation=\"softmax\")\n",
    "# ])\n",
    "\n",
    "# Displaying a summary of the model architecture, including parameters and output shapes\n",
    "model.summary()\n",
    "\n",
    "# Plotting the model architecture and saving it as an image\n",
    "# keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)\n",
    "\n",
    "# Accessing information about the layers in the model\n",
    "model.layers\n",
    "hidden1 = model.layers[1]# Retrieving information about the first hidden layer\n",
    "hidden1.name# Retrieving the name of the first hidden layer\n",
    "\n",
    "# Retrieving the weights and biases from the first hidden layer\n",
    "weights, biases = hidden1.get_weights()\n",
    "weights.shape# Displaying the shape of the weights\n",
    "biases.shape# Displaying the shape of the biases\n",
    "\n",
    "# Compiling the model with sparse categorical crossentropy loss,\n",
    "# stochastic gradient descent (sgd) optimizer, and accuracy metric\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model on the training data (X_train, y_train) for 30 epochs,\n",
    "# with validation data provided (X_valid, y_valid)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Print the parameters used for model training\n",
    "print(history.params)\n",
    "\n",
    "# Print the available information in the training history\n",
    "print(history.history.keys())\n",
    "\n",
    "# Create a DataFrame with the training history and plot learning curves\n",
    "import pandas as pd\n",
    "pd.DataFrame(history.history).plot(figsize=(4, 2.5))\n",
    "plt.grid(True)\n",
    "plt.xlabel('epochs')\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the trained model on the test data to assess its performance\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "# Select the first 3 instances from the test set for prediction\n",
    "X_new = X_test[:3]\n",
    "\n",
    "# Predict probabilities for each class for the selected instances\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)\n",
    "\n",
    "# Predict the class with the highest probability for each instance\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "print(\"The predicted classes are \",y_pred)\n",
    "\n",
    "# Extract the true labels for the selected instances\n",
    "y_new = y_test[:3]\n",
    "print(\"The true classes are \",y_new)\n",
    "\n",
    "# Plot the selected instances along with their true labels\n",
    "plt.figure(figsize=(7.2, 2.4))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[index]], fontsize=12)\n",
    "# Adjust spacing between subplots for better visualization\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a regression MLP (Multi-Layer Perceptron) using the Sequential API in Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.4317 - val_loss: 0.9149\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 819us/step - loss: 0.8115 - val_loss: 0.8130\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 791us/step - loss: 0.7117 - val_loss: 0.7058\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 809us/step - loss: 0.6651 - val_loss: 0.5976\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 812us/step - loss: 0.6299 - val_loss: 0.5630\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 858us/step - loss: 0.5970 - val_loss: 0.5699\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 813us/step - loss: 0.5733 - val_loss: 0.5719\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 831us/step - loss: 0.5517 - val_loss: 0.4957\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 849us/step - loss: 0.5313 - val_loss: 0.4903\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 784us/step - loss: 0.5152 - val_loss: 0.5240\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 784us/step - loss: 0.5013 - val_loss: 0.4655\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.4892 - val_loss: 0.4978\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 875us/step - loss: 0.4801 - val_loss: 0.4410\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 822us/step - loss: 0.4711 - val_loss: 0.4300\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 811us/step - loss: 0.4634 - val_loss: 0.4248\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 806us/step - loss: 0.4569 - val_loss: 0.4171\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 787us/step - loss: 0.4507 - val_loss: 0.4103\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 863us/step - loss: 0.4465 - val_loss: 0.4068\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.4415 - val_loss: 0.4031\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 864us/step - loss: 0.4379 - val_loss: 0.3999\n",
      "162/162 [==============================] - 0s 573us/step - loss: 0.4360\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "# Initialize the StandardScaler and standardization the training data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Scale the validation and test data using the same scaler\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Now, X_train, X_valid, and X_test are scaled and ready for training and testing a model\n",
    "\n",
    "# Set seed for reproducibility\n",
    "# Setting a random seed, such as tf.random.set_seed(42), in machine learning ensures consistent \n",
    "# generation of random numbers for processes like weight initialization or data shuffling. \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a Sequential model\n",
    "model = keras.models.Sequential([\n",
    "    # The input layer is implicitly defined when you specify the input_shape parameter in the first layer\n",
    "    # keras.layers.InputLayer(input_shape=X_train.shape[1:]), \n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),# First layer with 30 neurons, ReLU activation\n",
    "    keras.layers.Dense(1)# Second layer with 1 neuron (output layer for regression) \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "# Use mean squared error as the loss function and stochastic gradient descent as the optimizer\n",
    "\n",
    "# Train the model on the training data with validation on the validation data\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Evaluate the model on the test data and calculate mean squared error\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Take the first 3 samples from the test set and predict their outputs\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "# Plot the training and validation loss over epochs\n",
    "pd.DataFrame(history.history).plot(figsize=(4, 2.5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_xlim(0, 20)\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide & Deep neural network (Using the Functional API)\n",
    "Not all neural network models follow a simple sequential structure. The architecture of neural networks can indeed be quite diverse, and models with complex topologies, multiple inputs, and/or multiple outputs are not uncommon. One such example is the __Wide & Deep neural network__.\n",
    "    \n",
    "The Wide & Deep model, introduced by Google in a research paper titled \"Wide & Deep Learning for Recommender Systems,\" combines the strengths of both a wide linear model and a deep neural network. This architecture aims to provide the best of both worlds by capturing complex patterns as well as memorizing feature interactions.\n",
    "    \n",
    "__In the Wide & Deep model:__\n",
    "    \n",
    "* __Wide Component (Linear Model):__ This part of the model focuses on memorizing feature interactions through a linear model. It's particularly useful for capturing relationships between different features. (The term \"feature interactions\" refers to how different aspects, or features, relate to each other. In the context of movie recommendations, features could be things like movie genres, actors, or even the user's past behavior (like movies they've watched before).)\n",
    "    \n",
    "* __Deep Component (Neural Network):__ The deep part of the model is responsible for capturing intricate patterns and representations in the data. It helps in generalizing well to unseen examples.\n",
    "    \n",
    "By combining these two components, the Wide & Deep model is capable of handling both memorization and generalization tasks, making it particularly effective for recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 30)           270         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 30)           930         ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 38)           0           ['input_1[0][0]',                \n",
      "                                                                  'dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            39          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.9573 - val_loss: 0.8910\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 968us/step - loss: 0.7343 - val_loss: 0.7364\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 968us/step - loss: 0.6564 - val_loss: 0.6603\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 946us/step - loss: 0.6193 - val_loss: 0.5926\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 941us/step - loss: 0.5907 - val_loss: 0.5649\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 938us/step - loss: 0.5680 - val_loss: 0.5489\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 966us/step - loss: 0.5495 - val_loss: 0.5264\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 932us/step - loss: 0.5343 - val_loss: 0.5115\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 960us/step - loss: 0.5202 - val_loss: 0.4916\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 958us/step - loss: 0.5082 - val_loss: 0.4796\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 945us/step - loss: 0.4979 - val_loss: 0.4827\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 985us/step - loss: 0.4886 - val_loss: 0.4597\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 951us/step - loss: 0.4803 - val_loss: 0.4599\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 925us/step - loss: 0.4724 - val_loss: 0.4471\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 971us/step - loss: 0.4656 - val_loss: 0.4451\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 968us/step - loss: 0.4593 - val_loss: 0.4336\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 944us/step - loss: 0.4540 - val_loss: 0.4310\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 949us/step - loss: 0.4487 - val_loss: 0.4376\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 914us/step - loss: 0.4435 - val_loss: 0.4183\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 994us/step - loss: 0.4397 - val_loss: 0.4385\n",
      "162/162 [==============================] - 0s 679us/step - loss: 0.4267\n",
      "1/1 [==============================] - 0s 59ms/step\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "# Initialize the StandardScaler and standardization the training data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Scale the validation and test data using the same scaler\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Using the Functional API, we create layers, When hidden layer\n",
    "# is created, it's called like a function, taking the input as \n",
    "# an argument. This approach allows us to easily connect layers in \n",
    "# a flexible way, forming a more complex neural network architecture.\n",
    "\n",
    "# Define the input layer with the shape of the training data\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "# Create the first hidden layer\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "# Create the second hidden layer\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "# Create the concatenate layer, that concatenate the input layer with the output of the second hidden layer\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "# Create the output layer\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "# Build the model using the Functional API\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "# Display a summary of the model architecture, including layer information and parameters\n",
    "model.summary()\n",
    "\n",
    "# Compile the model, specifying the loss function and optimizer\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# Train the model on the training data for 20 epochs, validating on the validation set\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Evaluate the model on the test set and calculate the mean squared error\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Make predictions on new data (X_new)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple inputs\n",
    "  * Sending different subsets of input features through the wide or deep paths\n",
    "  * We will send __5__ features (features 0 to 4), and __6__ through the __deep path__ (features 2 to 7). \n",
    "  Note that __3__ features will go through __both__ (features 2, 3 and 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.1809 - val_loss: 1.2252\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7285 - val_loss: 0.7236\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6147 - val_loss: 0.5677\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5655 - val_loss: 0.5221\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 983us/step - loss: 0.5337 - val_loss: 0.4947\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5091 - val_loss: 0.4778\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4916 - val_loss: 0.4647\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.4786 - val_loss: 0.4536\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4684 - val_loss: 0.4423\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 996us/step - loss: 0.4607 - val_loss: 0.4325\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4548 - val_loss: 0.4243\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4497 - val_loss: 0.4182\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 993us/step - loss: 0.4453 - val_loss: 0.4137\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4416 - val_loss: 0.4105\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4382 - val_loss: 0.4072\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4351 - val_loss: 0.4047\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4325 - val_loss: 0.4033\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4300 - val_loss: 0.4013\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4271 - val_loss: 0.4003\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4255 - val_loss: 0.4024\n",
      "162/162 [==============================] - 0s 781us/step - loss: 0.4147\n",
      "1/1 [==============================] - 0s 79ms/step\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define two input layers, one for the wide path and one for the deep path\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "\n",
    "# Create the first hidden layer\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "# Create the second hidden layer\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "# Concatenate the output of the wide input layer and the second hidden layer of the deep path\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "# Create the output layer with 1 neuron and give it a name\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "# Build the model using two input layers and one output layer\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])\n",
    "\n",
    "# Compile the model, specifying mean squared error as the loss function\n",
    "# and using Stochastic Gradient Descent (SGD) as the optimizer with a specific learning rate\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# Prepare data for training and testing by splitting the inputs into wide and deep paths\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "# Train the model using both wide and deep inputs, monitoring performance on the validation set\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "\n",
    "# Evaluate the model on the test set, calculating the mean squared erro\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "\n",
    "# Make predictions on new data (X_new) using both wide and deep inputs\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding extra outputs\n",
    " * In specific tasks, multiple outputs are necessary. For example, in locating and classifying the main object in an image, both regression (finding object coordinates, width, and height) and classification are essential components.\n",
    " * In scenarios with multiple independent tasks from the same data, training a single neural network with one output per task often yields superior results. For instance, in multitask classification for facial images, one output can classify facial expressions (e.g., smiling, surprised), while another can identify whether the person is wearing glasses.\n",
    " * To enhance the model's generalization, auxiliary outputs can be added in a neural network architecture. This inclusion ensures that the underlying portion of the network learns valuable features independently, without solely depending on the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.2119 - main_output_loss: 1.9377 - aux_output_loss: 4.6791 - val_loss: 1.9104 - val_main_output_loss: 1.5227 - val_aux_output_loss: 5.3991\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.9287 - main_output_loss: 0.7081 - aux_output_loss: 2.9138 - val_loss: 1.1932 - val_main_output_loss: 0.6905 - val_aux_output_loss: 5.7171\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7629 - main_output_loss: 0.6240 - aux_output_loss: 2.0134 - val_loss: 1.1386 - val_main_output_loss: 0.5900 - val_aux_output_loss: 6.0752\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6874 - main_output_loss: 0.5846 - aux_output_loss: 1.6123 - val_loss: 1.1113 - val_main_output_loss: 0.5524 - val_aux_output_loss: 6.1413\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6448 - main_output_loss: 0.5572 - aux_output_loss: 1.4334 - val_loss: 1.0425 - val_main_output_loss: 0.5217 - val_aux_output_loss: 5.7292\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6148 - main_output_loss: 0.5337 - aux_output_loss: 1.3445 - val_loss: 0.9652 - val_main_output_loss: 0.5064 - val_aux_output_loss: 5.0942\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5920 - main_output_loss: 0.5150 - aux_output_loss: 1.2846 - val_loss: 0.8864 - val_main_output_loss: 0.4913 - val_aux_output_loss: 4.4426\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5737 - main_output_loss: 0.5000 - aux_output_loss: 1.2371 - val_loss: 0.8051 - val_main_output_loss: 0.4652 - val_aux_output_loss: 3.8637\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5585 - main_output_loss: 0.4875 - aux_output_loss: 1.1976 - val_loss: 0.7396 - val_main_output_loss: 0.4524 - val_aux_output_loss: 3.3243\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5463 - main_output_loss: 0.4782 - aux_output_loss: 1.1597 - val_loss: 0.6820 - val_main_output_loss: 0.4439 - val_aux_output_loss: 2.8256\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5365 - main_output_loss: 0.4711 - aux_output_loss: 1.1244 - val_loss: 0.6395 - val_main_output_loss: 0.4377 - val_aux_output_loss: 2.4557\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5278 - main_output_loss: 0.4651 - aux_output_loss: 1.0920 - val_loss: 0.5986 - val_main_output_loss: 0.4304 - val_aux_output_loss: 2.1125\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5203 - main_output_loss: 0.4602 - aux_output_loss: 1.0610 - val_loss: 0.5732 - val_main_output_loss: 0.4284 - val_aux_output_loss: 1.8765\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5138 - main_output_loss: 0.4562 - aux_output_loss: 1.0320 - val_loss: 0.5472 - val_main_output_loss: 0.4241 - val_aux_output_loss: 1.6552\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5080 - main_output_loss: 0.4527 - aux_output_loss: 1.0056 - val_loss: 0.5289 - val_main_output_loss: 0.4221 - val_aux_output_loss: 1.4898\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5027 - main_output_loss: 0.4495 - aux_output_loss: 0.9812 - val_loss: 0.5099 - val_main_output_loss: 0.4186 - val_aux_output_loss: 1.3319\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4979 - main_output_loss: 0.4468 - aux_output_loss: 0.9579 - val_loss: 0.4989 - val_main_output_loss: 0.4189 - val_aux_output_loss: 1.2186\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4934 - main_output_loss: 0.4441 - aux_output_loss: 0.9365 - val_loss: 0.4890 - val_main_output_loss: 0.4186 - val_aux_output_loss: 1.1225\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4889 - main_output_loss: 0.4414 - aux_output_loss: 0.9169 - val_loss: 0.4777 - val_main_output_loss: 0.4158 - val_aux_output_loss: 1.0350\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4852 - main_output_loss: 0.4393 - aux_output_loss: 0.8984 - val_loss: 0.4818 - val_main_output_loss: 0.4261 - val_aux_output_loss: 0.9834\n",
      "162/162 [==============================] - 0s 879us/step - loss: 0.4773 - main_output_loss: 0.4335 - aux_output_loss: 0.8716\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CB7281F910> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define two input layers, one for the wide path and one for the deep path\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "\n",
    "# Create the first hidden layer for the deep path with 30 neurons and ReLU activation\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "\n",
    "# Create the second hidden layer for the deep path with 30 neurons and ReLU activation\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "# Concatenate the output of the wide input layer and the second hidden layer of the deep path\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "\n",
    "# Create the main output layer with 1 neuron and give it a name\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "\n",
    "# Create an auxiliary output layer with 1 neuron from the second hidden layer\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "\n",
    "# Build the model with two inputs and two outputs (main and auxiliary)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "# Compile the model with mean squared error as the loss function for both outputs\n",
    "# Use SGD as the optimizer with a specific learning rate\n",
    "# Specify the loss weights to emphasize the main output during training\n",
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# Train the model using both wide and deep inputs, with main and auxiliary outputs\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
    "\n",
    "# Evaluate the model on the test set, obtaining total, main, and auxiliary losses\n",
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])\n",
    "\n",
    "# Make predictions on new data (X_new) for both main and auxiliary outputs\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* # Wide & Deep neural network (Using the subclassing API)\n",
    "    The Subclassing API in TensorFlow offers flexibility by allowing you to define custom models that include loops, varying shapes, conditional branching, and other dynamic behaviors that are not as straightforward to implement with the Sequential or Functional APIs. By subclassing the Model class, you gain complete control over the forward pass of your model, enabling you to implement complex architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.1632 - output_1_loss: 2.0220 - output_2_loss: 3.4341 - val_loss: 1.7432 - val_output_1_loss: 1.5685 - val_output_2_loss: 3.3155\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.9461 - output_1_loss: 0.7974 - output_2_loss: 2.2838 - val_loss: 0.9946 - val_output_1_loss: 0.7846 - val_output_2_loss: 2.8843\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7918 - output_1_loss: 0.6708 - output_2_loss: 1.8811 - val_loss: 0.8530 - val_output_1_loss: 0.6518 - val_output_2_loss: 2.6641\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7297 - output_1_loss: 0.6217 - output_2_loss: 1.7017 - val_loss: 0.8111 - val_output_1_loss: 0.6246 - val_output_2_loss: 2.4896\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6910 - output_1_loss: 0.5903 - output_2_loss: 1.5968 - val_loss: 0.7819 - val_output_1_loss: 0.6104 - val_output_2_loss: 2.3255\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6600 - output_1_loss: 0.5638 - output_2_loss: 1.5258 - val_loss: 0.7463 - val_output_1_loss: 0.5889 - val_output_2_loss: 2.1634\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6356 - output_1_loss: 0.5429 - output_2_loss: 1.4700 - val_loss: 0.7217 - val_output_1_loss: 0.5801 - val_output_2_loss: 1.9967\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6155 - output_1_loss: 0.5256 - output_2_loss: 1.4243 - val_loss: 0.6819 - val_output_1_loss: 0.5496 - val_output_2_loss: 1.8729\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5983 - output_1_loss: 0.5108 - output_2_loss: 1.3857 - val_loss: 0.6508 - val_output_1_loss: 0.5276 - val_output_2_loss: 1.7594\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5837 - output_1_loss: 0.4984 - output_2_loss: 1.3509 - val_loss: 0.6279 - val_output_1_loss: 0.5137 - val_output_2_loss: 1.6556\n",
      "162/162 [==============================] - 0s 977us/step - loss: 0.5672 - output_1_loss: 0.4808 - output_2_loss: 1.3441\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CB73E94700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 79ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the custom model class inheriting from keras.models.Model\n",
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)  # Call the constructor of the parent class (Model)\n",
    "        # Define the layers in the constructor\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)  # First hidden layer\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)  # Second hidden layer\n",
    "        self.main_output = keras.layers.Dense(1)  # Output layer for the main output\n",
    "        self.aux_output = keras.layers.Dense(1)  # Auxiliary output layer\n",
    "        \n",
    "        # the call method is the entry point for the forward pass during both training and inference. \n",
    "        # It encapsulates the computations that transform input data into meaningful predictions and outputs.\n",
    "    def call(self, inputs):\n",
    "        # Split the inputs; it's assumed that the model receives a tuple of two inputs\n",
    "        input_A, input_B = inputs\n",
    "        # Pass input_B through the hidden layers\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        # Concatenate input_A and the output of hidden2\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        # Compute the main output\n",
    "        main_output = self.main_output(concat)\n",
    "        # Compute the auxiliary output\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        # Return both outputs\n",
    "        return main_output, aux_output\n",
    "\n",
    "# Instantiate the model with 30 units and ReLU activation for each layer\n",
    "model = WideAndDeepModel(30, activation=\"relu\")\n",
    "\n",
    "# Compile the model specifying the loss function for each output and the optimizer\n",
    "model.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# Fit the model to the data\n",
    "# Note: X_train_A, X_train_B, y_train, X_valid_A, X_valid_B, y_valid should be defined beforehand\n",
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10,\n",
    "                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "# Note: X_test_A, X_test_B, y_test should be defined beforehand\n",
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
    "\n",
    "# Make predictions with the model\n",
    "# Note: X_new_A, X_new_B should be defined beforehand\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Restoring\n",
    "   * Keras uses the HDF5 format for saving a model, which includes the model's architecture, hyperparameters of every layer, and the values of model parameters such as weights and biases. \n",
    "   * The typical workflow involves one script for training and saving the model and one or more scripts or web services for loading the model to make predictions. Loading the model is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.7506 - val_loss: 1.3740\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7132 - val_loss: 0.6647\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6478 - val_loss: 0.6128\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6143 - val_loss: 0.5831\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5884 - val_loss: 0.5561\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5655 - val_loss: 0.5352\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5463 - val_loss: 0.5165\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5287 - val_loss: 0.5021\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5128 - val_loss: 0.4918\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4983 - val_loss: 0.4832\n",
      "162/162 [==============================] - 0s 782us/step - loss: 0.4784\n"
     ]
    }
   ],
   "source": [
    "# Set the seed for NumPy and TensorFlow to ensure reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define the model structure\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]), # First hidden layer with 30 neurons and ReLU activation, expected input shape is 8 features\n",
    "    keras.layers.Dense(30, activation=\"relu\"), # Second hidden layer with 30 neurons and ReLU activation\n",
    "    keras.layers.Dense(1) # Output layer with a single neuron (for regression tasks)\n",
    "]) \n",
    "\n",
    "# Compile the model specifying the loss function and optimizer\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# Train the model with the training data, validate using validation data, for 10 epochs\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Evaluate the model's performance on the test set\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Save the entire model to a HDF5 file\n",
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load the previously saved model from the HDF5 file\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")\n",
    "\n",
    "# Use the loaded model to make predictions on new data\n",
    "predictions = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Callbacks\n",
    " * The __fit()__ method in Keras can use __callbacks__ to perform specific actions at various stages of training, like at the beginning or end of an epoch. \n",
    "##  ModelCheckpoint\n",
    "  * A popular callback, __ModelCheckpoint__, automatically saves the model during training, typically after every epoch, allowing you to keep snapshots of the model at different stages. \n",
    "  * This is particularly useful for retaining the best version of your model based on performance metrics like validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.2028 - val_loss: 1.3171\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7012 - val_loss: 0.6840\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5962 - val_loss: 0.5324\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5477 - val_loss: 0.4990\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5112 - val_loss: 0.4648\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4812 - val_loss: 0.4448\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4588 - val_loss: 0.4236\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4419 - val_loss: 0.4157\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4294 - val_loss: 0.4144\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4200 - val_loss: 0.4130\n",
      "162/162 [==============================] - 0s 785us/step - loss: 0.4126\n"
     ]
    }
   ],
   "source": [
    "# Clear any existing Keras sessions to start with a clean slate\n",
    "# Clearing the session is particularly useful when you want to start fresh, \n",
    "# ensuring that any previously defined models, layers, or tensors are removed from memory.\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build a sequential model with three dense layers\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),  # First hidden layer with 30 neurons and ReLU activation, input shape is 8 features\n",
    "    keras.layers.Dense(30, activation=\"relu\"),  # Second hidden layer with 30 neurons and ReLU activation\n",
    "    keras.layers.Dense(1)  # Output layer with a single neuron (for regression tasks)\n",
    "])\n",
    "\n",
    "# Compile the model with mean squared error loss and stochastic gradient descent optimizer\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# Configure ModelCheckpoint callback to save the best model during training\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "\n",
    "# Train the model for 10 epochs, validating on separate validation data, and save the best model\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])# Use the ModelCheckpoint callback to save the best model during training\n",
    "\n",
    "# Load the best model (rollback to the model with the lowest validation loss)\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")\n",
    "\n",
    "# Evaluate the model's performance on the test set\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  EarlyStopping\n",
    "  * An alternative method for implementing early stopping in Keras by utilizing the __EarlyStopping__ callback.\n",
    "* This callback interrupts the training process when there is no progress on the validation set for a specified number of epochs (determined by the __patience__ argument). Additionally, it can optionally roll back to the best model, preventing overfitting.\n",
    " * By combining the ModelCheckpoint and EarlyStopping callbacks, you create a robust strategy: saving checkpoints of your model to handle potential interruptions (e.g., computer crashes) and early stopping to avoid unnecessary time and resource consumption when progress plateaus. This dual-callback approach enhances both the resilience and efficiency of your training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4133 - val_loss: 0.4115\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4081 - val_loss: 0.4356\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4038 - val_loss: 0.4293\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4001 - val_loss: 0.4318\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3972 - val_loss: 0.4078\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3942 - val_loss: 0.4335\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.4186\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3900 - val_loss: 0.3975\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3880 - val_loss: 0.4118\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3859 - val_loss: 0.4260\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3845 - val_loss: 0.3954\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3825 - val_loss: 0.4333\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3811 - val_loss: 0.4345\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3796 - val_loss: 0.4000\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3780 - val_loss: 0.4070\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3767 - val_loss: 0.3953\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3754 - val_loss: 0.4203\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.4021\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3729 - val_loss: 0.4038\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3720 - val_loss: 0.4478\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.4211\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3697 - val_loss: 0.4375\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3687 - val_loss: 0.3785\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3676 - val_loss: 0.4075\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3667 - val_loss: 0.4301\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3660 - val_loss: 0.4304\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.3686\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3644 - val_loss: 0.4171\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3638 - val_loss: 0.3896\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3628 - val_loss: 0.4352\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3620 - val_loss: 0.3936\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3611 - val_loss: 0.4401\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3612 - val_loss: 0.3776\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3601 - val_loss: 0.3728\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3593 - val_loss: 0.4377\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3589 - val_loss: 0.3588\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3583 - val_loss: 0.3959\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3577 - val_loss: 0.3674\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.3652\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.4048\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.4191\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3556 - val_loss: 0.4265\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.4268\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3542 - val_loss: 0.4510\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3541 - val_loss: 0.3710\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3532 - val_loss: 0.3712\n",
      "162/162 [==============================] - 0s 746us/step - loss: 0.3566\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with mean squared error loss and stochastic gradient descent optimizer\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# Configure EarlyStopping callback to interrupt training if no progress on validation set for 10 epochs, and restore the best weights\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model for up to 100 epochs, saving checkpoints and applying early stopping\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "# Evaluate the model's performance on the test set\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom callbacks\n",
    " * To demonstrate this concept, let's take a look at a custom callback. This callback will specifically show the ratio between the validation loss and the training loss throughout the training process, helping to identify overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/363 [========================>.....] - ETA: 0s - loss: 0.3570\n",
      "val/train: 1.06\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3582 - val_loss: 0.3804\n"
     ]
    }
   ],
   "source": [
    "# Define a custom callback class to print the ratio between validation loss and training loss\n",
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        # Print the ratio between validation loss and training loss at the end of each epoch\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "\n",
    "# Create an instance of the custom callback\n",
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "\n",
    "# Train the model for one epoch, using the custom callback to print the ratio\n",
    "history = model.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorBoard for Visualization\n",
    " * TensorBoard is a tool for visualizing and monitoring machine learning models.\n",
    " * Users can track metrics like loss and accuracy during training.\n",
    " * TensorBoard helps visualize the computational graph of a model.\n",
    " * It allows exploration of histograms of activations and weights.\n",
    " * Developers integrate TensorBoard into their TensorFlow code.\n",
    " * It provides insights into the training process and model performance.\n",
    "\n",
    "## How to use TensorBoard\n",
    " * To use TensorBoard, modify your program to output data to __binary__ log files called __event__ files. \n",
    " * Each data record is a __summary__. \n",
    " * The TensorBoard server monitors the log directory, automatically updating visualizations. \n",
    " * Aim to point TensorBoard to a root log directory and configure your program to write to a different subdirectory for each run. \n",
    " * This approach facilitates visualization and comparison of data from multiple program runs on the same TensorBoard server without confusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.8718 - val_loss: 2.4523\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 962us/step - loss: 0.7165 - val_loss: 0.6289\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 965us/step - loss: 0.6347 - val_loss: 0.6763\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.5945 - val_loss: 0.5451\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 964us/step - loss: 0.5615 - val_loss: 0.5141\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 926us/step - loss: 0.5350 - val_loss: 0.5051\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 948us/step - loss: 0.5142 - val_loss: 0.4942\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 965us/step - loss: 0.4975 - val_loss: 0.4584\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 974us/step - loss: 0.4833 - val_loss: 0.4589\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 945us/step - loss: 0.4716 - val_loss: 0.4459\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 990us/step - loss: 0.4620 - val_loss: 0.4342\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 968us/step - loss: 0.4533 - val_loss: 0.4203\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 953us/step - loss: 0.4461 - val_loss: 0.4221\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 969us/step - loss: 0.4393 - val_loss: 0.4384\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 997us/step - loss: 0.4330 - val_loss: 0.4036\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4274 - val_loss: 0.4674\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.4223 - val_loss: 0.3968\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 973us/step - loss: 0.4183 - val_loss: 0.4040\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4133 - val_loss: 0.3982\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 997us/step - loss: 0.4102 - val_loss: 0.3976\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4070 - val_loss: 0.3913\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 972us/step - loss: 0.4036 - val_loss: 0.3947\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 951us/step - loss: 0.4010 - val_loss: 0.5276\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.3981 - val_loss: 0.3830\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 970us/step - loss: 0.3956 - val_loss: 0.3724\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 966us/step - loss: 0.3930 - val_loss: 0.3698\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 966us/step - loss: 0.3913 - val_loss: 0.4633\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 957us/step - loss: 0.3891 - val_loss: 0.3804\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 966us/step - loss: 0.3875 - val_loss: 0.3980\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.3851 - val_loss: 0.3780\n"
     ]
    }
   ],
   "source": [
    "# Import the os module for operating system-related functionalities\n",
    "import os\n",
    "\n",
    "# Define the root log directory using os.path.join to ensure cross-platform compatibility\n",
    "root_logdir = os.path.join(os.curdir, \"logs\")\n",
    "\n",
    "# Function to generate a unique run log directory based on the current timestamp\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    # Generate a unique run identifier based on the current timestamp\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")  # Format: run_YYYY_MM_DD-HH_MM_SS\n",
    "    return os.path.join(root_logdir, run_id)  # Construct the full path using os.path.join\n",
    "\n",
    "# Get the run log directory using the defined function\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "# Display the run log directory\n",
    "run_logdir\n",
    "\n",
    "# Clear Keras session, and set random seeds for reproducibility\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define a simple Keras Sequential model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model with mean squared error loss and SGD optimizer\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# Create a TensorBoard callback for visualization during training\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "# Train the model using fit() with training and validation data and specified callbacks\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 12840), started 0:58:11 ago. (Use '!kill 12840' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-787b7a7176064723\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-787b7a7176064723\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard extension in Jupyter Notebook\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Start TensorBoard to visualize the logs in the 'logs' directory\n",
    "# %tensorboard --logdir logs\n",
    "\n",
    "%tensorboard --logdir=./logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    " * To perform hyperparameter tuning and leverage Scikit-Learn's tools for this purpose, you can use the __KerasRegressor__ wrapper provided by the __keras.wrappers.scikit_learn__ module. \n",
    " * This allows you to use Keras models as if they were Scikit-Learn regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dodge\\AppData\\Local\\Temp\\ipykernel_16732\\2819619125.py:27: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 1ms/step - loss: 1.6315 - val_loss: 23.1763\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.8240 - val_loss: 0.5634\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 879us/step - loss: 0.5587 - val_loss: 0.5110\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 875us/step - loss: 0.5137 - val_loss: 0.4761\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 900us/step - loss: 0.4840 - val_loss: 0.4464\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 867us/step - loss: 0.4617 - val_loss: 0.4288\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 863us/step - loss: 0.4475 - val_loss: 0.4215\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 876us/step - loss: 0.4370 - val_loss: 0.4200\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 867us/step - loss: 0.4287 - val_loss: 0.4189\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 863us/step - loss: 0.4223 - val_loss: 0.4222\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 884us/step - loss: 0.4176 - val_loss: 0.4243\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.4131 - val_loss: 0.4279\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 873us/step - loss: 0.4092 - val_loss: 0.4324\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 860us/step - loss: 0.4061 - val_loss: 0.4332\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 865us/step - loss: 0.4030 - val_loss: 0.4317\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 861us/step - loss: 0.4002 - val_loss: 0.4295\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 862us/step - loss: 0.3980 - val_loss: 0.4295\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 882us/step - loss: 0.3960 - val_loss: 0.4291\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 854us/step - loss: 0.3931 - val_loss: 0.4274\n",
      "162/162 [==============================] - 0s 597us/step - loss: 0.3903\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    }
   ],
   "source": [
    "# Clear Keras session to release resources and avoid conflicts with previous models\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)  # Set seed for NumPy random functions\n",
    "tf.random.set_seed(42)  # Set seed for TensorFlow random functions\n",
    "\n",
    "# Function to build a Keras model with configurable hyperparameters\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    # Add hidden layers with specified number of neurons and ReLU activation\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    \n",
    "    # Output layer for regression task\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    # Compile the model with mean squared error loss and SGD optimizer\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a KerasRegressor using the build_model function\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "\n",
    "# Train the Keras model using fit with early stopping\n",
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n",
    "# Evaluate the model on the test set and calculate Mean Squared Error (MSE)\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "\n",
    "# Make predictions on new data using the trained Keras model\n",
    "y_pred = keras_reg.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dodge\\AppData\\Local\\Temp\\ipykernel_16732\\757584540.py:27: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 0.7701 - val_loss: 0.5201\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.9804 - val_loss: 0.7780\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.5664 - val_loss: 0.5760\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4911 - val_loss: 0.6925\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4557 - val_loss: 0.9401\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.4501 - val_loss: 1.4434\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4444 - val_loss: 0.7481\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.4210 - val_loss: 0.4117\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.4115 - val_loss: 0.4104\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4019 - val_loss: 0.3960\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3977 - val_loss: 0.3959\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.3944 - val_loss: 0.4040\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.3821\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3919 - val_loss: 0.3845\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3899 - val_loss: 0.3802\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3890 - val_loss: 0.3869\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3910 - val_loss: 0.3887\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3895 - val_loss: 0.3781\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3914 - val_loss: 0.3804\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3867 - val_loss: 0.3820\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3854 - val_loss: 0.3779\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3844 - val_loss: 0.3788\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3824 - val_loss: 0.3799\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.3855 - val_loss: 0.3873\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3827 - val_loss: 0.3765\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.3800 - val_loss: 0.3724\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4133 - val_loss: 0.3931\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3947 - val_loss: 0.3923\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3847 - val_loss: 0.3745\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3802 - val_loss: 0.3791\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3829 - val_loss: 0.3704\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3784 - val_loss: 0.3659\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.3798 - val_loss: 0.3663\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.3766 - val_loss: 0.3669\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3769 - val_loss: 0.4604\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3852 - val_loss: 0.4346\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3773 - val_loss: 0.3609\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.3775 - val_loss: 0.3627\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3748 - val_loss: 0.3596\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3772 - val_loss: 0.3588\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3747 - val_loss: 0.3579\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.3786 - val_loss: 0.3568\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.3770 - val_loss: 0.3562\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.3738 - val_loss: 0.3539\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.3736 - val_loss: 0.3520\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.3731 - val_loss: 0.3589\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.3732 - val_loss: 0.3612\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.3729 - val_loss: 0.4017\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3750 - val_loss: 0.5817\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.3771 - val_loss: 0.3511\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3731 - val_loss: 0.3648\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3737 - val_loss: 0.3535\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3766 - val_loss: 0.3542\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3750 - val_loss: 0.3723\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3747 - val_loss: 0.3564\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.3512\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3719 - val_loss: 0.3515\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3852 - val_loss: 0.3593\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3749 - val_loss: 0.3561\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3718 - val_loss: 0.3612\n",
      "121/121 [==============================] - 0s 579us/step - loss: 0.3940\n",
      "[CV] END learning_rate=0.02217457394835346, n_hidden=1, n_neurons=4; total time=  14.8s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 0.9900 - val_loss: 3.5725\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4864 - val_loss: 1.2869\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.4448 - val_loss: 0.4037\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4358 - val_loss: 0.4367\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4312 - val_loss: 0.4355\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4292 - val_loss: 0.6200\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4279 - val_loss: 0.6525\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4259 - val_loss: 0.6317\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4232 - val_loss: 0.9199\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4218 - val_loss: 0.6555\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4194 - val_loss: 0.7270\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4187 - val_loss: 0.7001\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4173 - val_loss: 0.6759\n",
      "121/121 [==============================] - 0s 627us/step - loss: 0.4368\n",
      "[CV] END learning_rate=0.02217457394835346, n_hidden=1, n_neurons=4; total time=   3.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 0.9062 - val_loss: 0.4661\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4777 - val_loss: 0.4179\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4642 - val_loss: 0.4104\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.4562 - val_loss: 0.4464\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4573 - val_loss: 0.4377\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4461 - val_loss: 0.4547\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4568 - val_loss: 0.4371\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4460 - val_loss: 0.4492\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.4409 - val_loss: 0.4812\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4419 - val_loss: 0.4357\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4412 - val_loss: 0.4824\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4378 - val_loss: 0.4398\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.4375 - val_loss: 0.4134\n",
      "121/121 [==============================] - 0s 595us/step - loss: 0.4255\n",
      "[CV] END learning_rate=0.02217457394835346, n_hidden=1, n_neurons=4; total time=   3.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0631 - val_loss: 28.2548\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8814 - val_loss: 3.4782\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5251 - val_loss: 0.4703\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4421 - val_loss: 0.4020\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4093 - val_loss: 0.3861\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3908 - val_loss: 0.4521\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3794 - val_loss: 0.3736\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3697 - val_loss: 0.4372\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3630 - val_loss: 0.4318\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3580 - val_loss: 0.4307\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3545 - val_loss: 0.3442\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3639\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3446 - val_loss: 0.5101\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3443 - val_loss: 0.3703\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3393 - val_loss: 0.4482\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3383 - val_loss: 0.3567\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3339 - val_loss: 0.5817\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3351 - val_loss: 0.3256\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3295 - val_loss: 0.4874\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3292 - val_loss: 0.3403\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3260 - val_loss: 0.4657\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3246 - val_loss: 0.3941\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3220 - val_loss: 0.3585\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3221 - val_loss: 0.3257\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3191 - val_loss: 0.3914\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3167 - val_loss: 0.3237\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3159 - val_loss: 0.4173\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3149 - val_loss: 0.3365\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3112 - val_loss: 0.5492\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3134 - val_loss: 0.3460\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3121 - val_loss: 0.3813\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3096 - val_loss: 0.3045\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3069 - val_loss: 0.3861\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3048 - val_loss: 0.3364\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3033 - val_loss: 0.3080\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3017 - val_loss: 0.4434\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3019 - val_loss: 0.3943\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3031 - val_loss: 0.4614\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3009 - val_loss: 0.3055\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2986 - val_loss: 0.3637\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2970 - val_loss: 0.3065\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2950 - val_loss: 0.3123\n",
      "121/121 [==============================] - 0s 704us/step - loss: 0.3297\n",
      "[CV] END learning_rate=0.005432590230265345, n_hidden=2, n_neurons=94; total time=  14.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9362 - val_loss: 0.5746\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5134 - val_loss: 0.6822\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4447 - val_loss: 0.4741\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4182 - val_loss: 0.3841\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4002 - val_loss: 0.4040\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.6347\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.8584\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3729 - val_loss: 0.9511\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3667 - val_loss: 1.1354\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3627 - val_loss: 0.8032\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3580 - val_loss: 0.7924\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3529 - val_loss: 0.7982\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.9132\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3542 - val_loss: 0.8098\n",
      "121/121 [==============================] - 0s 724us/step - loss: 0.3727\n",
      "[CV] END learning_rate=0.005432590230265345, n_hidden=2, n_neurons=94; total time=   4.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9691 - val_loss: 6.7178\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6253 - val_loss: 37.8028\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6580 - val_loss: 17.9768\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6727 - val_loss: 4.3344\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4500 - val_loss: 0.6650\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4079 - val_loss: 0.4177\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3928 - val_loss: 0.4060\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3834 - val_loss: 0.4039\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3773 - val_loss: 0.4240\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3723 - val_loss: 0.3755\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3700 - val_loss: 0.4453\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3644 - val_loss: 0.4034\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3596 - val_loss: 0.3721\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3571 - val_loss: 0.4375\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3552 - val_loss: 0.3441\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3510 - val_loss: 0.3948\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3450\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.3663\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3448 - val_loss: 0.3931\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3424 - val_loss: 0.3931\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3442 - val_loss: 0.3574\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3393 - val_loss: 0.3456\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3376 - val_loss: 0.3468\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3375 - val_loss: 0.3760\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3352 - val_loss: 0.3202\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3327 - val_loss: 0.3387\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3315 - val_loss: 0.3785\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3306 - val_loss: 0.3204\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3311 - val_loss: 0.3981\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3280 - val_loss: 0.3733\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3259 - val_loss: 0.3637\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3241 - val_loss: 0.3817\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3240 - val_loss: 0.3674\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3219 - val_loss: 0.3851\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3213 - val_loss: 0.3233\n",
      "121/121 [==============================] - 0s 717us/step - loss: 0.3267\n",
      "[CV] END learning_rate=0.005432590230265345, n_hidden=2, n_neurons=94; total time=  10.9s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 4.5903 - val_loss: 3.0861\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 2.5036 - val_loss: 2.3863\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 1.7581 - val_loss: 1.9688\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3656 - val_loss: 1.4866\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 1.1128 - val_loss: 1.1115\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.9419 - val_loss: 0.8979\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.8288 - val_loss: 0.7743\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.7559 - val_loss: 0.7090\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.7088 - val_loss: 0.6717\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.6775 - val_loss: 0.6492\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.6555 - val_loss: 0.6377\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.6390 - val_loss: 0.6195\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6254 - val_loss: 0.6119\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.6137 - val_loss: 0.6035\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.6033 - val_loss: 0.5874\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.5934 - val_loss: 0.5812\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.5844 - val_loss: 0.5711\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.5757 - val_loss: 0.5643\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.5675 - val_loss: 0.5542\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5599 - val_loss: 0.5469\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.5525 - val_loss: 0.5341\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.5455 - val_loss: 0.5281\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.5388 - val_loss: 0.5215\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.5324 - val_loss: 0.5152\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5264 - val_loss: 0.5053\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.5206 - val_loss: 0.4949\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.5151 - val_loss: 0.4923\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.5100 - val_loss: 0.4833\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5048 - val_loss: 0.4784\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.5001 - val_loss: 0.4731\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4955 - val_loss: 0.4686\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4911 - val_loss: 0.4632\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.4870 - val_loss: 0.4587\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4830 - val_loss: 0.4541\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.4792 - val_loss: 0.4499\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4757 - val_loss: 0.4468\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.4723 - val_loss: 0.4426\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4689 - val_loss: 0.4400\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4658 - val_loss: 0.4366\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4628 - val_loss: 0.4337\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4600 - val_loss: 0.4313\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4573 - val_loss: 0.4286\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.4547 - val_loss: 0.4261\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4522 - val_loss: 0.4238\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4499 - val_loss: 0.4217\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4476 - val_loss: 0.4196\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.4455 - val_loss: 0.4177\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.4434 - val_loss: 0.4159\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.4415 - val_loss: 0.4142\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4396 - val_loss: 0.4125\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.4379 - val_loss: 0.4109\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4362 - val_loss: 0.4097\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4346 - val_loss: 0.4085\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4330 - val_loss: 0.4068\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4315 - val_loss: 0.4059\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.4301 - val_loss: 0.4052\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.4034\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.4275 - val_loss: 0.4029\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.4263 - val_loss: 0.4019\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4250 - val_loss: 0.4011\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4239 - val_loss: 0.4005\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.4228 - val_loss: 0.3997\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.4217 - val_loss: 0.3986\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4207 - val_loss: 0.3990\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4197 - val_loss: 0.3982\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.4187 - val_loss: 0.3966\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4178 - val_loss: 0.3964\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4169 - val_loss: 0.3964\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.3958\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.4151 - val_loss: 0.3953\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.4143 - val_loss: 0.3951\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4135 - val_loss: 0.3947\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.3931\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.4120 - val_loss: 0.3934\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.4113 - val_loss: 0.3938\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4105 - val_loss: 0.3930\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.4099 - val_loss: 0.3918\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.4092 - val_loss: 0.3924\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4085 - val_loss: 0.3927\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.4079 - val_loss: 0.3915\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.4073 - val_loss: 0.3917\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4066 - val_loss: 0.3926\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4060 - val_loss: 0.3913\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4054 - val_loss: 0.3911\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4048 - val_loss: 0.3900\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4042 - val_loss: 0.3909\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4037 - val_loss: 0.3887\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4031 - val_loss: 0.3872\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.4026 - val_loss: 0.3868\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.4020 - val_loss: 0.3881\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.4015 - val_loss: 0.3862\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4010 - val_loss: 0.3852\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4004 - val_loss: 0.3858\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3999 - val_loss: 0.3855\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3994 - val_loss: 0.3843\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3989 - val_loss: 0.3842\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3985 - val_loss: 0.3835\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3980 - val_loss: 0.3826\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3975 - val_loss: 0.3854\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3971 - val_loss: 0.3833\n",
      "121/121 [==============================] - 0s 621us/step - loss: 0.4107\n",
      "[CV] END learning_rate=0.00037078874137762155, n_hidden=1, n_neurons=51; total time=  24.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 2.7783 - val_loss: 6.4687\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 1.3888 - val_loss: 7.9865\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9913 - val_loss: 8.3447\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.8704 - val_loss: 7.8926\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8213 - val_loss: 7.1040\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.7929 - val_loss: 6.2430\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7717 - val_loss: 5.4286\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7541 - val_loss: 4.6673\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.7386 - val_loss: 4.0017\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.7244 - val_loss: 3.4346\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.7111 - val_loss: 2.9412\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6985 - val_loss: 2.5057\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.6867 - val_loss: 2.1380\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.6755 - val_loss: 1.8282\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.6647 - val_loss: 1.5581\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.6545 - val_loss: 1.3341\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.6446 - val_loss: 1.1436\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.6351 - val_loss: 0.9867\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.6259 - val_loss: 0.8584\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.6172 - val_loss: 0.7604\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.6088 - val_loss: 0.6910\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.6007 - val_loss: 0.6421\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.5929 - val_loss: 0.6048\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.5854 - val_loss: 0.5805\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5782 - val_loss: 0.5660\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.5713 - val_loss: 0.5610\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.5646 - val_loss: 0.5638\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.5581 - val_loss: 0.5735\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.5518 - val_loss: 0.5870\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.5460 - val_loss: 0.6075\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.5401 - val_loss: 0.6324\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.5346 - val_loss: 0.6592\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5294 - val_loss: 0.6892\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5244 - val_loss: 0.7187\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.5196 - val_loss: 0.7504\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.5151 - val_loss: 0.7856\n",
      "121/121 [==============================] - 0s 611us/step - loss: 0.5420\n",
      "[CV] END learning_rate=0.00037078874137762155, n_hidden=1, n_neurons=51; total time=   9.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 4.2004 - val_loss: 6.1673\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 2.0756 - val_loss: 5.0294\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 1.3458 - val_loss: 2.9250\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 1.0206 - val_loss: 1.7784\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.8690 - val_loss: 1.0635\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.7944 - val_loss: 0.8144\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.7549 - val_loss: 0.7266\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.7308 - val_loss: 0.6904\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.7140 - val_loss: 0.6775\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.7006 - val_loss: 0.6650\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.6886 - val_loss: 0.6607\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.6780 - val_loss: 0.6525\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.6681 - val_loss: 0.6393\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.6587 - val_loss: 0.6310\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.6498 - val_loss: 0.6205\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.6413 - val_loss: 0.6175\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.6333 - val_loss: 0.6089\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.6255 - val_loss: 0.6014\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.6180 - val_loss: 0.6027\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.6111 - val_loss: 0.5909\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.6042 - val_loss: 0.5826\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.5976 - val_loss: 0.5784\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.5913 - val_loss: 0.5677\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.5852 - val_loss: 0.5595\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.5793 - val_loss: 0.5551\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5736 - val_loss: 0.5470\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5680 - val_loss: 0.5495\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.5628 - val_loss: 0.5432\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.5576 - val_loss: 0.5396\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.5527 - val_loss: 0.5349\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.5478 - val_loss: 0.5316\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.5432 - val_loss: 0.5269\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.5386 - val_loss: 0.5233\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.5342 - val_loss: 0.5190\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.5300 - val_loss: 0.5167\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.5259 - val_loss: 0.5052\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.5219 - val_loss: 0.4977\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.5180 - val_loss: 0.4981\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.4937\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5107 - val_loss: 0.4901\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.5071 - val_loss: 0.4866\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.5038 - val_loss: 0.4819\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.5006 - val_loss: 0.4783\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.4974 - val_loss: 0.4757\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4943 - val_loss: 0.4747\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4914 - val_loss: 0.4722\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.4886 - val_loss: 0.4668\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4858 - val_loss: 0.4657\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4831 - val_loss: 0.4634\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.4805 - val_loss: 0.4604\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4780 - val_loss: 0.4577\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.4756 - val_loss: 0.4544\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4732 - val_loss: 0.4510\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4709 - val_loss: 0.4474\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.4687 - val_loss: 0.4441\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.4666 - val_loss: 0.4401\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4646 - val_loss: 0.4384\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.4626 - val_loss: 0.4367\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4605 - val_loss: 0.4347\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4586 - val_loss: 0.4335\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.4568 - val_loss: 0.4291\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4550 - val_loss: 0.4272\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.4533 - val_loss: 0.4252\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.4515 - val_loss: 0.4237\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4499 - val_loss: 0.4216\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4483 - val_loss: 0.4194\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4467 - val_loss: 0.4189\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4453 - val_loss: 0.4169\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.4438 - val_loss: 0.4149\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4424 - val_loss: 0.4140\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4410 - val_loss: 0.4129\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.4397 - val_loss: 0.4114\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.4383 - val_loss: 0.4103\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.4371 - val_loss: 0.4083\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4358 - val_loss: 0.4067\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4346 - val_loss: 0.4051\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.4334 - val_loss: 0.4040\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.4323 - val_loss: 0.4031\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4312 - val_loss: 0.4019\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4301 - val_loss: 0.4008\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4291 - val_loss: 0.3999\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4281 - val_loss: 0.3988\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.4271 - val_loss: 0.3979\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.3969\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.4252 - val_loss: 0.3960\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4243 - val_loss: 0.3952\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4234 - val_loss: 0.3944\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4225 - val_loss: 0.3935\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4217 - val_loss: 0.3930\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.4209 - val_loss: 0.3922\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4200 - val_loss: 0.3913\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4193 - val_loss: 0.3907\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.4185 - val_loss: 0.3901\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4178 - val_loss: 0.3894\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4170 - val_loss: 0.3890\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4163 - val_loss: 0.3884\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.4156 - val_loss: 0.3882\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4150 - val_loss: 0.3873\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.4143 - val_loss: 0.3868\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.4136 - val_loss: 0.3867\n",
      "121/121 [==============================] - 0s 611us/step - loss: 0.4130\n",
      "[CV] END learning_rate=0.00037078874137762155, n_hidden=1, n_neurons=51; total time=  24.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.7175 - val_loss: 2.3065\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6848 - val_loss: 0.6431\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5870 - val_loss: 0.5413\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5402 - val_loss: 0.4997\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5053 - val_loss: 0.4688\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4783 - val_loss: 0.4539\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4576 - val_loss: 0.4255\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4413 - val_loss: 0.4266\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4286 - val_loss: 0.4217\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4184 - val_loss: 0.4075\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4099 - val_loss: 0.3892\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4024 - val_loss: 0.3825\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3961 - val_loss: 0.3950\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3909 - val_loss: 0.4107\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3863 - val_loss: 0.3909\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.3855\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3785 - val_loss: 0.4041\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3756 - val_loss: 0.3787\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3720 - val_loss: 0.3920\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3699 - val_loss: 0.3819\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3675 - val_loss: 0.3923\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3648 - val_loss: 0.3730\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3625 - val_loss: 0.3797\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3608 - val_loss: 0.3488\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.3927\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3574 - val_loss: 0.3438\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3556 - val_loss: 0.3909\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.3603\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3521 - val_loss: 0.3762\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3514 - val_loss: 0.3392\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3427\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3375\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3645\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3445\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3367\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3442 - val_loss: 0.3889\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3440 - val_loss: 0.3683\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3429 - val_loss: 0.4329\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3424 - val_loss: 0.3458\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3413 - val_loss: 0.3804\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3403 - val_loss: 0.3326\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3387 - val_loss: 0.3304\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3387 - val_loss: 0.3301\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3370 - val_loss: 0.3732\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3375 - val_loss: 0.3308\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3355 - val_loss: 0.3438\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3350 - val_loss: 0.3429\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3345 - val_loss: 0.3432\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3335 - val_loss: 0.3286\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3328 - val_loss: 0.3910\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3328 - val_loss: 0.3313\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3315 - val_loss: 0.3333\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3311 - val_loss: 0.3325\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3306 - val_loss: 0.3292\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3291 - val_loss: 0.3690\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3304 - val_loss: 0.3375\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3286 - val_loss: 0.3956\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3283 - val_loss: 0.3279\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3273 - val_loss: 0.3228\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3266 - val_loss: 0.3271\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3262 - val_loss: 0.3461\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3256 - val_loss: 0.3541\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3257 - val_loss: 0.3248\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3246 - val_loss: 0.3716\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3246 - val_loss: 0.3230\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3245 - val_loss: 0.3201\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3233 - val_loss: 0.3555\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3235 - val_loss: 0.3197\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3222 - val_loss: 0.3557\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3228 - val_loss: 0.3198\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3213 - val_loss: 0.3447\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3213 - val_loss: 0.3199\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3209 - val_loss: 0.3206\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3201 - val_loss: 0.3788\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3208 - val_loss: 0.3159\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3188 - val_loss: 0.3650\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3200 - val_loss: 0.3555\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3189 - val_loss: 0.4255\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3197 - val_loss: 0.3179\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3178 - val_loss: 0.3603\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3177 - val_loss: 0.3158\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3169 - val_loss: 0.3408\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3170 - val_loss: 0.3145\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3161 - val_loss: 0.3264\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.3164\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3151 - val_loss: 0.3173\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3153 - val_loss: 0.3224\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3147 - val_loss: 0.3172\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3143 - val_loss: 0.3118\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3137 - val_loss: 0.3586\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3145 - val_loss: 0.3625\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3138 - val_loss: 0.3422\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3129 - val_loss: 0.3104\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3125 - val_loss: 0.3098\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3119 - val_loss: 0.3118\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3108 - val_loss: 0.3112\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3110 - val_loss: 0.3098\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3112 - val_loss: 0.3092\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3100 - val_loss: 0.3551\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3106 - val_loss: 0.3594\n",
      "121/121 [==============================] - 0s 700us/step - loss: 0.3410\n",
      "[CV] END learning_rate=0.0016535051383872372, n_hidden=2, n_neurons=70; total time=  29.3s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.5615 - val_loss: 5.1322\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7466 - val_loss: 1.4911\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6595 - val_loss: 0.6129\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6042 - val_loss: 0.8737\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5606 - val_loss: 1.1774\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5267 - val_loss: 1.3210\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4990 - val_loss: 1.4040\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4765 - val_loss: 1.4200\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4583 - val_loss: 1.2863\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4444 - val_loss: 1.1667\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4327 - val_loss: 1.0163\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.9122\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4154 - val_loss: 0.7752\n",
      "121/121 [==============================] - 0s 707us/step - loss: 0.4445\n",
      "[CV] END learning_rate=0.0016535051383872372, n_hidden=2, n_neurons=70; total time=   4.2s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.9435 - val_loss: 2.9286\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6764 - val_loss: 0.5851\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5704 - val_loss: 0.5388\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5299 - val_loss: 0.5117\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4980 - val_loss: 0.4852\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4742 - val_loss: 0.4662\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4555 - val_loss: 0.4479\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4416 - val_loss: 0.4279\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4301 - val_loss: 0.4647\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4226 - val_loss: 0.4132\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4141 - val_loss: 0.4743\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4085 - val_loss: 0.4186\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4033 - val_loss: 0.3977\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3985 - val_loss: 0.4686\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3951 - val_loss: 0.3928\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3908 - val_loss: 0.4561\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3878 - val_loss: 0.3923\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3848 - val_loss: 0.3881\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3819 - val_loss: 0.4452\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3799 - val_loss: 0.4074\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3778 - val_loss: 0.4112\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3755 - val_loss: 0.3860\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3734 - val_loss: 0.3792\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3717 - val_loss: 0.4402\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3703 - val_loss: 0.3612\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3679 - val_loss: 0.3708\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3666 - val_loss: 0.4603\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.3676\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3637 - val_loss: 0.4692\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3630 - val_loss: 0.4002\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3618 - val_loss: 0.3984\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3600 - val_loss: 0.4647\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3593 - val_loss: 0.4337\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3576 - val_loss: 0.4653\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3573 - val_loss: 0.3822\n",
      "121/121 [==============================] - 0s 708us/step - loss: 0.3519\n",
      "[CV] END learning_rate=0.0016535051383872372, n_hidden=2, n_neurons=70; total time=  10.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3341 - val_loss: 347.0064\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 2.7335 - val_loss: 705.2772\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 7.4583 - val_loss: 4575.3618\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 15.0690 - val_loss: 15509.4043\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 68.1053 - val_loss: 76813.1094\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 2853.0493 - val_loss: 335092.1875\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 3440.8933 - val_loss: 1510134.6250\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 50374.6055 - val_loss: 6751761.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 104367.2422 - val_loss: 30562662.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 1316600.8750 - val_loss: 149424288.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 2320115.7500 - val_loss: 660644544.0000\n",
      "121/121 [==============================] - 0s 520us/step - loss: 1748892.7500\n",
      "[CV] END learning_rate=0.018247961881920356, n_hidden=0, n_neurons=40; total time=   2.5s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9670 - val_loss: 7.8624\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.5169 - val_loss: 20.4696\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.5045 - val_loss: 24.3101\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.5097 - val_loss: 22.3710\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.5096 - val_loss: 21.8682\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.5089 - val_loss: 21.2819\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.5113 - val_loss: 19.9069\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.5102 - val_loss: 22.5045\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.5069 - val_loss: 20.1015\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.5087 - val_loss: 10.7139\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.5083 - val_loss: 19.7339\n",
      "121/121 [==============================] - 0s 518us/step - loss: 0.9651\n",
      "[CV] END learning_rate=0.018247961881920356, n_hidden=0, n_neurons=40; total time=   2.5s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2545 - val_loss: 161.1399\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 804us/step - loss: 0.6953 - val_loss: 25.3713\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 796us/step - loss: 1.1449 - val_loss: 549.3639\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 23.9305 - val_loss: 495.8207\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 1.1143 - val_loss: 2229.5793\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 7.6090 - val_loss: 1176.1396\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 4.1157 - val_loss: 1285.1996\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 34.8587 - val_loss: 1179.4684\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 9.5209 - val_loss: 1144.0831\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 1.3468 - val_loss: 152.7648\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 10.1315 - val_loss: 82.6303\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.6327 - val_loss: 0.8445\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.7237 - val_loss: 698.7521\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 8.0215 - val_loss: 406.5941\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 2.0831 - val_loss: 986.6260\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 34.1845 - val_loss: 798.2900\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 8.4570 - val_loss: 1057.6332\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 3.5594 - val_loss: 458.5973\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 22.9286 - val_loss: 280.7328\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 3.2499 - val_loss: 313.8563\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 3.9917 - val_loss: 512.9033\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 4.1436 - val_loss: 358.5378\n",
      "121/121 [==============================] - 0s 546us/step - loss: 0.6005\n",
      "[CV] END learning_rate=0.018247961881920356, n_hidden=0, n_neurons=40; total time=   4.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3972 - val_loss: 3.7134\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6017 - val_loss: 1.5069\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4933 - val_loss: 0.4490\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4382 - val_loss: 0.4063\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.3909\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3990 - val_loss: 0.3986\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3884 - val_loss: 0.3837\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.3954\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3750 - val_loss: 0.3955\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3706 - val_loss: 0.3834\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3662 - val_loss: 0.3762\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3616 - val_loss: 0.3772\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3579 - val_loss: 0.3678\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3561 - val_loss: 0.3760\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3525 - val_loss: 0.3693\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3500 - val_loss: 0.3745\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3469 - val_loss: 0.3700\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3449 - val_loss: 0.3640\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3415 - val_loss: 0.3594\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3396 - val_loss: 0.3578\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3373 - val_loss: 0.3738\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3355 - val_loss: 0.3672\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3331 - val_loss: 0.3675\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3319 - val_loss: 0.3414\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3290 - val_loss: 0.3566\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3273 - val_loss: 0.3401\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3266 - val_loss: 0.3488\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3249 - val_loss: 0.3612\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3231 - val_loss: 0.3573\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3215 - val_loss: 0.3350\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3210 - val_loss: 0.3238\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3198 - val_loss: 0.3258\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3178 - val_loss: 0.3403\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3167 - val_loss: 0.3439\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3156 - val_loss: 0.3260\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3148 - val_loss: 0.3447\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3137 - val_loss: 0.3187\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3128 - val_loss: 0.3537\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3118 - val_loss: 0.3260\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3113 - val_loss: 0.3426\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3099 - val_loss: 0.3248\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3086 - val_loss: 0.3202\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3084 - val_loss: 0.3189\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3069 - val_loss: 0.3345\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3069 - val_loss: 0.3141\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3054 - val_loss: 0.3284\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3045 - val_loss: 0.3348\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3044 - val_loss: 0.3310\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3028 - val_loss: 0.3120\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3027 - val_loss: 0.3327\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3013 - val_loss: 0.3446\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3005 - val_loss: 0.3473\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3005 - val_loss: 0.3375\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2995 - val_loss: 0.3078\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2978 - val_loss: 0.3284\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2980 - val_loss: 0.3147\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2960 - val_loss: 0.3557\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2961 - val_loss: 0.3170\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2951 - val_loss: 0.2975\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2942 - val_loss: 0.3288\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2944 - val_loss: 0.3757\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2926 - val_loss: 0.4541\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2940 - val_loss: 0.2990\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2918 - val_loss: 0.3987\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2905 - val_loss: 0.3944\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2917 - val_loss: 0.3044\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2915 - val_loss: 0.3617\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2900 - val_loss: 0.3290\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2892 - val_loss: 0.3147\n",
      "121/121 [==============================] - 0s 720us/step - loss: 0.3220\n",
      "[CV] END learning_rate=0.004545509695633103, n_hidden=3, n_neurons=30; total time=  20.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0126 - val_loss: 4.1222\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5498 - val_loss: 0.5166\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4711 - val_loss: 0.4468\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4303 - val_loss: 0.4279\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4048 - val_loss: 0.4222\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3906 - val_loss: 0.3777\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3810 - val_loss: 0.3580\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3741 - val_loss: 0.3561\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3678 - val_loss: 0.3803\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3634 - val_loss: 0.3830\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.4198\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3549 - val_loss: 0.4420\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3521 - val_loss: 0.4832\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.5628\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.5652\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3445 - val_loss: 0.5688\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.6056\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3404 - val_loss: 0.6788\n",
      "121/121 [==============================] - 0s 720us/step - loss: 0.3666\n",
      "[CV] END learning_rate=0.004545509695633103, n_hidden=3, n_neurons=30; total time=   5.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1562 - val_loss: 0.8025\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5424 - val_loss: 0.5654\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4660 - val_loss: 0.4537\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4318 - val_loss: 0.5218\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4143 - val_loss: 0.3825\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4015 - val_loss: 0.4395\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3931 - val_loss: 0.4078\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3861 - val_loss: 0.4153\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3798 - val_loss: 0.4567\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3783 - val_loss: 0.3579\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3726 - val_loss: 0.4395\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3699 - val_loss: 0.4238\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3673 - val_loss: 0.3629\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3645 - val_loss: 0.4313\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3625 - val_loss: 0.3706\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3595 - val_loss: 0.4296\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3577 - val_loss: 0.3770\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3553 - val_loss: 0.3729\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3535 - val_loss: 0.4353\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.3675\n",
      "121/121 [==============================] - 0s 739us/step - loss: 0.3487\n",
      "[CV] END learning_rate=0.004545509695633103, n_hidden=3, n_neurons=30; total time=   6.2s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.5579 - val_loss: 0.8146\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.7014 - val_loss: 0.6974\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.6323 - val_loss: 0.5795\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.5846 - val_loss: 0.5398\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.5479 - val_loss: 0.5052\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.5185 - val_loss: 0.4859\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.4955 - val_loss: 0.4618\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.4793 - val_loss: 0.4649\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.4661 - val_loss: 0.4507\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4562 - val_loss: 0.4253\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.4483 - val_loss: 0.4146\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4415 - val_loss: 0.4089\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.4355 - val_loss: 0.4031\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.4310 - val_loss: 0.4231\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.4265 - val_loss: 0.4055\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.4230 - val_loss: 0.4007\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4195 - val_loss: 0.4162\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.4167 - val_loss: 0.3902\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.4135 - val_loss: 0.4040\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4114 - val_loss: 0.3903\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4093 - val_loss: 0.4028\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4066 - val_loss: 0.3873\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4048 - val_loss: 0.3994\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4030 - val_loss: 0.3743\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.4010 - val_loss: 0.4111\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3996 - val_loss: 0.3715\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3979 - val_loss: 0.3961\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3960 - val_loss: 0.3817\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3943 - val_loss: 0.3967\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3933 - val_loss: 0.3677\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3913 - val_loss: 0.3665\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3903 - val_loss: 0.3642\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3887 - val_loss: 0.3866\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3873 - val_loss: 0.3718\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3861 - val_loss: 0.3603\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3846 - val_loss: 0.3915\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3836 - val_loss: 0.3652\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3822 - val_loss: 0.4184\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3812 - val_loss: 0.3575\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3800 - val_loss: 0.3951\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3791 - val_loss: 0.3583\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3773 - val_loss: 0.3551\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.3769 - val_loss: 0.3541\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3753 - val_loss: 0.4021\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3752 - val_loss: 0.3524\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.3679\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3727 - val_loss: 0.3712\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3720 - val_loss: 0.3622\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.3709 - val_loss: 0.3508\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.3698 - val_loss: 0.3981\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3696 - val_loss: 0.3634\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3685 - val_loss: 0.3512\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3678 - val_loss: 0.3615\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3671 - val_loss: 0.3473\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3656 - val_loss: 0.3947\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3662 - val_loss: 0.3487\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3646 - val_loss: 0.4216\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3642 - val_loss: 0.3536\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3632 - val_loss: 0.3446\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3622 - val_loss: 0.3536\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3617 - val_loss: 0.3653\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3610 - val_loss: 0.3832\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3607 - val_loss: 0.3491\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3596 - val_loss: 0.4093\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.3597 - val_loss: 0.3507\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3591 - val_loss: 0.3463\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3581 - val_loss: 0.3857\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3580 - val_loss: 0.3431\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3568 - val_loss: 0.4034\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3572 - val_loss: 0.3409\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3558 - val_loss: 0.4020\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.3560 - val_loss: 0.3457\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3552 - val_loss: 0.3405\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3544 - val_loss: 0.4038\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3548 - val_loss: 0.3416\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3530 - val_loss: 0.4017\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3636\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3529 - val_loss: 0.4960\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.3535 - val_loss: 0.3366\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.4109\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3519 - val_loss: 0.3370\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3507 - val_loss: 0.3932\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3509 - val_loss: 0.3361\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.3498 - val_loss: 0.3650\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3497 - val_loss: 0.3359\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3487 - val_loss: 0.3525\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3490 - val_loss: 0.3441\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3485 - val_loss: 0.3444\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3337\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3472 - val_loss: 0.4201\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3809\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3475 - val_loss: 0.4138\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3468 - val_loss: 0.3386\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3463 - val_loss: 0.3586\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3458 - val_loss: 0.3385\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3450 - val_loss: 0.3498\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3450 - val_loss: 0.3348\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3447 - val_loss: 0.3480\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3438 - val_loss: 0.3595\n",
      "121/121 [==============================] - 0s 637us/step - loss: 0.3691\n",
      "[CV] END learning_rate=0.0020587676114196553, n_hidden=1, n_neurons=49; total time=  24.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.6381 - val_loss: 11.2160\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.7487 - val_loss: 3.7498\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.6643 - val_loss: 0.9871\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.6067 - val_loss: 0.6044\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5624 - val_loss: 1.0679\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5294 - val_loss: 1.5045\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.5041 - val_loss: 1.8856\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.4843 - val_loss: 2.0953\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.4689 - val_loss: 1.8828\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.4575 - val_loss: 1.6755\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.4477 - val_loss: 1.4525\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4397 - val_loss: 1.3505\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4334 - val_loss: 1.1529\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.4284 - val_loss: 0.8031\n",
      "121/121 [==============================] - 0s 652us/step - loss: 0.4511\n",
      "[CV] END learning_rate=0.0020587676114196553, n_hidden=1, n_neurons=49; total time=   3.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.4668 - val_loss: 2.3894\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.7725 - val_loss: 1.2950\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.6822 - val_loss: 0.8775\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.6372 - val_loss: 0.6097\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.5897 - val_loss: 0.6456\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5587 - val_loss: 0.5234\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.5313 - val_loss: 0.5047\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.5111 - val_loss: 0.6261\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.4960 - val_loss: 0.5782\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4803 - val_loss: 0.4695\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4690 - val_loss: 0.4375\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4584 - val_loss: 0.4342\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4495 - val_loss: 0.4220\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.4439 - val_loss: 0.4516\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.4380 - val_loss: 0.4150\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.4323 - val_loss: 0.4365\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4283 - val_loss: 0.4509\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4246 - val_loss: 0.4107\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.4214 - val_loss: 0.3891\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4174 - val_loss: 0.3939\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4150 - val_loss: 0.3826\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4126 - val_loss: 0.3825\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4099 - val_loss: 0.3808\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.4072 - val_loss: 0.3976\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.4061 - val_loss: 0.4225\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.4034 - val_loss: 0.3792\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4013 - val_loss: 0.4204\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.4009 - val_loss: 0.3880\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3981 - val_loss: 0.5602\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3994 - val_loss: 0.3792\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3951 - val_loss: 0.3837\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3940 - val_loss: 0.4049\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3926 - val_loss: 0.3811\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3913 - val_loss: 0.4121\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3914 - val_loss: 0.3609\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3885 - val_loss: 0.3775\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3886 - val_loss: 0.4213\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3873 - val_loss: 0.6480\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3892 - val_loss: 0.4835\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.3851 - val_loss: 0.6385\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3874 - val_loss: 0.3574\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3820 - val_loss: 0.5457\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3830 - val_loss: 0.6472\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.3830 - val_loss: 0.9535\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3887 - val_loss: 0.5661\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3812 - val_loss: 0.9310\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.3829 - val_loss: 1.0067\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3836 - val_loss: 0.6622\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.3807 - val_loss: 0.3504\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3759 - val_loss: 0.5012\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3770 - val_loss: 0.3537\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3741 - val_loss: 0.5051\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3769 - val_loss: 0.4500\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.3745 - val_loss: 0.3737\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3730 - val_loss: 0.3815\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3721 - val_loss: 0.4441\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3723 - val_loss: 0.3548\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3712 - val_loss: 0.3773\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3709 - val_loss: 0.3534\n",
      "121/121 [==============================] - 0s 602us/step - loss: 0.3668\n",
      "[CV] END learning_rate=0.0020587676114196553, n_hidden=1, n_neurons=49; total time=  14.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0633 - val_loss: 0.5242\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5212 - val_loss: 4.0015\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4716 - val_loss: 1.3068\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.4887\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3845 - val_loss: 0.3901\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3699 - val_loss: 0.4205\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3612 - val_loss: 0.3709\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.4222\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3490 - val_loss: 0.4016\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3452 - val_loss: 0.3966\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3425 - val_loss: 0.3714\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3376 - val_loss: 0.3646\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3335 - val_loss: 0.3816\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3326 - val_loss: 0.3698\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3283 - val_loss: 0.3676\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3270 - val_loss: 0.3570\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3237 - val_loss: 0.3791\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3226 - val_loss: 0.3405\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3193 - val_loss: 0.3701\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3178 - val_loss: 0.3400\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3159 - val_loss: 0.3569\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3144 - val_loss: 0.3361\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3119 - val_loss: 0.3693\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3124 - val_loss: 0.3263\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3089 - val_loss: 0.4344\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3064 - val_loss: 0.3084\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3057 - val_loss: 0.3801\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3040 - val_loss: 0.3565\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3021 - val_loss: 0.3927\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3011 - val_loss: 0.3595\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3015 - val_loss: 0.3622\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2984 - val_loss: 0.2935\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2955 - val_loss: 0.3314\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2936 - val_loss: 0.3113\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2922 - val_loss: 0.2953\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2913 - val_loss: 0.3976\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2908 - val_loss: 0.4771\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2933 - val_loss: 0.6441\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2920 - val_loss: 0.3016\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2887 - val_loss: 0.3737\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2867 - val_loss: 0.2896\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2838 - val_loss: 0.2919\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2831 - val_loss: 0.2846\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2808 - val_loss: 0.3039\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2806 - val_loss: 0.2958\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2792 - val_loss: 0.4106\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2782 - val_loss: 0.2821\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2775 - val_loss: 0.5397\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2821 - val_loss: 0.5656\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2825 - val_loss: 0.6184\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2801 - val_loss: 0.3072\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2740 - val_loss: 0.3261\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2744 - val_loss: 0.2985\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2718 - val_loss: 0.2948\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2713 - val_loss: 0.3077\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2716 - val_loss: 0.2797\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2689 - val_loss: 0.3294\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2695 - val_loss: 0.3021\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2683 - val_loss: 0.2738\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2661 - val_loss: 0.2944\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2666 - val_loss: 0.3369\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2646 - val_loss: 0.3163\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2643 - val_loss: 0.4014\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2637 - val_loss: 0.3108\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2611 - val_loss: 0.4583\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2666 - val_loss: 0.5345\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2684 - val_loss: 0.4788\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2641 - val_loss: 0.2767\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2623 - val_loss: 0.3048\n",
      "121/121 [==============================] - 0s 965us/step - loss: 0.2983\n",
      "[CV] END learning_rate=0.0058036029342010235, n_hidden=3, n_neurons=74; total time=  27.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0616 - val_loss: 0.7202\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5589 - val_loss: 0.6439\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4528 - val_loss: 0.7953\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4004 - val_loss: 0.6901\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3764 - val_loss: 0.4450\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3639 - val_loss: 0.3542\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3564 - val_loss: 0.4737\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3511 - val_loss: 0.6154\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3455 - val_loss: 0.8492\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3423 - val_loss: 0.7788\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3379 - val_loss: 0.8545\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3347 - val_loss: 0.8623\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3319 - val_loss: 1.0098\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3306 - val_loss: 0.9192\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3282 - val_loss: 0.8907\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3248 - val_loss: 0.8483\n",
      "121/121 [==============================] - 0s 945us/step - loss: 0.3490\n",
      "[CV] END learning_rate=0.0058036029342010235, n_hidden=3, n_neurons=74; total time=   7.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9621 - val_loss: 2.0435\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 1.6035\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4533 - val_loss: 0.6984\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4146 - val_loss: 0.6957\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3927 - val_loss: 0.5648\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3815 - val_loss: 0.4990\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3727 - val_loss: 0.3468\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3642 - val_loss: 0.3634\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3589 - val_loss: 0.3779\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3562 - val_loss: 0.3907\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3503 - val_loss: 0.6172\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3511 - val_loss: 0.5019\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3450 - val_loss: 0.4275\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3406 - val_loss: 0.3533\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3373 - val_loss: 0.3485\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3333 - val_loss: 0.4492\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3342 - val_loss: 0.4695\n",
      "121/121 [==============================] - 0s 940us/step - loss: 0.3374\n",
      "[CV] END learning_rate=0.0058036029342010235, n_hidden=3, n_neurons=74; total time=   7.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.2654 - val_loss: 5.0160\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5755 - val_loss: 4.1248\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4780 - val_loss: 0.8244\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4065 - val_loss: 0.3663\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3772 - val_loss: 0.3611\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3647 - val_loss: 0.3772\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3566 - val_loss: 0.3768\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3505 - val_loss: 0.3984\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3459 - val_loss: 0.3914\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3430 - val_loss: 0.3854\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3400 - val_loss: 0.3676\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3355 - val_loss: 0.3611\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3319 - val_loss: 0.3768\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3308 - val_loss: 0.3729\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3274 - val_loss: 0.3521\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3250 - val_loss: 0.3851\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3228 - val_loss: 0.3338\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3207 - val_loss: 0.3992\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3175 - val_loss: 0.3528\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3150 - val_loss: 0.3693\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3135 - val_loss: 0.3499\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3114 - val_loss: 0.4038\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3095 - val_loss: 0.3151\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3090 - val_loss: 0.3086\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3047 - val_loss: 0.3445\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3035 - val_loss: 0.3125\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3033 - val_loss: 0.3162\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3016 - val_loss: 0.3678\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2989 - val_loss: 0.3394\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2970 - val_loss: 0.3002\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2967 - val_loss: 0.3045\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2944 - val_loss: 0.3036\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2923 - val_loss: 0.3480\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2909 - val_loss: 0.3194\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2897 - val_loss: 0.2924\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2886 - val_loss: 0.3333\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2879 - val_loss: 0.3102\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2877 - val_loss: 0.3324\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2854 - val_loss: 0.3117\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2844 - val_loss: 0.3419\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2832 - val_loss: 0.3000\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2814 - val_loss: 0.2972\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2816 - val_loss: 0.3262\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2795 - val_loss: 0.3462\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2788 - val_loss: 0.2912\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2781 - val_loss: 0.3273\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2761 - val_loss: 0.3121\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2769 - val_loss: 0.2885\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2740 - val_loss: 0.2831\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2740 - val_loss: 0.3644\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2730 - val_loss: 0.3131\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2709 - val_loss: 0.3806\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2718 - val_loss: 0.3001\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2697 - val_loss: 0.3054\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2684 - val_loss: 0.3432\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2693 - val_loss: 0.2951\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2678 - val_loss: 0.3439\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2696 - val_loss: 0.3129\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2674 - val_loss: 0.2811\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2653 - val_loss: 0.3145\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2668 - val_loss: 0.3660\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2640 - val_loss: 0.2957\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2634 - val_loss: 0.4022\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2636 - val_loss: 0.2967\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2610 - val_loss: 0.3550\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2642 - val_loss: 0.3113\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2637 - val_loss: 0.3196\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2616 - val_loss: 0.3121\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2618 - val_loss: 0.3268\n",
      "121/121 [==============================] - 0s 758us/step - loss: 0.3009\n",
      "[CV] END learning_rate=0.0059640580092043885, n_hidden=3, n_neurons=80; total time=  27.3s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9343 - val_loss: 0.8310\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4992 - val_loss: 0.4744\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4242 - val_loss: 0.6362\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3928 - val_loss: 0.6887\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3739 - val_loss: 0.7429\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3622 - val_loss: 0.9273\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3550 - val_loss: 1.1678\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 1.2115\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3427 - val_loss: 1.3450\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3393 - val_loss: 1.0957\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3343 - val_loss: 1.1340\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3310 - val_loss: 1.0655\n",
      "121/121 [==============================] - 0s 822us/step - loss: 0.3613\n",
      "[CV] END learning_rate=0.0059640580092043885, n_hidden=3, n_neurons=80; total time=   4.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9533 - val_loss: 2.1219\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5071 - val_loss: 5.2662\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4684 - val_loss: 2.3605\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4275 - val_loss: 0.4664\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3866 - val_loss: 0.4698\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3762 - val_loss: 0.4313\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3663 - val_loss: 0.3742\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3590 - val_loss: 0.3656\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3520 - val_loss: 0.4049\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.3563\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.4001\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3408 - val_loss: 0.3436\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3372 - val_loss: 0.3294\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3347 - val_loss: 0.3804\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3319 - val_loss: 0.3256\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3279 - val_loss: 0.3554\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3260 - val_loss: 0.3209\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3227 - val_loss: 0.3301\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3206 - val_loss: 0.4124\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3188 - val_loss: 0.3147\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3181 - val_loss: 0.4170\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3183 - val_loss: 0.3247\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3143 - val_loss: 0.3863\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3127 - val_loss: 0.3431\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3104 - val_loss: 0.3114\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3077 - val_loss: 0.3261\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3051 - val_loss: 0.3728\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3064 - val_loss: 0.3224\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3076 - val_loss: 0.3858\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3057 - val_loss: 0.3054\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3011 - val_loss: 0.3066\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2990 - val_loss: 0.3293\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2995 - val_loss: 0.3075\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2960 - val_loss: 0.3095\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2946 - val_loss: 0.2975\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2926 - val_loss: 0.3887\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2923 - val_loss: 0.4197\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2948 - val_loss: 0.3902\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2912 - val_loss: 0.2986\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2902 - val_loss: 0.3953\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2885 - val_loss: 0.2949\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2871 - val_loss: 0.3481\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2860 - val_loss: 0.2956\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2837 - val_loss: 0.3291\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2838 - val_loss: 0.2990\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2818 - val_loss: 0.3076\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2805 - val_loss: 0.2856\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2805 - val_loss: 0.2879\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2777 - val_loss: 0.3894\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2785 - val_loss: 0.2857\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2761 - val_loss: 0.4759\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2792 - val_loss: 0.2969\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2760 - val_loss: 0.3609\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2749 - val_loss: 0.3596\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2740 - val_loss: 1.1946\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2753 - val_loss: 0.3551\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2738 - val_loss: 0.4557\n",
      "121/121 [==============================] - 0s 802us/step - loss: 0.2944\n",
      "[CV] END learning_rate=0.0059640580092043885, n_hidden=3, n_neurons=80; total time=  20.5s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1748 - val_loss: 2.2399\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6379 - val_loss: 15.8107\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6486 - val_loss: 5.8945\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5168 - val_loss: 3.4256\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4611 - val_loss: 0.5365\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4148 - val_loss: 0.3951\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3997 - val_loss: 0.3929\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3893 - val_loss: 0.4112\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3822 - val_loss: 0.4043\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.4012\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.3787\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3652 - val_loss: 0.3682\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3608 - val_loss: 0.3838\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3580 - val_loss: 0.3880\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3544 - val_loss: 0.3697\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3515 - val_loss: 0.3813\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3863\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3762\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3439 - val_loss: 0.3839\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3424 - val_loss: 0.3728\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3402 - val_loss: 0.3730\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3392 - val_loss: 0.3688\n",
      "121/121 [==============================] - 0s 739us/step - loss: 0.3577\n",
      "[CV] END learning_rate=0.004591455636549436, n_hidden=2, n_neurons=59; total time=   6.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9900 - val_loss: 0.7280\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5451 - val_loss: 1.1145\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4745 - val_loss: 1.7014\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4366 - val_loss: 1.5315\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4126 - val_loss: 1.1045\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3978 - val_loss: 0.5412\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3873 - val_loss: 0.3802\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3792 - val_loss: 0.3565\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3719 - val_loss: 0.4297\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3671 - val_loss: 0.4038\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3616 - val_loss: 0.4414\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3571 - val_loss: 0.4879\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3546 - val_loss: 0.5733\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3520 - val_loss: 0.6253\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.5954\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.6038\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.5393\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3431 - val_loss: 0.6004\n",
      "121/121 [==============================] - 0s 654us/step - loss: 0.3597\n",
      "[CV] END learning_rate=0.004591455636549436, n_hidden=2, n_neurons=59; total time=   5.3s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.1118 - val_loss: 1.5060\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6104 - val_loss: 21.1634\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5983 - val_loss: 18.9614\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7141 - val_loss: 1.7545\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4479 - val_loss: 0.6174\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.5396\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4069 - val_loss: 0.4894\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3977 - val_loss: 0.4622\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3903 - val_loss: 0.4557\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3855 - val_loss: 0.4263\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3797 - val_loss: 0.4310\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3755 - val_loss: 0.4214\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3709 - val_loss: 0.4116\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3686 - val_loss: 0.4144\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3653 - val_loss: 0.4014\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3625 - val_loss: 0.4188\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3598 - val_loss: 0.3913\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3581 - val_loss: 0.3905\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.4247\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3538 - val_loss: 0.3942\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.4186\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3518 - val_loss: 0.3709\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.3761\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.4124\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3605\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3449 - val_loss: 0.3673\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3432 - val_loss: 0.4044\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3422 - val_loss: 0.3672\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3943\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3420 - val_loss: 0.3875\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3385 - val_loss: 0.4057\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3375 - val_loss: 0.3940\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3361 - val_loss: 0.4399\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3355 - val_loss: 0.3672\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3350 - val_loss: 0.3603\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3324 - val_loss: 0.3748\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3325 - val_loss: 0.3425\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3313 - val_loss: 0.3615\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3305 - val_loss: 0.3530\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3291 - val_loss: 0.3845\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3282 - val_loss: 0.3822\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3275 - val_loss: 0.4218\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3271 - val_loss: 0.3221\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3260 - val_loss: 0.3321\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3263 - val_loss: 0.3483\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3254 - val_loss: 0.3671\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3226 - val_loss: 0.3281\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3228 - val_loss: 0.3386\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3206 - val_loss: 0.3955\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3206 - val_loss: 0.3694\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3193 - val_loss: 0.4120\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3199 - val_loss: 0.3369\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3187 - val_loss: 0.3322\n",
      "121/121 [==============================] - 0s 638us/step - loss: 0.3273\n",
      "[CV] END learning_rate=0.004591455636549436, n_hidden=2, n_neurons=59; total time=  14.5s\n",
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8787 - val_loss: 1.0609\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4420 - val_loss: 0.4771\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3938 - val_loss: 0.9905\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3786 - val_loss: 0.3514\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3649 - val_loss: 0.3587\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3552 - val_loss: 0.3609\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3374\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3441 - val_loss: 0.3346\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3386 - val_loss: 0.3360\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3344 - val_loss: 0.3832\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3329 - val_loss: 0.4854\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3306 - val_loss: 0.6479\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3259 - val_loss: 0.3114\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3201 - val_loss: 0.3111\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3172 - val_loss: 0.3069\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3140 - val_loss: 0.2969\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3111 - val_loss: 0.3766\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3088 - val_loss: 0.3036\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3072 - val_loss: 0.3078\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3042 - val_loss: 0.3876\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3021 - val_loss: 0.2962\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2988 - val_loss: 0.3552\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2976 - val_loss: 0.3406\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2960 - val_loss: 0.3430\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2942 - val_loss: 0.3659\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2928 - val_loss: 0.3971\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2920 - val_loss: 0.5684\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2927 - val_loss: 0.5445\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2898 - val_loss: 0.3105\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2870 - val_loss: 0.4133\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2863 - val_loss: 0.2894\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2825 - val_loss: 0.3320\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2819 - val_loss: 0.2783\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2795 - val_loss: 0.2976\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2784 - val_loss: 0.3175\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2787 - val_loss: 0.2836\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2760 - val_loss: 0.3459\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2746 - val_loss: 0.3000\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2771 - val_loss: 0.3371\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2738 - val_loss: 0.2730\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2745 - val_loss: 0.3620\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2741 - val_loss: 0.2763\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2688 - val_loss: 0.3514\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2692 - val_loss: 0.3016\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2679 - val_loss: 0.2737\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2680 - val_loss: 0.3025\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2680 - val_loss: 0.2794\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2661 - val_loss: 0.3653\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2666 - val_loss: 0.3123\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2640 - val_loss: 0.2703\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2635 - val_loss: 0.2933\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2640 - val_loss: 0.4058\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2636 - val_loss: 0.5729\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2641 - val_loss: 0.3773\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2620 - val_loss: 0.3134\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2634 - val_loss: 0.2738\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2611 - val_loss: 0.2677\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2585 - val_loss: 0.3753\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2600 - val_loss: 0.2655\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2573 - val_loss: 0.2732\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2550 - val_loss: 0.3059\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2564 - val_loss: 0.3185\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2572 - val_loss: 0.3944\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2610 - val_loss: 0.4003\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2553 - val_loss: 0.4514\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2554 - val_loss: 0.4214\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2558 - val_loss: 0.4396\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2526 - val_loss: 0.5385\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2559 - val_loss: 0.5321\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.2869\n",
      "162/162 [==============================] - 0s 920us/step - loss: 0.2869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2869483530521393"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear Keras session to release resources and avoid conflicts with previous models\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)  # Set seed for NumPy random functions\n",
    "tf.random.set_seed(42)  # Set seed for TensorFlow random functions\n",
    "\n",
    "# Function to build a Keras model with configurable hyperparameters\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    # Add hidden layers with specified number of neurons and ReLU activation\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    \n",
    "    # Output layer for regression task\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    # Compile the model with mean squared error loss and SGD optimizer\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a KerasRegressor using the build_model function\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "\n",
    "# Import necessary libraries\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define parameter distributions for hyperparameter tuning\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],  # Number of hidden layers\n",
    "    \"n_neurons\": np.arange(1, 100).tolist(),  # Number of neurons per hidden layer\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2).rvs(1000).tolist(),  # Learning rate with reciprocal distribution\n",
    "}\n",
    "\n",
    "# Create a RandomizedSearchCV object\n",
    "rnd_search_cv = RandomizedSearchCV(\n",
    "    keras_reg,  # Keras model to be tuned\n",
    "    param_distribs,  # Hyperparameter distributions\n",
    "    n_iter=10,  # Number of parameter settings sampled\n",
    "    cv=3,  # Number of cross-validation folds\n",
    "    verbose=2  # Verbosity level: 0 - silent, 1 - progress bar, 2 - one line per fit\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV on the training data with early stopping\n",
    "rnd_search_cv.fit(\n",
    "    X_train, y_train,  # Training data and labels\n",
    "    epochs=100,  # Number of training epochs\n",
    "    validation_data=(X_valid, y_valid),  # Validation data and labels\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)]  # Early stopping to prevent overfitting\n",
    ")\n",
    "\n",
    "# Display the best hyperparameters\n",
    "rnd_search_cv.best_params_\n",
    "\n",
    "# Display the best score (mean cross-validated score)\n",
    "rnd_search_cv.best_score_\n",
    "\n",
    "# Display the best estimator (best model)\n",
    "rnd_search_cv.best_estimator_\n",
    "\n",
    "# Evaluate the best estimator on the test set\n",
    "rnd_search_cv.score(X_test, y_test)\n",
    "\n",
    "# Get the best model from the search\n",
    "model = rnd_search_cv.best_estimator_.model\n",
    "\n",
    "# Display the model architecture\n",
    "model\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
