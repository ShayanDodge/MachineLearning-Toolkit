{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Using TensorFlow like NumPy`\n",
    "* TensorFlow can be used in a way similar to NumPy for numerical computations, array manipulations, and mathematical operations. \n",
    "* TensorFlow like NumPy allows you to benefit from TensorFlow's powerful features such as `automatic differentiation`, `GPU acceleration`, `distributed computing`, and seamless integration with `deep learning frameworks`. This approach is ideal for scientific computing and building complex machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and operations\n",
    "\n",
    "* In TensorFlow, `tensors` are fundamental objects that represent data in the computation graph. They flow through operations, allowing for efficient computation and differentiation.\n",
    "\n",
    "* Tensors are central to TensorFlow's design, enabling efficient and scalable computation in deep learning models. By understanding how to create and manipulate tensors, you can effectively work with TensorFlow's API to build and train sophisticated machine learning models.\n",
    "\n",
    "* These tensors will be important when we create `custom cost functions`, `custom metrics`, `custom layers`, and more, so letâ€™s see how to create and manipulate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating Tensors\n",
    "* You can create tensors in TensorFlow just like NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "<dtype: 'int32'>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a constant tensor (Immutable:)\n",
    "a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# Create a variable tensor (can be updated)\n",
    "b = tf.Variable([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# Accessing shape, dtype, and ndim\n",
    "tensor = tf.constant([[1, 2], [3, 4]])\n",
    "# shape: Attribute that returns the shape of the tensor.\n",
    "print(tensor.shape)  # Output: (2, 2)\n",
    "# dtype: Attribute that returns the data type of the tensor.\n",
    "print(tensor.dtype)  # Output: <dtype: 'int32'>\n",
    "# ndim: Method that returns the number of dimensions (or rank) of the tensor.\n",
    "print(tensor.ndim)  # Output: 2\n",
    "\n",
    "# Create tensors with specified shape and dtype\n",
    "zeros_tensor = tf.zeros((3, 3), dtype=tf.float32)\n",
    "ones_tensor = tf.ones((2, 2), dtype=tf.int32)\n",
    "random_tensor = tf.random.normal((3, 3), mean=0.0, stddev=1.0, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Indexing\n",
    "* Indexing refers to accessing individual elements within a tensor using specific indices :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a 2D tensor\n",
    "tensor_2d = tf.constant([[1, 2, 3],\n",
    "                          [4, 5, 6]])\n",
    "\n",
    "# Accessing individual elements\n",
    "print(tensor_2d[0, 0])  # Output: 1\n",
    "print(tensor_2d[1, 2])  # Output: 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Slicing\n",
    "* Slicing allows you to extract sub-tensors (slices) from a larger tensor based on specific ranges of indices along each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
      "tf.Tensor([2 5], shape=(2,), dtype=int32)\n",
      "tf.Tensor([[4 5]], shape=(1, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Slicing along rows and columns\n",
    "print(tensor_2d[0, :])  # Slice the first row: [1, 2, 3]\n",
    "print(tensor_2d[:, 1])  # Slice the second column: [2, 5, 8]\n",
    "\n",
    "# Slicing sub-tensors\n",
    "print(tensor_2d[1:, :2])  # Slice rows from index 1 onwards and columns up to index 2:\n",
    "                          # [[4, 5],\n",
    "                          #  [7, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[10 11 12]\n",
      " [13 14 15]\n",
      " [16 17 18]], shape=(3, 3), dtype=int32)\n",
      "tf.Tensor([ 5 14 23], shape=(3,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  4  7]\n",
      " [10 13 16]], shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a 3D tensor (3x3x3)\n",
    "tensor_3d = tf.Variable([\n",
    "    [[1, 2, 3],\n",
    "     [4, 5, 6],\n",
    "     [7, 8, 9]],\n",
    "\n",
    "    [[10, 11, 12],\n",
    "     [13, 14, 15],\n",
    "     [16, 17, 18]],\n",
    "\n",
    "    [[19, 20, 21],\n",
    "     [22, 23, 24],\n",
    "     [25, 26, 27]]\n",
    "])\n",
    "\n",
    "# Slicing along different dimensions\n",
    "print(tensor_3d[1, :, :])  # Slice the entire 2nd 'layer' (2nd matrix)\n",
    "print(tensor_3d[:, 1, 1])  # Slice the (1, 1) element from each 'layer'\n",
    "print(tensor_3d[0:2, :, 0])  # Slice the first two 'layers' and extract the first column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modification of elements\n",
    "* The tensorflow does not support item assignment occurs when you attempt to directly modify an individual element of a TensorFlow tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Mutability and In-Place Modification\n",
    "* One of the main features of tf.Variable is its mutability. \n",
    "* You can modify the value of a variable using the `assign()`, `assign_add()`, or `assign_sub()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After assign(new_value): 10.0\n",
      "After assign_add(increment_by): 12.0\n",
      "After assign_sub(decrement_by): 9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a TensorFlow variable initialized with a scalar value\n",
    "var = tf.Variable(5.0)\n",
    "\n",
    "# Example: Using assign() to set a new value\n",
    "new_value = 10.0\n",
    "var.assign(new_value)\n",
    "\n",
    "print(\"After assign(new_value):\", var.numpy())  # Output: 10.0\n",
    "\n",
    "# Example: Using assign_add() to increment the variable\n",
    "increment_by = 2.0\n",
    "var.assign_add(increment_by)\n",
    "\n",
    "print(\"After assign_add(increment_by):\", var.numpy())  # Output: 12.0\n",
    "\n",
    "# Example: Using assign_sub() to decrement the variable\n",
    "decrement_by = 3.0\n",
    "var.assign_sub(decrement_by)\n",
    "\n",
    "print(\"After assign_sub(decrement_by):\", var.numpy())  # Output: 9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Modifying Individual Elements\n",
    "* You can modify individual elements (or slices) of a tf.Variable using the `assign()` method on specific indices or slices.\n",
    "* Direct item assignment `(var[0] = value)` does `not work` on tf.Variable objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 3 4]\n",
      "[1 3 3 4]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a tf.Variable\n",
    "var = tf.Variable([1, 2, 3, 4])\n",
    "\n",
    "# Modify individual elements using assign() method\n",
    "var[1].assign(5)  # Modify the second element to 5\n",
    "print(var.numpy())  # Output: [1, 5, 3, 4]\n",
    "\n",
    "# Modify range of elements using assign() method\n",
    "var[1:3].assign(3)  # Modify the second element to 5\n",
    "print(var.numpy())  # Output: [1, 5, 3, 4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Advanced Modification with `scatter_nd_update()`\n",
    "* TensorFlow provides advanced method like `scatter_nd_update()` to modify specific elements or slices of a variable based on indices.\n",
    "* These methods are useful for updating variable values in a more complex and efficient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scatter_nd_update(indices_sparse, updates_sparse): [0. 5. 0. 7.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a TensorFlow variable initialized with zeros\n",
    "var = tf.Variable([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "# Define sparse indices and updates\n",
    "indices_sparse = tf.constant([[1], [3]])  # Define sparse indices as a 2D tensor\n",
    "updates_sparse = tf.constant([5.0, 7.0])  # Define corresponding sparse updates\n",
    "\n",
    "# Perform sparse scatter_nd_update\n",
    "updated_var = tf.tensor_scatter_nd_update(var, indices_sparse, updates_sparse)\n",
    "\n",
    "print(\"After scatter_nd_update(indices_sparse, updates_sparse):\", updated_var.numpy())\n",
    "# Output: [0.0, 5.0, 0.0, 7.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scatter_nd_update(indices_sparse, updates_sparse):\n",
      "[[0. 5. 0.]\n",
      " [0. 0. 7.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a TensorFlow variable initialized with zeros (2D tensor)\n",
    "var = tf.Variable([[0.0, 0.0, 0.0],\n",
    "                   [0.0, 0.0, 0.0],\n",
    "                   [0.0, 0.0, 0.0]])\n",
    "\n",
    "# Define sparse indices and updates for a 2D tensor\n",
    "indices_sparse = tf.constant([[0, 1],   # Update element at row 0, column 1\n",
    "                              [1, 2]])  # Update element at row 1, column 2\n",
    "\n",
    "updates_sparse = tf.constant([5.0, 7.0])  # Corresponding sparse update values\n",
    "\n",
    "# Perform sparse scatter_nd_update on the 2D tensor\n",
    "updated_var = tf.tensor_scatter_nd_update(var, indices_sparse, updates_sparse)\n",
    "\n",
    "print(\"After scatter_nd_update(indices_sparse, updates_sparse):\")\n",
    "print(updated_var.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After modifying the range of elements:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 5. 7. 3.]\n",
      " [0. 1. 2. 4.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a TensorFlow variable initialized with zeros (2D tensor: 3x4 matrix)\n",
    "var = tf.Variable([[0.0, 0.0, 0.0, 0.0],\n",
    "                   [0.0, 0.0, 0.0, 0.0],\n",
    "                   [0.0, 0.0, 0.0, 0.0]])\n",
    "\n",
    "# Define the range of row and column indices to update (slicing)\n",
    "start_row = 1\n",
    "end_row = 3\n",
    "start_col = 1\n",
    "end_col = 4\n",
    "\n",
    "# Generate new values for the specified range (2D tensor: corresponding submatrix)\n",
    "new_values = tf.constant([[5.0, 7.0, 3.0],\n",
    "                          [1.0, 2.0, 4.0]])\n",
    "\n",
    "# Calculate the dimensions of the new values tensor\n",
    "num_rows, num_cols = new_values.shape\n",
    "\n",
    "# Generate indices for the range to update using meshgrid and stack\n",
    "rows, cols = tf.meshgrid(tf.range(start_row, end_row), tf.range(start_col, end_col), indexing='ij')\n",
    "indices_sparse = tf.stack([rows, cols], axis=-1)\n",
    "\n",
    "# Perform sparse scatter_nd_update to modify the specified submatrix range\n",
    "updated_var = tf.tensor_scatter_nd_update(var, indices_sparse, new_values)\n",
    "\n",
    "print(\"After modifying the range of elements:\")\n",
    "print(updated_var.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Array Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1 2 3 4]], shape=(1, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# .reshape():\n",
    "# .reshape() is used to change the shape of a tensor while keeping the same underlying data.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a tensor\n",
    "x = tf.constant([[1, 2],\n",
    "                 [3, 4]])\n",
    "\n",
    "# Reshape the tensor to a new shape\n",
    "x_reshaped = tf.reshape(x, [1, 4])  # Reshape to shape (1, 4)\n",
    "print(x_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# tf.squeeze():\n",
    "# This function removes dimensions of size 1 from the shape of a tensor.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a tensor with shape (1, 3, 1)\n",
    "x = tf.constant([[[1], [2], [3]]])\n",
    "\n",
    "# Squeeze the tensor to remove dimensions of size 1\n",
    "x_squeezed = tf.squeeze(x)\n",
    "print(x_squeezed)  # Output shape will be (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 1 2 1 2]\n",
      " [3 4 3 4 3 4]\n",
      " [1 2 1 2 1 2]\n",
      " [3 4 3 4 3 4]], shape=(4, 6), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# tile():\n",
    "# tf.tile() is used to construct a new tensor by tiling the input tensor.\n",
    "# It replicates the input tensor's data along specified dimensions.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a tensor\n",
    "x = tf.constant([[1, 2],\n",
    "                 [3, 4]])\n",
    "\n",
    "# Tile the tensor along rows and columns\n",
    "x_tiled = tf.tile(x, [2, 3])  # Tile to (2, 3) replication\n",
    "print(x_tiled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Arithmetic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a 2D tensor (3x3)\n",
    "tensor_a = tf.random.normal((3, 3), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "tensor_b = tf.random.normal((3, 3), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "\n",
    "# Addition: Element-wise addition of two tensors.\n",
    "result = tf.add(tensor_a, tensor_b)\n",
    "result = tensor_a.__add__(tensor_b)\n",
    "result = tensor_a + tensor_b\n",
    "# Subtraction: Element-wise subtraction of two tensors.\n",
    "result = tf.subtract(tensor_a, tensor_b)\n",
    "# Multiplication: Element-wise multiplication of two tensors.\n",
    "result = tf.multiply(tensor_a, tensor_b)\n",
    "# Division: Element-wise division of two tensors.\n",
    "result = tf.divide(tensor_a, tensor_b)\n",
    "# Exponentiation: Element-wise exponentiation of a tensor.\n",
    "exponent = 2\n",
    "result = tf.pow(tensor_a, exponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Broadcasting Basics\n",
    "* Broadcasting is a technique used in TensorFlow to perform `element-wise operations` on tensors of different shapes by implicitly aligning their dimensions. The main concept is to extend (or \"broadcast\") smaller tensors to match the shape of larger tensors before applying element-wise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  6]\n",
      " [ 8 10 12]]\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Broadcasting Scalars\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a tensor with shape (2, 3)\n",
    "tensor_a = tf.constant([[1, 2, 3],\n",
    "                         [4, 5, 6]])\n",
    "\n",
    "# Define a scalar value\n",
    "scalar_b = tf.constant(2)\n",
    "\n",
    "# Element-wise multiplication using broadcasting\n",
    "result = tensor_a * scalar_b\n",
    "\n",
    "# Display the result\n",
    "print(result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11 22 33]\n",
      " [14 25 36]]\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Broadcasting Vectors\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a matrix with shape (2, 3)\n",
    "matrix_a = tf.constant([[1, 2, 3],\n",
    "                        [4, 5, 6]])\n",
    "\n",
    "# Define a vector with shape (3,)\n",
    "vector_b = tf.constant([10, 20, 30])\n",
    "\n",
    "# Element-wise addition using broadcasting\n",
    "result = matrix_a + vector_b\n",
    "\n",
    "# Display the result\n",
    "print(result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  20  30]\n",
      " [ 80 100 120]]\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Broadcasting Along Different Axes\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a matrix with shape (2, 3)\n",
    "matrix_a = tf.constant([[1, 2, 3],\n",
    "                        [4, 5, 6]])\n",
    "\n",
    "# Define a vector with shape (2,)\n",
    "vector_c = tf.constant([10, 20])\n",
    "\n",
    "# Element-wise multiplication using broadcasting along different axes\n",
    "result = matrix_a * vector_c[:, tf.newaxis]\n",
    "\n",
    "# Display the result\n",
    "print(result.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Reduction operations\n",
    "* Reduction operations in TensorFlow are used to compute aggregate values (e.g., sum, mean, maximum, minimum) over specific dimensions of a tensor, resulting in a tensor with reduced dimensions or a scalar value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum along axis 0: [5 7 9]\n",
      "Sum along axis 1: [ 6 15]\n"
     ]
    }
   ],
   "source": [
    "# 1. tf.reduce_sum\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a tensor\n",
    "tensor = tf.constant([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "\n",
    "# Compute sum along axis 0 (sum of each column)\n",
    "sum_along_axis0 = tf.reduce_sum(tensor, axis=0)\n",
    "\n",
    "# Compute sum along axis 1 (sum of each row)\n",
    "sum_along_axis1 = tf.reduce_sum(tensor, axis=1)\n",
    "\n",
    "print(\"Sum along axis 0:\", sum_along_axis0.numpy())  # Output: [5 7 9]\n",
    "print(\"Sum along axis 1:\", sum_along_axis1.numpy())  # Output: [ 6 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean along axis 0: [2 3 4]\n",
      "Mean along axis 1: [2 5]\n"
     ]
    }
   ],
   "source": [
    "# 2. tf.reduce_mean\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a tensor\n",
    "tensor = tf.constant([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "\n",
    "# Compute mean along axis 0 (mean of each column)\n",
    "mean_along_axis0 = tf.reduce_mean(tensor, axis=0)\n",
    "\n",
    "# Compute mean along axis 1 (mean of each row)\n",
    "mean_along_axis1 = tf.reduce_mean(tensor, axis=1)\n",
    "\n",
    "print(\"Mean along axis 0:\", mean_along_axis0.numpy())  # Output: [2.5 3.5 4.5]\n",
    "print(\"Mean along axis 1:\", mean_along_axis1.numpy())  # Output: [2. 5.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum along axis 0: [4 5 6]\n",
      "Minimum along axis 1: [1 4]\n"
     ]
    }
   ],
   "source": [
    "# 3. tf.reduce_max and tf.reduce_min\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a tensor\n",
    "tensor = tf.constant([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "\n",
    "# Compute maximum along axis 0 (maximum of each column)\n",
    "max_along_axis0 = tf.reduce_max(tensor, axis=0)\n",
    "\n",
    "# Compute minimum along axis 1 (minimum of each row)\n",
    "min_along_axis1 = tf.reduce_min(tensor, axis=1)\n",
    "\n",
    "print(\"Maximum along axis 0:\", max_along_axis0.numpy())  # Output: [4 5 6]\n",
    "print(\"Minimum along axis 1:\", min_along_axis1.numpy())  # Output: [1 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation (all elements): 1.7078252\n",
      "Standard Deviation along axis 0 (column-wise): [1.5 1.5 1.5]\n",
      "Standard Deviation along axis 1 (row-wise): [0.8164966 0.8164966]\n"
     ]
    }
   ],
   "source": [
    "# 4. tf.math.reduce_std\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a tensor\n",
    "tensor = tf.constant([[1.0, 2.0, 3.0],\n",
    "                       [4.0, 5.0, 6.0]])\n",
    "\n",
    "# Compute the standard deviation across all elements\n",
    "std_all = tf.math.reduce_std(tensor)\n",
    "\n",
    "# Compute the standard deviation along axis 0 (column-wise)\n",
    "std_axis0 = tf.math.reduce_std(tensor, axis=0)\n",
    "\n",
    "# Compute the standard deviation along axis 1 (row-wise)\n",
    "std_axis1 = tf.math.reduce_std(tensor, axis=1)\n",
    "\n",
    "print(\"Standard Deviation (all elements):\", std_all.numpy())\n",
    "print(\"Standard Deviation along axis 0 (column-wise):\", std_axis0.numpy())\n",
    "print(\"Standard Deviation along axis 1 (row-wise):\", std_axis1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Basic Mathematical Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a 2D tensor (3x3)\n",
    "tensor = tf.random.normal((3, 3), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "\n",
    "# Square Root: Element-wise square root of a tensor.\n",
    "result = tf.sqrt(tensor)\n",
    "# Absolute Value: Element-wise absolute value of a tensor.\n",
    "result = tf.abs(tensor)\n",
    "# Negative: Element-wise negation of a tensor.\n",
    "result = tf.negative(tensor)\n",
    "# Sine, Cosine, Tangent: Element-wise trigonometric functions.\n",
    "result_sin = tf.sin(tensor)\n",
    "result_cos = tf.cos(tensor)\n",
    "result_tan = tf.tan(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication of the tensors\n",
    "import tensorflow as tf\n",
    "\n",
    "# Matrix multiplication\n",
    "mat1 = tf.constant([[1, 2], [3, 4]])\n",
    "mat2 = tf.constant([[5, 6], [7, 8]])\n",
    "mat_mult = tf.matmul(mat1, mat2)\n",
    "\n",
    "\n",
    "# Perform matrix multiplication using the @ operator\n",
    "mat_mult = mat1 @ mat2  # This is equivalent to tf.matmul(mat1, mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [4 5 6]], shape=(2, 3), dtype=int32)\n",
      "Transposed tensor:\n",
      "tf.Tensor(\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]], shape=(3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Transpose of a tensor\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a tensor with shape (2, 3)\n",
    "x = tf.constant([[1, 2, 3],\n",
    "                 [4, 5, 6]])\n",
    "\n",
    "# Transpose the tensor\n",
    "x_transposed = tf.transpose(x)  # By default, this swaps the first and second dimensions\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(x)\n",
    "print(\"Transposed tensor:\")\n",
    "print(x_transposed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix A:\n",
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "Inverse of matrix A:\n",
      "tf.Tensor(\n",
      "[[-2.0000002   1.0000001 ]\n",
      " [ 1.5000001  -0.50000006]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Inverse of a tensor\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a square matrix (2x2) as a TensorFlow constant\n",
    "A = tf.constant([[1.0, 2.0],\n",
    "                 [3.0, 4.0]])\n",
    "\n",
    "# Compute the inverse of the matrix A\n",
    "A_inverse = tf.linalg.inv(A)\n",
    "\n",
    "# Print the original matrix and its inverse\n",
    "print(\"Original matrix A:\")\n",
    "print(A)\n",
    "\n",
    "print(\"Inverse of matrix A:\")\n",
    "print(A_inverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Comparison Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a 2D tensor (3x3)\n",
    "tensor_a = tf.random.normal((3, 3), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "tensor_b = tf.random.normal((3, 3), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "\n",
    "# Create condition_a and condition_b boolean tensors\n",
    "condition_a = tf.constant([True, False, True])   # Example boolean tensor 1\n",
    "condition_b = tf.constant([False, True, True])   # Example boolean tensor 2\n",
    "\n",
    "# Equal: Element-wise equality comparison of two tensors.\n",
    "result = tf.equal(tensor_a, tensor_b)\n",
    "# Not Equal: Element-wise inequality comparison of two tensors.\n",
    "result = tf.not_equal(tensor_a, tensor_b)\n",
    "# Greater Than, Less Than: Element-wise comparison of two tensors.\n",
    "result_gt = tf.greater(tensor_a, tensor_b)\n",
    "result_lt = tf.less(tensor_a, tensor_b)\n",
    "# Logical AND, OR: Element-wise logical operations.\n",
    "result_and = tf.logical_and(condition_a, condition_b)\n",
    "result_or = tf.logical_or(condition_a, condition_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Clipping and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a tensor with values\n",
    "tensor = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "# Clip the tensor values to be between 2.0 and 4.0\n",
    "clipped_tensor = tf.clip_by_value(tensor, clip_value_min=2.0, clip_value_max=4.0)\n",
    "\n",
    "# Perform L2 normalization along axis 1 (normalize each row)\n",
    "normalized_x = tf.nn.l2_normalize(tensor, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a tensor with values\n",
    "tensor = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "# Is NaN, Is Finite: Element-wise checks for NaN (Not a Number) and finite values.\n",
    "result_isnan = tf.math.is_nan(tensor)\n",
    "result_isfinite = tf.math.is_finite(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Applying TensorFlow Operations to NumPy Arrays\n",
    "* You can convert NumPy arrays to TensorFlow tensors using `tf.convert_to_tensor()` and then apply TensorFlow operations to these tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result (TensorFlow tensor):\n",
      "tf.Tensor(\n",
      "[[ 1.  4.]\n",
      " [ 9. 16.]], shape=(2, 2), dtype=float64)\n",
      "Result (NumPy array):\n",
      "[[ 1.  4.]\n",
      " [ 9. 16.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Create a NumPy array\n",
    "numpy_array = np.array([[1.0, 2.0],\n",
    "                         [3.0, 4.0]])\n",
    "\n",
    "# Convert NumPy array to a TensorFlow tensor\n",
    "tensor = tf.convert_to_tensor(numpy_array)\n",
    "\n",
    "# Apply TensorFlow operations to the tensor\n",
    "result = tf.square(tensor)  # Square each element\n",
    "\n",
    "# Convert the result back to a NumPy array (if needed)\n",
    "result_numpy = result.numpy()\n",
    "\n",
    "print(\"Result (TensorFlow tensor):\")\n",
    "print(result)\n",
    "\n",
    "print(\"Result (NumPy array):\")\n",
    "print(result_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can apply TensorFlow operations on a NumPy array without converting it to a TensorFlow tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result (NumPy array after TensorFlow operation):\n",
      "tf.Tensor(\n",
      "[[ 1.  4.]\n",
      " [ 9. 16.]], shape=(2, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Create a NumPy array\n",
    "numpy_array = np.array([[1.0, 2.0],\n",
    "                         [3.0, 4.0]])\n",
    "\n",
    "# Apply TensorFlow operations directly on the NumPy array\n",
    "result_numpy = tf.square(numpy_array)  # Square each element using NumPy operation\n",
    "\n",
    "print(\"Result (NumPy array after TensorFlow operation):\")\n",
    "print(result_numpy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Applying NumPy Operations to TensorFlow Tensors\n",
    "* Similarly, you can convert TensorFlow tensors to NumPy arrays using .numpy() and apply NumPy operations to these arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result (NumPy array):\n",
      "[[1.        1.4142135]\n",
      " [1.7320508 2.       ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Create a TensorFlow tensor\n",
    "tensor = tf.constant([[1.0, 2.0],\n",
    "                      [3.0, 4.0]])\n",
    "\n",
    "# Convert TensorFlow tensor to a NumPy array\n",
    "numpy_array = tensor.numpy()\n",
    "\n",
    "# Apply NumPy operations to the NumPy array\n",
    "result_numpy = np.sqrt(numpy_array)  # Compute element-wise square root\n",
    "\n",
    "print(\"Result (NumPy array):\")\n",
    "print(result_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To apply NumPy operations directly on TensorFlow tensors without converting them explicitly, you can utilize TensorFlow's ability to seamlessly interoperate with NumPy arrays within eager execution mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result (NumPy array after applying NumPy operation on TensorFlow tensor):\n",
      "[[ 1.  4.]\n",
      " [ 9. 16.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Create a TensorFlow tensor\n",
    "tensor = tf.constant([[1.0, 2.0],\n",
    "                      [3.0, 4.0]])\n",
    "\n",
    "# Apply NumPy operations directly on the TensorFlow tensor\n",
    "result_numpy = np.square(tensor.numpy())  # Square each element using NumPy operation on tensor's numpy array\n",
    "\n",
    "print(\"Result (NumPy array after applying NumPy operation on TensorFlow tensor):\")\n",
    "print(result_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Operations Between Tensors of Different Types\n",
    "\n",
    "* When you attempt to perform operations (like addition, multiplication, etc.) between tensors of different data types, TensorFlow will `raise an exception` indicating the type incompatibility.\n",
    "* If you need to perform operations between tensors of different types, you can `explicitly convert tensors` to compatible types using `tf.cast()`:\n",
    "* It's important to maintain data type consistency in your TensorFlow computations to ensure efficient execution and avoid unnecessary type conversions. When building TensorFlow models, specifying data types explicitly can help in optimizing performance and avoiding unexpected behaviors due to type mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Create tensors of different types\n",
    "\n",
    "# tensor_int = tf.constant(5)  # Integer tensor\n",
    "# tensor_float = tf.constant(3.0)  # Float tensor\n",
    "\n",
    "# # Attempt to add tensors of different types\n",
    "# result = tensor_int + tensor_float  # This will raise a TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit Type Conversion\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create tensors of different types\n",
    "tensor_int = tf.constant(5)  # Integer tensor\n",
    "tensor_float = tf.constant(3.0)  # Float tensor\n",
    "\n",
    "# Convert integer tensor to float\n",
    "tensor_int_float = tf.cast(tensor_int, tf.float32)\n",
    "\n",
    "# Perform addition after type conversion\n",
    "result = tensor_int_float + tensor_float  # This will work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. SparseTensor in TensorFlow\n",
    "\n",
    "* Sparse tensors in TensorFlow are a special type of tensor that efficiently represents and manipulates tensors containing mostly zero values. \n",
    "* This is particularly useful for handling sparse data structures where most elements are zero, such as sparse matrices or high-dimensional data with sparse representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor:\n",
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 1]\n",
      " [1 2]\n",
      " [2 0]], shape=(3, 2), dtype=int64), values=tf.Tensor([3. 4. 5.], shape=(3,), dtype=float32), dense_shape=tf.Tensor([3 3], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# 1. Creating a SparseTensor\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the non-zero elements and their indices\n",
    "indices = tf.constant([[0, 1], [1, 2], [2, 0]])  # Example indices with dtype int32\n",
    "values = tf.constant([3.0, 4.0, 5.0])             # Corresponding non-zero values\n",
    "dense_shape = tf.constant([3, 3], dtype=tf.int64)  # Shape of the dense tensor (3x3) with dtype int64\n",
    "\n",
    "# Convert indices to dtype int64 (if needed, though it's already int64 in this case)\n",
    "indices = tf.cast(indices, dtype=tf.int64)\n",
    "\n",
    "# Create the SparseTensor\n",
    "sparse_tensor = tf.sparse.SparseTensor(indices=indices, values=values, dense_shape=dense_shape)\n",
    "\n",
    "print(\"SparseTensor:\")\n",
    "print(sparse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Tensor:\n",
      "tf.Tensor(\n",
      "[[0. 3. 0.]\n",
      " [0. 0. 4.]\n",
      " [5. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 2. Converting SparseTensor to Dense Tensor\n",
    "\n",
    "dense_tensor = tf.sparse.to_dense(sparse_tensor)\n",
    "print(\"Dense Tensor:\")\n",
    "print(dense_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value at index (1, 2): 4.0\n"
     ]
    }
   ],
   "source": [
    "# 3. Accessing Elements of SparseTensor\n",
    "\n",
    "# Define the example sparse tensor\n",
    "indices = tf.constant([[0, 1], [1, 2], [2, 0]], dtype=tf.int64)\n",
    "values = tf.constant([3.0, 4.0, 5.0])\n",
    "dense_shape = tf.constant([3, 3], dtype=tf.int64)\n",
    "sparse_tensor = tf.sparse.SparseTensor(indices=indices, values=values, dense_shape=dense_shape)\n",
    "\n",
    "# Accessing a specific element at row=1, column=2\n",
    "row_index = 1\n",
    "col_index = 2\n",
    "\n",
    "# Convert sparse tensor to dense tensor and access the element\n",
    "dense_tensor = tf.sparse.to_dense(sparse_tensor)\n",
    "element_value = dense_tensor[row_index, col_index]\n",
    "\n",
    "print(\"Value at index ({}, {}):\".format(row_index, col_index), element_value.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled SparseTensor:\n",
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 1]\n",
      " [1 2]\n",
      " [2 0]], shape=(3, 2), dtype=int64), values=tf.Tensor([ 6.  8. 10.], shape=(3,), dtype=float32), dense_shape=tf.Tensor([3 3], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# 4. Performing Element-wise Operations\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the example sparse tensor\n",
    "indices = tf.constant([[0, 1], [1, 2], [2, 0]], dtype=tf.int64)\n",
    "values = tf.constant([3.0, 4.0, 5.0])\n",
    "dense_shape = tf.constant([3, 3], dtype=tf.int64)\n",
    "sparse_tensor = tf.sparse.SparseTensor(indices=indices, values=values, dense_shape=dense_shape)\n",
    "\n",
    "# Define the scalar value for multiplication\n",
    "scalar = 2.0\n",
    "\n",
    "# Perform element-wise multiplication of SparseTensor values by the scalar\n",
    "scaled_values = sparse_tensor.values * scalar\n",
    "\n",
    "# Create a new SparseTensor with the scaled values\n",
    "scaled_sparse_tensor = tf.sparse.SparseTensor(indices=sparse_tensor.indices,\n",
    "                                              values=scaled_values,\n",
    "                                              dense_shape=sparse_tensor.dense_shape)\n",
    "\n",
    "print(\"Scaled SparseTensor:\")\n",
    "print(scaled_sparse_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated SparseTensor:\n",
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 1]\n",
      " [1 2]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [4 2]\n",
      " [5 0]], shape=(6, 2), dtype=int64), values=tf.Tensor([ 3.  4.  5.  6.  8. 10.], shape=(6,), dtype=float32), dense_shape=tf.Tensor([6 3], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# 5. Combining SparseTensors\n",
    "\n",
    "# Concatenate SparseTensors along a specified axis\n",
    "concatenated_sparse_tensor = tf.sparse.concat(axis=0, sp_inputs=[sparse_tensor, scaled_sparse_tensor])\n",
    "print(\"Concatenated SparseTensor:\")\n",
    "print(concatenated_sparse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of SparseTensor * DenseMatrix:\n",
      "tf.Tensor(\n",
      "[[0. 3. 0.]\n",
      " [0. 0. 4.]\n",
      " [5. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 6. Applying Operations to SparseTensors\n",
    "\n",
    "# Example: Use SparseTensor in a TensorFlow operation (e.g., matrix multiplication)\n",
    "dense_matrix = tf.constant([[1.0, 0.0, 0.0],\n",
    "                            [0.0, 1.0, 0.0],\n",
    "                            [0.0, 0.0, 1.0]])\n",
    "\n",
    "result_sparse_tensor = tf.sparse.sparse_dense_matmul(sparse_tensor, dense_matrix)\n",
    "print(\"Result of SparseTensor * DenseMatrix:\")\n",
    "print(result_sparse_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. String tensors in TensorFlow \n",
    "* These tensors are used to handle and manipulate string data within TensorFlow computational graphs, which can be useful for tasks like natural language processing (NLP), text generation, and preprocessing textual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Tensor:\n",
      "tf.Tensor(b'Hello, TensorFlow!', shape=(), dtype=string)\n",
      "String Tensor:\n",
      "tf.Tensor([b'apple' b'banana' b'orange'], shape=(3,), dtype=string)\n",
      "String Lengths:\n",
      "tf.Tensor([5 6 6], shape=(3,), dtype=int32)\n",
      "Concatenated String:\n",
      "tf.Tensor(b'Hello, TensorFlow', shape=(), dtype=string)\n",
      "String Element:\n",
      "tf.Tensor(b'apple', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Creating String Tensors\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a string tensor using tf.constant()\n",
    "string_tensor = tf.constant(\"Hello, TensorFlow!\")\n",
    "\n",
    "print(\"String Tensor:\")\n",
    "print(string_tensor)\n",
    "\n",
    "# Create a string tensor from a list of strings\n",
    "string_list = [\"apple\", \"banana\", \"orange\"]\n",
    "string_tensor = tf.constant(string_list)\n",
    "\n",
    "print(\"String Tensor:\")\n",
    "print(string_tensor)\n",
    "\n",
    "# Compute the length of each string in the tensor\n",
    "string_lengths = tf.strings.length(string_tensor)\n",
    "\n",
    "print(\"String Lengths:\")\n",
    "print(string_lengths)\n",
    "\n",
    "# Concatenate two string tensors\n",
    "string_tensor1 = tf.constant(\"Hello\")\n",
    "string_tensor2 = tf.constant(\"TensorFlow\")\n",
    "concatenated_string = tf.strings.join([string_tensor1, string_tensor2], separator=\", \")\n",
    "\n",
    "print(\"Concatenated String:\")\n",
    "print(concatenated_string)\n",
    "\n",
    "# Accessing individual string elements\n",
    "string_element = string_tensor[0]  # Access the first element of the string tensor\n",
    "\n",
    "print(\"String Element:\")\n",
    "print(string_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Customizing Models and Training Algorithms`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Custom Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Huber loss function\n",
    "* The Huber loss function combines the benefits of the mean squared error (MSE) for small errors and the mean absolute error (MAE) for large errors, resulting in a robust loss function that is less sensitive to outliers compared to pure MSE.\n",
    "* **Robustness to Outliers:** The Huber loss function is designed to be more robust to outliers compared to the traditional squared loss (mean squared error, MSE). It achieves this by treating errors differently based on their magnitude:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Huber loss function implementation.\n",
    "    \"\"\"\n",
    "    # Calculate the error (difference between true and predicted values)\n",
    "    error = y_true - y_pred\n",
    "    # Determine which errors are considered \"small\" (absolute error < 1)\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    # Compute the squared loss for small errors (squared error / 2)\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    # Compute the linear loss for large errors (absolute error - 0.5)\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    # Use a conditional statement to select between squared_loss and linear_loss\n",
    "    # based on whether the error is considered \"small\" or \"large\"\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualizing the Huber loss function\n",
    "# import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# plt.figure(figsize=(4, 2))\n",
    "# z = np.linspace(-4, 4, 200)\n",
    "# plt.plot(z, huber_fn(0, z), \"b-\", linewidth=2, label=\"huber($z$)\")\n",
    "# plt.plot(z, z**2 / 2, \"b:\", linewidth=1, label=r\"$\\frac{1}{2}z^2$\")\n",
    "# plt.plot([-1, -1], [0, huber_fn(0., -1.)], \"r--\")\n",
    "# plt.plot([1, 1], [0, huber_fn(0., 1.)], \"r--\")\n",
    "# plt.gca().axhline(y=0, color='k')\n",
    "# plt.gca().axvline(x=0, color='k')\n",
    "# plt.axis([-4, 4, 0, 4])\n",
    "# plt.grid(True)\n",
    "# plt.xlabel(\"$z$\")\n",
    "# plt.legend(fontsize=14)\n",
    "# plt.title(\"Huber loss\", fontsize=14)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 4ms/step - loss: 0.6081 - mae: 0.9657 - val_loss: 0.2727 - val_mae: 0.5615\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.2101 - mae: 0.5031 - val_loss: 0.2210 - val_mae: 0.5072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20b95d27790>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Neural Network with Huber Loss using TensorFlow/Keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "# Standardize the input features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the Huber loss function for regression\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "# Determine the input shape for the neural network\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "# Build a sequential neural network model using Keras\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# Compile the model with Huber loss function and Nadam optimizer\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "# Train the model on the scaled training data with validation data\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Saving/Loading Models with Custom Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model with the Custom Loss Function\n",
    "model.save(\"my_model_with_a_custom_loss.h5\")\n",
    "\n",
    "# Load the Model with the Custom Loss Function\n",
    "\n",
    "# Create a dictionary where you map the names of your functions or objects to\n",
    "# the actual functions or objects themselves. This dictionary will be used to \n",
    "# load and access these custom components later on. \n",
    "custom_objects={\"huber_fn\": huber_fn}\n",
    "\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n",
    "                                custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current implementation treats errors between â€“1 and 1 as \"small.\" If you need a different \n",
    "# threshold, one approach is to create a function that generates a customized loss function.\n",
    "\n",
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\")\n",
    "\n",
    "# Save the Model with the Custom Loss Function\n",
    "model.save(\"my_model_with_a_custom_loss_threshold_2.h5\")\n",
    "\n",
    "# Unfortunately, the model doesn't save the threshold value along with it. Hence, you'll need to\n",
    "# specify the threshold when loading the model. Remember to use \"huber_fn\" as the name, which is\n",
    "# the name given to Keras, not the name of the function that created it.\n",
    "\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\", custom_objects={\"huber_fn\":create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss: A Subclass of the `keras.losses.Loss` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 3s 4ms/step - loss: 0.7976 - mae: 0.9701 - val_loss: 0.3898 - val_mae: 0.5821\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.2592 - mae: 0.5261 - val_loss: 0.3127 - val_mae: 0.5337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' When you save a model, Keras calls the loss instanceâ€™s\\nget_config() method and saves the config as JSON in the HDF5 file. \\nTherefore the threshold will be saved along with it; and when you load the\\n model, you just need to map the class name to the class itself '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This class allows customization of the threshold\n",
    "defining 'small' errors.\n",
    "\"\"\"\n",
    "\n",
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold} \n",
    "    \n",
    "\"\"\"\n",
    "The constructor of the class accepts any number of keyword arguments (**kwargs) \n",
    "and forwards them to the parent constructor. The parent constructor manages \n",
    "standard hyperparameters, including the name of the loss function and the \n",
    "reduction algorithm used to aggregate individual instance losses.\n",
    "\n",
    "**kwargs is a special syntax in Python that allows you to pass a variable number of keyword arguments\n",
    "to a function.\n",
    "\n",
    "The call() method takes the labels and predictions, computes all\n",
    "the instance losses, and returns them.\n",
    "\n",
    "The get_config() method returns a dictionary mapping each\n",
    "hyperparameter name to its value. It first calls the parent classâ€™s\n",
    "get_config() method, then adds the new hyperparameters to this\n",
    "dictionary using {**x} syntax.\"\"\"\n",
    "\n",
    "# Define a sequential model using Keras\n",
    "model = keras.models.Sequential([\n",
    "    # First dense layer with 30 neurons, SELU activation, LeCun normal initialization, and input shape determined by input_shape variable\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=input_shape),\n",
    "    # Second dense layer with 1 neuron\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# Compile the model with HuberLoss as the loss function with threshold 2., Nadam optimizer, and Mean Absolute Error (MAE) as a metric\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "# Train the model for 2 epochs using scaled training data and validation data\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "# Save the trained model with a custom loss class as an HDF5 file\n",
    "\n",
    "model.save(\"my_model_with_a_custom_loss_class.h5\")\n",
    "\n",
    "''' When you save a model, Keras calls the loss instanceâ€™s\n",
    "get_config() method and saves the config as JSON in the HDF5 file. \n",
    "Therefore the threshold will be saved along with it; and when you load the\n",
    " model, you just need to map the class name to the class itself '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2426 - mae: 0.5099 - val_loss: 0.2475 - val_mae: 0.4976\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.2305 - mae: 0.4987 - val_loss: 0.2255 - val_mae: 0.4862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model from the HDF5 file, specifying the custom loss function class HuberLoss\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\", custom_objects={\"HuberLoss\": HuberLoss})\n",
    "\n",
    "# Train the loaded model for 2 epochs using scaled training data and validation data\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "# Access the threshold attribute of the loss function used in the model\n",
    "model.loss.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom activation function\n",
    "####  `Softplus Activation Function`\n",
    "\n",
    "\n",
    "The softplus activation function is a smooth and continuous approximation of the ReLU (Rectified Linear Unit) activation function. It is defined mathematically as:\n",
    "\n",
    "$$\n",
    "\\text{softplus}(x) = \\log(1 + e^x)\n",
    "$$\n",
    "\n",
    "The key properties of the softplus function are:\n",
    "\n",
    "- **Smoothness**: The softplus function is a smooth and differentiable function, meaning it is continuous and has a well-defined derivative at all points.\n",
    "\n",
    "- **Range**: The range of the softplus function is (0, infty), which means it always produces positive output values.\n",
    "\n",
    "In summary, both softplus and ReLU are popular activation functions in neural networks. Softplus provides a smooth alternative to ReLU while retaining its desirable properties, making it suitable for hidden layers in neural networks. ReLU, on the other hand, is widely used due to its simplicity, efficiency, and ability to introduce non-linearity to the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the range of x values\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "# Define the ReLU function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Define the softplus function\n",
    "def softplus(x):\n",
    "    return np.log1p(np.exp(x))\n",
    "\n",
    "# Plot ReLU and softplus functions\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(x, relu(x), label='ReLU', color='blue')\n",
    "plt.plot(x, softplus(x), label='Softplus', color='red')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Activation')\n",
    "plt.title('ReLU vs Softplus Activation Function')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom initializer\n",
    "####  `Glorot initializer`\n",
    "\n",
    "The Glorot initializer, also known as Xavier initializer, is a method for initializing the weights of neural network layers. It was proposed by Xavier Glorot and Yoshua Bengio in their paper \"Understanding the difficulty of training deep feedforward neural networks\".\n",
    "\n",
    "- The goal of Glorot initialization is to keep the scale of the gradients roughly the same across different layers of the network, which helps to prevent the gradients from vanishing or exploding during training.\n",
    "\n",
    "- The Glorot initializer initializes the weights of a layer by drawing them from a uniform or normal distribution with zero mean and a variance that depends on the number of input and output units of the layer. The variance is typically calculated as a function of the number of input and output units, which helps in ensuring that the weights are initialized in such a way that the signal from the input layer doesn't vanish or explode as it passes through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom regularizer\n",
    "####  `â„“1 Regularization`\n",
    "\n",
    " In â„“1 regularization, the penalty term added to the loss function is the absolute sum of the weights' magnitudes. \n",
    " - It encourages sparsity in the weight vectors, effectively pushing some weights to exactly zero. \n",
    " - This can be useful for feature selection and model simplification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom constraint\n",
    "####  `Non negative constraint`\n",
    "\n",
    "A custom constraint that ensures weights are all positive can be implemented by defining a function that enforces this constraint and using it when defining the weights of a neural network layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Activation function\n",
    "def my_softplus(z): \n",
    "    \"\"\"\n",
    "    Compute the softplus function, which is a smooth approximation of ReLU.\n",
    "    \n",
    "    Args:\n",
    "        z: Input tensor.\n",
    "    \n",
    "    Returns:\n",
    "        Output tensor after applying the softplus function.\n",
    "    \"\"\"\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "## Initializer\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    \"\"\"\n",
    "    Initialize weights using Glorot initialization (also known as Xavier initialization).\n",
    "    \n",
    "    Args:\n",
    "        shape: Shape of the weight tensor.\n",
    "        dtype: Data type of the weights.\n",
    "    \n",
    "    Returns:\n",
    "        Initialized weight tensor.\n",
    "    \"\"\"\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "## Regulizer\n",
    "def my_l1_regularizer(weights):\n",
    "    \"\"\"\n",
    "    Compute the L1 regularization term.\n",
    "    \n",
    "    Args:\n",
    "        weights: Weight tensor.\n",
    "    \n",
    "    Returns:\n",
    "        L1 regularization term.\n",
    "    \"\"\"\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "## Constraint\n",
    "def my_positive_weights(weights):\n",
    "    \"\"\"\n",
    "    Custom constraint ensuring all weights are positive.\n",
    "    \n",
    "    Args:\n",
    "        weights: Weight tensor.\n",
    "    \n",
    "    Returns:\n",
    "        Weight tensor with negative values replaced by zero.\n",
    "    \"\"\"\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n",
    "## These custom functions can then be used normally, for example: \n",
    "layer = keras.layers.Dense(1, activation=my_softplus,\n",
    "                           kernel_initializer=my_glorot_initializer,\n",
    "                           kernel_regularizer=my_l1_regularizer,\n",
    "                           kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 1.7523 - mae: 0.9482 - val_loss: inf - val_mae: inf\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5953 - mae: 0.5235 - val_loss: inf - val_mae: inf\n"
     ]
    }
   ],
   "source": [
    "#  Train, and save a neural network model using the Keras API with several custom components:\n",
    "# - Custom softplus activation function\n",
    "# - Custom Glorot initializer\n",
    "# - Custom L1 regularization\n",
    "# - Custom positive weights constraint\n",
    "\n",
    "\n",
    "# Clear any previous TensorFlow session to avoid cluttering memory\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define the neural network model\n",
    "model = keras.models.Sequential([\n",
    "    # First hidden layer with SELU activation and LeCun normal initialization\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    # Output layer with custom softplus activation, L1 regularization, positive weights constraint,\n",
    "    # and Glorot initialization\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=my_l1_regularizer,\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])\n",
    "\n",
    "# Compile the model with Mean Squared Error loss and Nadam optimizer\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "# Train the model on the training data with validation data for 2 epochs\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save(\"my_model_with_many_custom_parts.h5\")\n",
    "\n",
    "# Load the trained model\n",
    "model = keras.models.load_model(\n",
    "    \"my_model_with_many_custom_parts.h5\",\n",
    "    custom_objects={\n",
    "       \"my_l1_regularizer\": my_l1_regularizer,\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving hyperparameters of custom functions**\n",
    "\n",
    "If a function has hyperparameters that need to be saved along with the model, it's often beneficial to subclass the appropriate class from Keras. Here are the classes you can `subclass` depending on the type of function:\n",
    "\n",
    "- **Regularizers**: If you have a function that computes a regularization term and it has hyperparameters that need to be saved with the model, subclass `keras.regularizers.Regularizer`. This allows you to define your custom regularization function along with its hyperparameters and ensure they are saved and loaded along with the model.\n",
    "- **Constraints**: If you have a function that imposes constraints on the model's weights and has hyperparameters, subclass `keras.constraints`.Constraint. This allows you to define your custom constraint function along with its hyperparameters and ensure they are saved and loaded along with the model.\n",
    "- **Initializers**: If you have a function that initializes the model's weights and has hyperparameters, subclass `keras.initializers.Initializer`. This allows you to define your custom initialization function along with its hyperparameters and ensure they are saved and loaded along with the model.\n",
    "- **Layers**: If you have a custom layer or activation function that requires hyperparameters to be saved along with the model, subclass `keras.layers.Layer`. This allows you to define your custom layer or activation function with its hyperparameters and ensure they are saved and loaded along with the model.\n",
    "\n",
    "**Note** that for losses, layers (including activation functions), and models, you must implement the `call()` method. However, for regularizers, initializers, and constraints, you should implement the `__call__()` method. Metrics have a slightly different requirement, as we'll discuss next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom L1 regularizer subclassing keras.regularizers.Regularizer.\n",
    "# This allows the custom regularization function to be saved and loaded along with the model,\n",
    "# ensuring consistency in behavior across different instances of the model.\n",
    "\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        # Initialize the custom L1 regularizer.\n",
    "        self.factor = factor\n",
    "    \n",
    "    def __call__(self, weights):\n",
    "        # Compute the L1 regularization term.\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Get the configuration of the regularizer.\n",
    "        Returns:\n",
    "            A dictionary containing the configuration of the regularizer,\n",
    "            including the value of the hyperparameter factor.\n",
    "        \"\"\"\n",
    "        return {\"factor\": self.factor}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 3s 4ms/step - loss: 1.8190 - mae: 0.9543 - val_loss: inf - val_mae: inf\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6252 - mae: 0.5176 - val_loss: inf - val_mae: inf\n"
     ]
    }
   ],
   "source": [
    "# Train, and save a neural network model using the Keras API with several custom components:\n",
    "# - Custom softplus activation function\n",
    "# - Custom Glorot initializer\n",
    "# - Custom L1 regularization (Hyperparameters is reserved)\n",
    "# - Custom positive weights constraint\n",
    "# It also illustrates how to load the saved model with custom components.\n",
    "\n",
    "# Clear any previous TensorFlow session to avoid cluttering memory\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define the neural network model\n",
    "model = keras.models.Sequential([\n",
    "    # First hidden layer with SELU activation and LeCun normal initialization\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    # Output layer with custom softplus activation, custom L1 regularization,\n",
    "    # positive weights constraint, and Glorot initialization\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=MyL1Regularizer(0.01),\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])\n",
    "\n",
    "# Compile the model with Mean Squared Error loss and Nadam optimizer\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "# Train the model on the training data with validation data for 2 epochs\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save(\"my_model_with_many_custom_parts.h5\")\n",
    "\n",
    "# Load the saved model with custom components\n",
    "model = keras.models.load_model(\n",
    "    \"my_model_with_many_custom_parts.h5\",\n",
    "    custom_objects={\n",
    "       \"MyL1Regularizer\": MyL1Regularizer,\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Metrics\n",
    "\n",
    "While both *losses* and *metrics* are used to assess model performance, their roles, characteristics, and requirements differ significantly. Loss functions drive the optimization process during training, requiring differentiability, while evaluation metrics provide interpretable measures of model performance.\n",
    "\n",
    "1. **Loss Functions:**\n",
    "\n",
    "- Loss functions, such as cross-entropy for classification tasks or mean squared error for regression tasks, are primarily used during the training phase of a machine learning model. They measure the discrepancy between the predicted outputs of the model and the actual target values.\n",
    "- Loss functions need to be differentiable (at least in regions where they are evaluated) because they are used to compute gradients during backpropagation, which is essential for updating the model parameters (weights) using gradient descent or its variants.\n",
    "- While interpretability is not a primary concern for loss functions, they should accurately capture the notion of error between predictions and ground truth to guide the optimization process effectively.\n",
    "\n",
    "2. **Evaluation Metrics:**\n",
    "\n",
    "- Evaluation metrics, such as accuracy, precision, recall, F1-score, etc., are used to assess the performance of a trained model.\n",
    "- They provide interpretable measures of model performance that are often more intuitive for humans to understand.\n",
    "- Unlike loss functions, evaluation metrics do not need to be differentiable or have non-zero gradients everywhere. Their primary purpose is to offer insights into how well the model is performing on a given task.\n",
    "- While loss functions are optimized directly during training, evaluation metrics are used to gauge the model's performance on validation or test data and guide decisions such as model selection or hyperparameter tuning.\n",
    "\n",
    "However, typically, creating a custom metric function mirrors the process of creating a custom loss function. Essentially, we could employ the `Huber loss function` we previously established as a metric without any significant modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 3.1260 - huber_fn: 1.0765\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8706 - huber_fn: 0.2927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20b9b6ce170>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear any previous TensorFlow session to avoid cluttering memory\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define the neural network model\n",
    "model = keras.models.Sequential([\n",
    "    # First hidden layer with SELU activation and LeCun normal initialization\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    # Output layer\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# Compile the model with Mean Squared Error loss and Nadam optimizer,\n",
    "# along with a custom Huber metric with delta parameter set to 2.0\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
    "\n",
    "# Train the model on the training data for 2 epochs\n",
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: When you use the same function as both the loss and a metric, you might see slightly different results. This is mainly because of how numbers are stored and handled in computers, which can lead to tiny differences in the final values.\n",
    "\n",
    "To add to that, when you're working with sample weights (which give more importance to some data points over others), things get a bit more complex:\n",
    "\n",
    "- The loss that's calculated during training is like an average of all the batch losses seen so far. Each batch loss is the sum of the losses for each data point, divided by the number of data points in that batch.\n",
    "- The metric, however, is more like a weighted average of all the individual losses, taking into account the sample weights. It's not exactly the same calculation as the loss.\n",
    "\n",
    "In simple terms, if you do the math, you'll find that the loss is basically the metric multiplied by the average of the sample weights, plus a tiny bit of error due to how computers handle numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 0.1241 - huber_fn: 0.2516\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.1177 - huber_fn: 0.2388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.12412070482969284, 0.12485544338276583)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model with a custom Huber loss function with delta parameter set to 2.0,\n",
    "# Nadam optimizer, and a custom Huber metric.\n",
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
    "\n",
    "# Generate random sample weights for each training sample.\n",
    "sample_weight = np.random.rand(len(y_train))\n",
    "\n",
    "# Train the model on the scaled training data for 2 epochs with the specified sample weights.\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2, sample_weight=sample_weight)\n",
    "\n",
    "# Compare the first epoch's loss computed from history with the product of the first epoch's Huber metric\n",
    "# and the mean of the sample weights. \n",
    "history.history[\"loss\"][0], history.history[\"huber_fn\"][0] * sample_weight.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Streaming metrics**\n",
    "\n",
    "**Note**: During training, Keras calculates and tracks the mean of metrics for each batch since the start of the epoch. While this is usually fine, it may not always reflect what you need.  \n",
    "\n",
    "* Consider a binary classifierâ€™s **precision**, for example. The  precision is the number of true positives divided by the number of positive predictions. Suppose the model made five positive predictions in the first batch, four of which were correct: thatâ€™s 80% precision. Then suppose the model made three positive predictions in the second batch, but they were all incorrect: thatâ€™s 0% precision for the second batch. If you just compute the mean of these two precisions, you\n",
    "get 40%. But wait a secondâ€”thatâ€™s not the modelâ€™s precision over these\n",
    "two batches! Indeed, there were a total of four true positives (4 + 0) out of eight positive predictions (5 + 3), so the overall precision is 50%, not\n",
    "40%.\n",
    "\n",
    "* What's required is an object like `keras.metrics.Precision` that keeps count of true positives and false positives and computes their ratio accurately.\n",
    "\n",
    "This is called a **streaming metric** (or *stateful metric*), as it is gradually updated, batch after batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.8, shape=(), dtype=float32)\n",
      "tf.Tensor(0.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This code snippet demonstrates the usage of the precision metric in Keras.\n",
    "# It calculates precision for two sets of predictions and labels and retrieves the result.\n",
    "# Additionally, it showcases how to access the variables associated with the precision metric\n",
    "# and how to reset its states.\n",
    "\n",
    "# Creating a precision metric object\n",
    "precision = keras.metrics.Precision()\n",
    "\n",
    "# Calculating precision for the first set of predictions and labels\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])\n",
    "\n",
    "# Getting the result of precision calculation\n",
    "print(precision.result())\n",
    "\n",
    "# Calculating precision for the second set of predictions and labels\n",
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\n",
    "\n",
    "# Getting the result of precision calculation\n",
    "print(precision.result())\n",
    "\n",
    "# Getting the variables associated with the precision metric\n",
    "precision.variables\n",
    "\n",
    "# Resetting the states of the precision metric\n",
    "precision.reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Streaming metrics:** `HuberMetric` class\n",
    "* A simple example that keeps track of the total Huber loss and the number of instances seen so far. \n",
    "* When asked for the result, it returns the ratio, which is simply the mean Huber loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Mean):\n",
    "    def __init__(self, threshold=1.0, name='HuberMetric', dtype=None):\n",
    "        \"\"\"\n",
    "        Initializes the HuberMetric instance.\n",
    "\n",
    "        Args:\n",
    "            threshold (float): Threshold value for the Huber loss function.\n",
    "            name (str): Name of the metric.\n",
    "            dtype (tf.dtypes.DType): Data type of the metric result.\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        super().__init__(name=name, dtype=dtype)\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Updates the state of the metric based on the true and predicted values.\n",
    "\n",
    "        Args:\n",
    "            y_true (tf.Tensor): True values.\n",
    "            y_pred (tf.Tensor): Predicted values.\n",
    "            sample_weight (tf.Tensor): Optional sample weights.\n",
    "\n",
    "        \"\"\"\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        super().update_state(metric, sample_weight)\n",
    "        \n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns the configuration of the metric.\n",
    "\n",
    "        Returns:\n",
    "            dict: Configuration dictionary.\n",
    "        \"\"\"\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 0.4525 - HuberMetric: 0.9119\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.1223 - HuberMetric: 0.2465\n",
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 0.2321 - HuberMetric: 0.2321\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2228 - HuberMetric: 0.2228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check that the `HuberMetric` class works well:\n",
    "\n",
    "# Clearing any existing Keras sessions to ensure a fresh start\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Creating a simple neural network model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# Compiling the model with a custom Huber loss function and the HuberMetric\n",
    "model.compile(loss=keras.losses.Huber(2.0), optimizer=\"nadam\", weighted_metrics=[HuberMetric(2.0)])\n",
    "\n",
    "# Generating sample weights for the training data\n",
    "sample_weight = np.random.rand(len(y_train))\n",
    "\n",
    "# Training the model for 2 epochs\n",
    "history = model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32),\n",
    "                    epochs=2, sample_weight=sample_weight)\n",
    "\n",
    "# Calculating and printing the loss and metric value for the first epoch\n",
    "history.history[\"loss\"][0], history.history[\"HuberMetric\"][0] * sample_weight.mean()\n",
    "\n",
    "# Saving the model with the custom metric to disk\n",
    "model.save(\"my_model_with_a_custom_metric_v2.h5\")\n",
    "\n",
    "# Loading the model with the custom metric from disk\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_metric_v2.h5\",\n",
    "                                custom_objects={\"HuberMetric\": HuberMetric})\n",
    "\n",
    "# Fine-tuning the loaded model for 2 more epochs\n",
    "model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)\n",
    "\n",
    "# Retrieving and printing the threshold value of the last metric in the model\n",
    "model.metrics[-1].threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Layers\n",
    "\n",
    "- When TensorFlow lacks a needed layer or you want to simplify repetitive patterns, you can create custom layers. For example, if your model repeatedly uses layers A, B, C, you could group them into a custom layer D. This makes the model easier to manage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Exponential layer*\n",
    "* Adding an exponential layer at the output of a regression model can be useful if the values to predict are positive and with very different scales (e.g., 0.001, 10., 10000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.36787945, 1.        , 2.7182817 ], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a custom layer using Lambda to apply the exponential function\n",
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))\n",
    "\n",
    "# Apply the exponential_layer to a sample input\n",
    "exponential_layer([-1., 0., 1.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 0.7736 - val_loss: 0.4135\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4549 - val_loss: 0.3756\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4094 - val_loss: 0.3636\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3988 - val_loss: 0.3951\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 4.2842 - val_loss: 5.4561\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 5.5800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.579950332641602"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear any previous TensorFlow session and set random seeds for reproducibility\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define and compile the neural network model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "    exponential_layer  # Apply the custom exponential layer\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "\n",
    "# Train the model on training data and validate on validation data\n",
    "model.fit(X_train_scaled, y_train, epochs=5,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "model.evaluate(X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
