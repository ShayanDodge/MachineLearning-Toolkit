{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `The Data API`\n",
    "In TensorFlow, the Data API refers to a set of tools and utilities provided by TensorFlow for efficiently loading and preprocessing data. It offers a streamlined and flexible way to work with large datasets, making it easier to build and train machine learning models.\n",
    "\n",
    "* The Data API in TensorFlow centers on the notion of a **dataset**, which is essentially a sequence of data items. While datasets typically read data from disk incrementally, for simplicity, one can create a dataset entirely in RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Dataset\n",
    "* The `from_tensor_slices()` function in TensorFlow takes a tensor and generates a `tf.data.Dataset` where each element corresponds to a slice of the input tensor along its first dimension. For example, if the input tensor has a shape of (10, ...), the resulting dataset will contain 10 items, each representing a slice of the tensor along the first dimension, namely tensors 0 through 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Generate a tensor containing values from 0 to 9 using tf.range()\n",
    "X = tf.range(10)\n",
    "\n",
    "# Create a tf.data.Dataset from the tensor X using from_tensor_slices()\n",
    "# This function creates a dataset where each element is a slice of X along its first dimension\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Print the dataset to observe its structure\n",
    "print(dataset)\n",
    "\n",
    "# Alternatively, you can create a dataset containing a range of values from 0 to 9 using tf.data.Dataset.range()\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Iterate through the dataset and print each item\n",
    "for item in dataset:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chaining Transformations\n",
    "\n",
    "In the context of TensorFlow's Data API, transformations refer to the operations applied to datasets to **modify** or **preprocess** the data in various ways. These transformations are used to prepare the data for training machine learning models.\n",
    "\n",
    "**Common transformations include:**\n",
    "\n",
    "* **Batching**: Grouping multiple examples into batches, which enables processing multiple examples in parallel, typically to improve efficiency during training.\n",
    "\n",
    "* **Repeating**: The `repeat()` transformation is used to repeat the elements of a dataset for a specified number of epochs or indefinitely if no argument is provided. This transformation is often used to ensure that the dataset provides enough data for training over multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Repeat the dataset third time to create a new dataset that contains two repetitions of the original data\n",
    "# Then, batch the dataset into batches of size 7, meaning each batch will contain 7 elements\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "\n",
    "# Iterate through the transformed dataset\n",
    "for item in dataset:\n",
    "    # Print each batch of the dataset\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a dataset containing elements from 0 to 9\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Repeat the dataset twice\n",
    "dataset = dataset.repeat(3)\n",
    "\n",
    "# Batch the dataset into batches of size 7, dropping any remainder\n",
    "dataset = dataset.batch(7, drop_remainder=True)\n",
    "\n",
    "# Iterate through the dataset\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Mapping**: Applying a function to each element of the dataset. This function can be used for various purposes, such as data preprocessing, feature engineering, or data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "64\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a simple transformation function\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Create a dataset containing elements from 0 to 9\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Apply the square function to each element of the dataset in parallel\n",
    "# Specify num_parallel_calls to control the degree of parallelism\n",
    "# Here, tf.data.experimental.AUTOTUNE dynamically determines the degree of parallelism\n",
    "dataset = dataset.map(square, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Iterate through the transformed dataset\n",
    "for item in dataset:\n",
    "    print(item.numpy())  # Print each transformed element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Applying**: The `apply()` method is used to apply a transformation that operates on the dataset as **a whole** rather than individual elements.\n",
    "\n",
    "   * It allows for more complex transformations that involve **aggregating**, **filtering**, or **modifying** the dataset **as a whole**.\n",
    "\n",
    "   * The `apply()` method can be used to perform operations such as **batch-wise normalization**, or custom dataset preprocessing.\n",
    "\n",
    "   * Unlike the `map()` method, the transformation function passed to `apply()` operates on the entire dataset or subsets of it rather than individual elements. \n",
    "\n",
    "   * The transformation function passed to the apply() method must return a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a dataset containing elements from 0 to 4\n",
    "dataset = tf.data.Dataset.range(5)\n",
    "\n",
    "# Define a transformation function to create a copy of the dataset\n",
    "def copy_dataset(ds):\n",
    "    return ds\n",
    "\n",
    "# Apply the copy_dataset function to the dataset using the apply() method\n",
    "copied_dataset = dataset.apply(copy_dataset)\n",
    "\n",
    "# Iterate through the copied dataset\n",
    "for item in copied_dataset:\n",
    "    print(item.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Filtering**: Removing examples from the dataset based on certain criteria, such as removing outliers or selecting specific classes for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a dataset containing elements from 0 to 9\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Apply a filter using a lambda function to keep only elements greater than 5\n",
    "filtered_dataset = dataset.filter(lambda x: x > 5)\n",
    "\n",
    "# Iterate through the filtered dataset\n",
    "for item in filtered_dataset:\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Takeing**: Sometimes you just need to check out a few things from a dataset. That's where the `take()` method comes in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a dataset containing elements from 0 to 9\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Using tf.take to select the first five items\n",
    "subset = dataset.take(5)\n",
    "\n",
    "# Iterating over the subset\n",
    "for item in subset:\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffling**: Randomly shuffling the data to introduce randomness and prevent the model from learning the order of the examples.\n",
    "   * **Here's how it works:** the method creates a new dataset that initially fills a buffer with items from the source dataset. Then, whenever you request an item, it randomly picks one from the buffer and replaces it with a fresh item from the source dataset until it's gone through the entire source dataset. After that, it keeps randomly selecting items from the buffer until it's empty. \n",
    "\n",
    "   * It's crucial to set the buffer size large enough for effective shuffling, but not so large that it exceeds your available RAM. Even if you have plenty of memory, it's unnecessary to surpass the dataset's size. \n",
    "   \n",
    "   * If you want the shuffle to produce the same random order each time you run your program, you can specify a random seed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset with numbers from 0 to 9, repeated three times\n",
    "dataset = tf.data.Dataset.range(10).repeat(3) \n",
    "\n",
    "# Shuffle the dataset with a buffer size of 5 and a random seed of 42,\n",
    "# then batch the shuffled dataset into groups of 7\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "\n",
    "# Iterate over the dataset and print each batch\n",
    "for item in dataset:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mixing lines from various files together**\n",
    "\n",
    "* If you have a **big dataset** that **can't fit in memory**, just shuffling won't cut it because the buffer size is too small compared to the dataset.\n",
    "\n",
    "* To add further shuffling to the instances, a typical method involves **dividing the original data** into several files, then reading them in a random sequence during training. Nonetheless, instances within the same file may still be grouped together. To prevent this, one can randomly select multiple files and read them concurrently, mixing their entries. Additionally, a shuffling buffer can be applied using the `shuffle()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the California dataset to multiple CSV files**\n",
    "\n",
    "1. Let's start by loading and preparing the California housing dataset. \n",
    "   * We first load it, then split it into a training set, a validation set and a test set, and finally we scale it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Splitting CSV file**: For a very large dataset that does not fit in memory, you will typically want to split it into many files first, then have TensorFlow read these files in parallel. \n",
    "   * To demonstrate this, let's start by splitting the housing dataset and save it to 20 CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Importing the os module for file path manipulation\n",
    "import numpy as np  # Importing numpy for array operations\n",
    "\n",
    "# Define a function to save data to multiple CSV files\n",
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    # Define the directory where CSV files will be stored\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    # Define the format for the file path, where {} will be replaced by name_prefix and {:02d} will be replaced by file_idx\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "    # Initialize an empty list to store file paths\n",
    "    filepaths = []\n",
    "    # Get the total number of rows in the data\n",
    "    m = len(data)\n",
    "\n",
    "    # Split the indices of rows into approximately equal parts (number of parts specified by n_parts)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        # Generate the file path for the current part\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        # Append the file path to the list of file paths\n",
    "        filepaths.append(part_csv)\n",
    "        # Open the file for writing\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            # Write the header if provided\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            # Write each row of data to the file\n",
    "            for row_idx in row_indices:\n",
    "                # Write each column of the row, separated by commas\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    # Return the list of file paths\n",
    "    return filepaths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5214</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.049945</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>1.605993</td>\n",
       "      <td>37.63</td>\n",
       "      <td>-122.43</td>\n",
       "      <td>1.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.3275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.490060</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>3.443340</td>\n",
       "      <td>33.69</td>\n",
       "      <td>-117.39</td>\n",
       "      <td>1.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.542373</td>\n",
       "      <td>1.591525</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>2.250847</td>\n",
       "      <td>38.44</td>\n",
       "      <td>-122.98</td>\n",
       "      <td>1.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.1736</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.289003</td>\n",
       "      <td>0.997442</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>33.55</td>\n",
       "      <td>-117.70</td>\n",
       "      <td>2.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0549</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.312457</td>\n",
       "      <td>1.085092</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>2.244384</td>\n",
       "      <td>33.93</td>\n",
       "      <td>-116.93</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
       "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
       "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
       "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
       "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.43             1.442  \n",
       "1    -117.39             1.687  \n",
       "2    -122.98             1.621  \n",
       "3    -117.70             2.621  \n",
       "4    -116.93             0.956  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the features (X) and target variable (y) for training, validation and test data\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "\n",
    "# Define the header for the CSV files by concatenating feature names and target variable name\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "# Save training, validation and test data to multiple CSV files with 20 parts\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)\n",
    "\n",
    "# Now, let's read and display the first few lines of one of the CSV files using Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read the first few lines of the first CSV file of the training dataset\n",
    "pd.read_csv(train_filepaths[0]).head()\n",
    "\n",
    "# # Or, alternatively, we can read the first few lines in text mode\n",
    "# # Open the first CSV file of the training dataset\n",
    "# with open(train_filepaths[0]) as f:\n",
    "#     # Read and print the first 5 lines\n",
    "#     for i in range(5):\n",
    "#         print(f.readline(), end=\"\")\n",
    "\n",
    "# # Finally, we have the file paths of the saved CSV files for training data\n",
    "# train_filepaths\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
