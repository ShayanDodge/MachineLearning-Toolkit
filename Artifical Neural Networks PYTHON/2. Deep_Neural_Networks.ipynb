{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Vanishing/Exploding Gradients Problems\n",
    " * The vanishing gradients problem occurs when the gradients of the loss function with respect to the parameters become extremely small during backpropagation.\n",
    " * Conversely, the exploding gradients problem arises when the gradients of the loss function grow exponentially as they propagate backward through the layers during backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    " * Proper initialization of network weights can help alleviate both vanishing and exploding gradients problems. \n",
    " * Techniques like __Xavier__ initialization and __He__ initialization are commonly used to ensure that the weights are initialized in a way that keeps the signal propagated through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Constant', 'GlorotNormal', 'GlorotUniform', 'HeNormal', 'HeUniform', 'Identity', 'Initializer', 'LecunNormal', 'LecunUniform', 'Ones', 'Orthogonal', 'RandomNormal', 'RandomUniform', 'TruncatedNormal', 'VarianceScaling', 'Zeros', 'constant', 'deserialize', 'get', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform', 'identity', 'lecun_normal', 'lecun_uniform', 'ones', 'orthogonal', 'random_normal', 'random_uniform', 'serialize', 'truncated_normal', 'variance_scaling', 'zeros']\n"
     ]
    }
   ],
   "source": [
    "# Importing essential package\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# List comprehension to get the names of initializers in Keras without underscores\n",
    "initializer_names = [name for name in dir(keras.initializers) if not name.startswith(\"_\")]\n",
    "\n",
    "# Printing the initializer names\n",
    "print(initializer_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating a dense layer with built-in initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x21528f4fd60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dense layer with 10 units, ReLU activation, and He normal weight initialization\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Defining an initializer with Variance Scaling (custom kernel initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an initializer with Variance Scaling\n",
    "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
    "\n",
    "# Creating a dense layer with 10 units, ReLU activation, and custom kernel initializer\n",
    "dense_layer = keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions\n",
    " * Non-saturating activation functions refer to activation functions that do not suffer from the vanishing gradient problem to the same extent as traditional saturating activation functions like sigmoid and tanh. These functions typically have gradients that do not diminish as quickly as the __sigmoid__ or __tanh__ functions for large input values, thereby alleviating the vanishing gradient problem. \n",
    " * One prominent example of a non-saturating activation function is the Rectified Linear Unit (__ReLU__) and its variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Rectified Linear Unit (ReLU):__\n",
    "    * ReLU is a popular activation function in deep learning, defined as __f(x)=max(0,x)__. It outputs zero for negative inputs and passes positive inputs unchanged. \n",
    "    * It's computationally efficient and doesn't suffer from vanishing gradients for positive inputs. However, it may lead to the __\"dying ReLU\"__ problem, causing neurons to become inactive for negative inputs. \n",
    "    * Variants like __Leaky ReLU__, __Parametric ReLU (PReLU)__, and __Exponential Linear Unit (ELU)__ address this issue by allowing a small, non-zero gradient for negative inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing ReLU variants in Keras.layers module\n",
    "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Leaky ReLU:__\n",
    "    * Leaky ReLU is an activation function similar to ReLU but with a small, non-zero gradient for negative input values. \n",
    "    * It's defined as __f(x)=max(αx,x)__, where α is a small constant (e.g., 0.01). \n",
    "    * Leaky ReLU addresses the __\"dying ReLU\"__ problem by preventing neurons from becoming inactive for negative inputs during training. It retains the efficiency of ReLU while mitigating its limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 1.2816 - accuracy: 0.6146 - val_loss: 0.8781 - val_accuracy: 0.7270\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.7951 - accuracy: 0.7462 - val_loss: 0.7061 - val_accuracy: 0.7752\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.6796 - accuracy: 0.7793 - val_loss: 0.6369 - val_accuracy: 0.7870\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.6195 - accuracy: 0.7969 - val_loss: 0.5835 - val_accuracy: 0.8110\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5816 - accuracy: 0.8079 - val_loss: 0.5530 - val_accuracy: 0.8188\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5546 - accuracy: 0.8144 - val_loss: 0.5304 - val_accuracy: 0.8276\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5341 - accuracy: 0.8208 - val_loss: 0.5121 - val_accuracy: 0.8308\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5183 - accuracy: 0.8250 - val_loss: 0.5050 - val_accuracy: 0.8302\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5056 - accuracy: 0.8285 - val_loss: 0.4874 - val_accuracy: 0.8372\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4945 - accuracy: 0.8312 - val_loss: 0.4798 - val_accuracy: 0.8392\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using the Leaky ReLU:\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Defining the neural network model with Leaky ReLU activation\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),  # Leaky ReLU activation\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),  # Leaky ReLU activation\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __Parametric ReLU:__\n",
    "    * Parametric ReLU (PReLU) extends Leaky ReLU by allowing the coefficient α to be learned during training rather than fixed. PReLU can adaptively adjust the leakage for negative inputs based on data, potentially improving performance over fixed coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 1.3504 - accuracy: 0.5968 - val_loss: 0.9022 - val_accuracy: 0.7100\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.8050 - accuracy: 0.7419 - val_loss: 0.7154 - val_accuracy: 0.7670\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.6840 - accuracy: 0.7787 - val_loss: 0.6437 - val_accuracy: 0.7876\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6213 - accuracy: 0.7953 - val_loss: 0.5873 - val_accuracy: 0.8026\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5810 - accuracy: 0.8070 - val_loss: 0.5535 - val_accuracy: 0.8164\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5522 - accuracy: 0.8139 - val_loss: 0.5298 - val_accuracy: 0.8234\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5306 - accuracy: 0.8199 - val_loss: 0.5099 - val_accuracy: 0.8328\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5140 - accuracy: 0.8249 - val_loss: 0.5022 - val_accuracy: 0.8326\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5008 - accuracy: 0.8285 - val_loss: 0.4846 - val_accuracy: 0.8398\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4894 - accuracy: 0.8319 - val_loss: 0.4763 - val_accuracy: 0.8410\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using the Leaky PReLU:\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Defining the neural network model with Leaky ReLU activation\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. __Exponential Linear Unit (ELU):__\n",
    "    * ELU is defined as f(x)=x if x≥0 and f(x)=α(e^x−1) if x<0, where α is a small positive constant.\n",
    "    * ELU behaves similarly to ReLU for positive input values but has a smooth curve for negative input values, allowing it to handle negative inputs more gracefully than ReLU. \n",
    "    * ELU also prevents the \"dying ReLU\" problem by ensuring a non-zero gradient for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.1114 - accuracy: 0.6441 - val_loss: 0.7819 - val_accuracy: 0.7482\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.7236 - accuracy: 0.7634 - val_loss: 0.6526 - val_accuracy: 0.7878\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.6358 - accuracy: 0.7896 - val_loss: 0.6006 - val_accuracy: 0.8024\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5887 - accuracy: 0.8041 - val_loss: 0.5570 - val_accuracy: 0.8140\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5581 - accuracy: 0.8128 - val_loss: 0.5321 - val_accuracy: 0.8280\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5357 - accuracy: 0.8192 - val_loss: 0.5130 - val_accuracy: 0.8320\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5183 - accuracy: 0.8246 - val_loss: 0.4969 - val_accuracy: 0.8386\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5047 - accuracy: 0.8274 - val_loss: 0.4901 - val_accuracy: 0.8358\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4935 - accuracy: 0.8311 - val_loss: 0.4758 - val_accuracy: 0.8448\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4837 - accuracy: 0.8328 - val_loss: 0.4691 - val_accuracy: 0.8428\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using the Leaky PReLU:\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Defining the neural network model with Leaky ReLU activation\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", activation=\"elu\"),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", activation=\"elu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. __Scaled Exponential Linear Unit (SELU):__\n",
    "    * It's an activation function introduced to address the vanishing/exploding gradient problem while promoting self-normalizing properties in neural networks.\n",
    "    * SELU has a self-normalizing property, meaning the output distribution remains close to mean 0 and standard deviation 1, which helps stabilize training.\n",
    "    * It overcomes the vanishing/exploding gradient problem by promoting stable mean and variance propagation through the network.\n",
    "    * It has an exponential component for negative inputs, ensuring non-zero gradients, thus avoiding the \"dying ReLU\" problem.\n",
    "    * SELU is specifically designed for feedforward neural networks, and its performance may vary depending on the architecture and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 36s 20ms/step - loss: 1.3252 - accuracy: 0.4923 - val_loss: 1.0440 - val_accuracy: 0.5922\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 37s 22ms/step - loss: 0.8742 - accuracy: 0.6761 - val_loss: 0.6652 - val_accuracy: 0.7656\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 38s 22ms/step - loss: 1.0586 - accuracy: 0.6026 - val_loss: 0.8854 - val_accuracy: 0.6582\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 38s 22ms/step - loss: 0.8120 - accuracy: 0.6931 - val_loss: 0.7537 - val_accuracy: 0.7324\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 38s 22ms/step - loss: 0.7184 - accuracy: 0.7357 - val_loss: 0.6326 - val_accuracy: 0.7668\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using the Leaky SeLU:\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Calculating mean and standard deviation of training pixel values\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "# Standardizing the input features using mean and standard deviation\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Constructing the neural network model with SELU activation\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "# Adding 99 hidden layers with SELU activation\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "# Adding output layer with softmax activation for 10 classes\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Compiling the model with sparse categorical crossentropy loss and SGD optimizer\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model on scaled training data for 5 epochs with validation data for monitoring\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    " * Batch Normalization is a technique used in deep learning to enhance the training process of neural networks. \n",
    " * This means that it adjusts the activations of each layer so that they have a mean of zero and a standard deviation of one. By doing this normalization, Batch Normalization helps to stabilize the training process and improve the speed at which neural networks converge to an optimal solution.\n",
    "\n",
    " * Batch Normalization has become a standard technique in deep learning because it contributes to __faster training__, __improved model performance__, and __greater stability__ during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.8471 - accuracy: 0.7150 - val_loss: 0.5629 - val_accuracy: 0.8084\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5730 - accuracy: 0.8020 - val_loss: 0.4867 - val_accuracy: 0.8360\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5208 - accuracy: 0.8177 - val_loss: 0.4511 - val_accuracy: 0.8460\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4812 - accuracy: 0.8330 - val_loss: 0.4282 - val_accuracy: 0.8500\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4570 - accuracy: 0.8395 - val_loss: 0.4129 - val_accuracy: 0.8550\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4399 - accuracy: 0.8444 - val_loss: 0.3995 - val_accuracy: 0.8590\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4237 - accuracy: 0.8519 - val_loss: 0.3885 - val_accuracy: 0.8628\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4138 - accuracy: 0.8530 - val_loss: 0.3829 - val_accuracy: 0.8668\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4031 - accuracy: 0.8573 - val_loss: 0.3756 - val_accuracy: 0.8690\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3907 - accuracy: 0.8615 - val_loss: 0.3692 - val_accuracy: 0.8708\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using the Batch Normalization:\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a Sequential model\n",
    "model = keras.models.Sequential([\n",
    "    # Flatten layer to convert 2D input into 1D\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # BatchNormalization layer applied after the Flatten layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # Dense layer with 300 neurons and ReLU activation function\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    # BatchNormalization layer applied after the first Dense layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # Dense layer with 100 neurons and ReLU activation function\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    # BatchNormalization layer applied after the second Dense layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # Dense layer with 10 neurons and softmax activation function for classification\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compiling the model with sparse categorical crossentropy loss and SGD optimizer\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model on training data for 10 epochs with validation data for monitoring\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_14 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 784)              3136      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_256 (Dense)           (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_257 (Dense)           (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_258 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display a summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization_6/gamma:0', True),\n",
       " ('batch_normalization_6/beta:0', True),\n",
       " ('batch_normalization_6/moving_mean:0', False),\n",
       " ('batch_normalization_6/moving_variance:0', False)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the BatchNormalization layer after the first layer in the model\n",
    "bn1 = model.layers[1]\n",
    "\n",
    "# List the names and trainability of the variables in the BatchNormalization layer\n",
    "# This can be useful for inspecting the variables of the BatchNormalization layer\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization: Before or After Activation?\n",
    "* Applying Batch Normalization (BN) __before__ the __activation function__ is a debated topic in deep learning. While sometimes it yields __better results__, it's subject to discussion. Additionally, it's common practice to omit bias terms in layers preceding BatchNormalization. Since BatchNormalization introduces its own bias parameters, setting __use_bias=False__ for these preceding layers avoids redundant parameters and optimizes model efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 1.0467 - accuracy: 0.6695 - val_loss: 0.6839 - val_accuracy: 0.7888\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6864 - accuracy: 0.7788 - val_loss: 0.5597 - val_accuracy: 0.8242\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5994 - accuracy: 0.8026 - val_loss: 0.5008 - val_accuracy: 0.8384\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5453 - accuracy: 0.8191 - val_loss: 0.4648 - val_accuracy: 0.8500\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5108 - accuracy: 0.8269 - val_loss: 0.4413 - val_accuracy: 0.8546\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4889 - accuracy: 0.8337 - val_loss: 0.4242 - val_accuracy: 0.8578\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4717 - accuracy: 0.8372 - val_loss: 0.4099 - val_accuracy: 0.8614\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4547 - accuracy: 0.8434 - val_loss: 0.3995 - val_accuracy: 0.8656\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4432 - accuracy: 0.8466 - val_loss: 0.3900 - val_accuracy: 0.8662\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4309 - accuracy: 0.8508 - val_loss: 0.3824 - val_accuracy: 0.8688\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using the Batch Normalization before the activation function:\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a Sequential model\n",
    "model = keras.models.Sequential([\n",
    "    # Flatten layer to convert 2D input into 1D\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # BatchNormalization layer applied before the activation function\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # Dense layer with 300 neurons and no bias term\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    # BatchNormalization layer applied before the activation function\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # Activation function ReLU applied\n",
    "    keras.layers.Activation(\"relu\"), \n",
    "    # Dense layer with 100 neurons and no bias term\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    # BatchNormalization layer applied before the activation function\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # Activation function ReLU applied\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    # Dense layer with 10 neurons and softmax activation function for classification\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compiling the model with sparse categorical crossentropy loss and SGD optimizer\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model on training data for 10 epochs with validation data for monitoring\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping\n",
    " * Gradient clipping addresses exploding gradients problem by imposing a constraint on the gradients during training. Specifically, it involves scaling the gradients if their norm exceeds a certain threshold.\n",
    " \n",
    " \n",
    " * __Gradient Clipping With clipvalue__: Each component of the gradient vector is individually scaled if it falls outside a specified range. This technique restricts the magnitude of each gradient component without necessarily preserving the overall direction of the gradient vector.\n",
    " \n",
    " * __Gradient Clipping With clipnorm__: The entire gradient vector is scaled down if its norm exceeds a specified threshold. This ensures that the direction of the gradient vector is preserved while preventing it from becoming too large.\n",
    "\n",
    "  * If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting clipnorm instead of clipvalue.\n",
    "\n",
    " * Gradient clipping is commonly used in recurrent neural networks (RNNs) because Batch Normalization (BN) is difficult to implement in RNNs. However, for other network types, like feedforward or convolutional neural networks (CNNs), Batch Normalization is usually sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer with gradient clipping by clipvalue\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "\n",
    "# Set optimizer with gradient clipping by clipnorm\n",
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing Pretrained Layers\n",
    "* Reusing pretrained layers, often referred to as transfer learning, is a common technique in machine learning where a model trained on one task is adapted for use on a new, related task. \n",
    "* This approach is particularly effective when the pretrained model has been trained on a large dataset, as it has learned general features that can be useful for other tasks.\n",
    "* Transfer learning does __`not`__ work as well with __`small`__, densely connected neural networks. This is because these networks learn only a few specific patterns, which aren't very useful for other tasks. On the other hand, transfer learning works __`best`__ with __`deep convolutional neural networks (CNN)`__. These networks are better at learning general features, especially in the lower layers, making them more useful for different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the fashion MNIST training set in two:\n",
    "* `X_train_A`: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "* `X_train_B`: a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "\n",
    "The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "\n",
    "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using `Dense` layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the CNN chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Creating set A and set B\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Splitter of the dataset into two subsets\n",
    "def split_dataset(X, y):\n",
    "\n",
    "    # Creating masks for classes 5 (sandals) and 6 (shirts)\n",
    "    y_5_or_6 = (y == 5) | (y == 6)\n",
    "    # Subsetting dataset into classes A (excluding classes 5 and 6) and B (only classes 5 and 6)\n",
    "    y_A = y[~y_5_or_6]\n",
    "    # Adjusting class indices for class A to maintain continuity\n",
    "    y_A[y_A > 6] -= 2  # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    # For class B, creating a binary classification task: 1 for shirts (class 6), 0 otherwise\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32)\n",
    "    \n",
    "    return ((X[~y_5_or_6], y_A), (X[y_5_or_6], y_B))\n",
    "\n",
    "# Splitting datasets into subsets for classes A and B\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "\n",
    "# Limiting class B training set to 200 samples for efficiency\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]\n",
    "\n",
    "# Print the shape of the training data for the model B\n",
    "print(X_train_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.5989 - accuracy: 0.8164 - val_loss: 0.3967 - val_accuracy: 0.8642\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.3614 - accuracy: 0.8765 - val_loss: 0.3296 - val_accuracy: 0.8832\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.3221 - accuracy: 0.8887 - val_loss: 0.3016 - val_accuracy: 0.8931\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.3018 - accuracy: 0.8961 - val_loss: 0.2878 - val_accuracy: 0.8971\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2880 - accuracy: 0.9009 - val_loss: 0.2764 - val_accuracy: 0.9033\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2774 - accuracy: 0.9043 - val_loss: 0.2725 - val_accuracy: 0.9016\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2688 - accuracy: 0.9081 - val_loss: 0.2662 - val_accuracy: 0.9048\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2617 - accuracy: 0.9107 - val_loss: 0.2607 - val_accuracy: 0.9051\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2561 - accuracy: 0.9118 - val_loss: 0.2561 - val_accuracy: 0.9083\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2509 - accuracy: 0.9136 - val_loss: 0.2514 - val_accuracy: 0.9101\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2462 - accuracy: 0.9150 - val_loss: 0.2480 - val_accuracy: 0.9091\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2419 - accuracy: 0.9167 - val_loss: 0.2464 - val_accuracy: 0.9123\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2384 - accuracy: 0.9186 - val_loss: 0.2420 - val_accuracy: 0.9111\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2348 - accuracy: 0.9197 - val_loss: 0.2405 - val_accuracy: 0.9113\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2318 - accuracy: 0.9202 - val_loss: 0.2432 - val_accuracy: 0.9136\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2286 - accuracy: 0.9219 - val_loss: 0.2389 - val_accuracy: 0.9143\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2261 - accuracy: 0.9222 - val_loss: 0.2408 - val_accuracy: 0.9150\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2231 - accuracy: 0.9237 - val_loss: 0.2444 - val_accuracy: 0.9111\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2207 - accuracy: 0.9249 - val_loss: 0.2320 - val_accuracy: 0.9158\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2184 - accuracy: 0.9256 - val_loss: 0.2329 - val_accuracy: 0.9145\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on set A (Fashion MNIST)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the architecture of model_A\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model with loss function, optimizer, and metrics\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model on training data and validate on validation data\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))\n",
    "\n",
    "# Save the trained model\n",
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_16 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_262 (Dense)           (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_263 (Dense)           (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_264 (Dense)           (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_265 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_266 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_267 (Dense)           (None, 8)                 408       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 276,158\n",
      "Trainable params: 276,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Displaying model A summary\n",
    "model_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 33ms/step - loss: 0.6229 - accuracy: 0.6900 - val_loss: 0.4415 - val_accuracy: 0.8387\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.3835 - accuracy: 0.8900 - val_loss: 0.3447 - val_accuracy: 0.8945\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2951 - accuracy: 0.9550 - val_loss: 0.2833 - val_accuracy: 0.9331\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2382 - accuracy: 0.9750 - val_loss: 0.2401 - val_accuracy: 0.9473\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1990 - accuracy: 0.9900 - val_loss: 0.2098 - val_accuracy: 0.9625\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1701 - accuracy: 0.9900 - val_loss: 0.1866 - val_accuracy: 0.9655\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1491 - accuracy: 0.9950 - val_loss: 0.1690 - val_accuracy: 0.9726\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1324 - accuracy: 0.9950 - val_loss: 0.1547 - val_accuracy: 0.9767\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1193 - accuracy: 0.9950 - val_loss: 0.1424 - val_accuracy: 0.9757\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1082 - accuracy: 0.9950 - val_loss: 0.1325 - val_accuracy: 0.9777\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0992 - accuracy: 0.9950 - val_loss: 0.1240 - val_accuracy: 0.9817\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0913 - accuracy: 0.9950 - val_loss: 0.1172 - val_accuracy: 0.9828\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0850 - accuracy: 0.9950 - val_loss: 0.1112 - val_accuracy: 0.9828\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0793 - accuracy: 0.9950 - val_loss: 0.1059 - val_accuracy: 0.9848\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0747 - accuracy: 0.9950 - val_loss: 0.1013 - val_accuracy: 0.9858\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0707 - accuracy: 0.9950 - val_loss: 0.0974 - val_accuracy: 0.9858\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0671 - accuracy: 0.9950 - val_loss: 0.0937 - val_accuracy: 0.9858\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0638 - accuracy: 0.9950 - val_loss: 0.0905 - val_accuracy: 0.9858\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0609 - accuracy: 0.9950 - val_loss: 0.0874 - val_accuracy: 0.9858\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0581 - accuracy: 0.9950 - val_loss: 0.0847 - val_accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on set B (Fashion MNIST)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the architecture of model_B\n",
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compiling the model with binary_crossentropy loss and SGD optimizer\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model with training data and validating it with validation data\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_17 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_268 (Dense)           (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_269 (Dense)           (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_270 (Dense)           (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_271 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_272 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_273 (Dense)           (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Displaying model B summary\n",
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.9930 - accuracy: 0.1250 - val_loss: 1.8582 - val_accuracy: 0.1562\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.8310 - accuracy: 0.1400 - val_loss: 1.7122 - val_accuracy: 0.1917\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.6840 - accuracy: 0.1900 - val_loss: 1.5796 - val_accuracy: 0.2323\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.5506 - accuracy: 0.2250 - val_loss: 1.4527 - val_accuracy: 0.2667\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 33ms/step - loss: 1.1641 - accuracy: 0.3650 - val_loss: 0.7693 - val_accuracy: 0.5243\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6407 - accuracy: 0.6500 - val_loss: 0.5100 - val_accuracy: 0.7698\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.4365 - accuracy: 0.8300 - val_loss: 0.3798 - val_accuracy: 0.8692\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.3297 - accuracy: 0.8900 - val_loss: 0.2997 - val_accuracy: 0.9108\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2607 - accuracy: 0.9200 - val_loss: 0.2481 - val_accuracy: 0.9371\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2147 - accuracy: 0.9400 - val_loss: 0.2130 - val_accuracy: 0.9513\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1829 - accuracy: 0.9600 - val_loss: 0.1869 - val_accuracy: 0.9574\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1590 - accuracy: 0.9750 - val_loss: 0.1662 - val_accuracy: 0.9635\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1399 - accuracy: 0.9800 - val_loss: 0.1483 - val_accuracy: 0.9686\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1239 - accuracy: 0.9800 - val_loss: 0.1373 - val_accuracy: 0.9706\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1132 - accuracy: 0.9800 - val_loss: 0.1277 - val_accuracy: 0.9736\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1041 - accuracy: 0.9850 - val_loss: 0.1200 - val_accuracy: 0.9746\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0965 - accuracy: 0.9850 - val_loss: 0.1131 - val_accuracy: 0.9767\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0900 - accuracy: 0.9850 - val_loss: 0.1071 - val_accuracy: 0.9807\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0837 - accuracy: 0.9900 - val_loss: 0.1016 - val_accuracy: 0.9817\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0787 - accuracy: 0.9900 - val_loss: 0.0971 - val_accuracy: 0.9828\n"
     ]
    }
   ],
   "source": [
    "# Utilizing pre-trained layers from model A to initialize and enhance the \n",
    "# training of a new neural network on dataset B.\n",
    "\n",
    "# Loading model A\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "\n",
    "# Cloning model A to avoid shared layers during training\n",
    "model_A_clone = keras.models.clone_model(model_A) # Clone model A to preserve its architecture\n",
    "model_A_clone.set_weights(model_A.get_weights()) # Set the weights of the cloned model to match those of model A\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1]) # Create model B based on model A by using all layers except the last one\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\")) # Add a new Dense layer with sigmoid activation to model B\n",
    "\n",
    "# Freezing layers of model B on A for initial training\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compiling and training model B on A\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "# Unfreezing layers of model B on A for further training\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compiling and continuing training of model B on A\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9825\n",
      "[0.08476961404085159, 0.9825000166893005]\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.9835\n",
      "[0.09095914661884308, 0.9835000038146973]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating model B and model B on A on test set B\n",
    "print(model_B.evaluate(X_test_B, y_test_B))\n",
    "print(model_B_on_A.evaluate(X_test_B, y_test_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Momentum optimization\n",
    " * Momentum optimization `accelerates` gradient descent by accumulating past gradients, helping to navigate towards the minimum loss faster.\n",
    " * Momentum optimization requires careful tuning of hyperparameters such as the momentum coefficient Poorly chosen values can lead to suboptimal performance or instability in the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Nesterov Accelerated Gradient\n",
    "* Nesterov Accelerated Gradient, also known as Nesterov momentum or Nesterov `accelerated` gradient descent (NAG), is an enhancement of the standard momentum optimization algorithm.\n",
    "* In essence, Nesterov Accelerated Gradient computes the gradient not at the current position but slightly ahead in the direction of the momentum. This anticipatory step helps to provide a more accurate estimate of the gradient, allowing for smoother and faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. AdaGrad\n",
    "* In AdaGrad, the adjustment of learning rates for each parameter is based on the accumulated squared gradients, effectively reducing the learning rate in directions where gradients historically have been large. This adaptivity enables smaller steps in steep directions, such as those encountered in the elongated bowl problem, where traditional gradient descent may struggle. By decreasing step sizes along steeper gradients, AdaGrad facilitates more efficient progress towards the global optimum, addressing directional challenges early in the optimization process.\n",
    "* AdaGrad works well for simple problems but often stops too early in training neural networks because it reduces the learning rate too much, preventing further progress toward the best solution.\n",
    "* Despite Keras offering the Adagrad optimizer, it's advisable `not` to employ it for training deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RMSProp\n",
    "* As observed, AdaGrad's tendency to slow down too quickly can prevent convergence to the global optimum. RMSProp addresses this issue by accumulating gradients only from the most recent iterations.\n",
    "* The decay rate (rho), often set to 0.9, is a new hyperparameter in RMSProp. However, this default value often performs well, eliminating the need for tuning in many cases.\n",
    "* This optimizer almost always performs much `better` than AdaGrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Adam Optimization\n",
    "* Adam `combines` the advantages of both `AdaGrad` and `RMSProp`. \n",
    "* It maintains adaptive learning rates for each parameter and also keeps track of exponentially decaying average of past gradients and their squares. \n",
    "* This allows Adam to converge quickly and efficiently, even in the presence of noisy gradients, and it is widely used in practice due to its robustness and ease of use.\n",
    "* The momentum decay hyperparameter __β1__ is typically initialized to __0.9__, while the scaling decay hyperparameter __β2__ is often initialized to __0.999__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. AdaMax Optimization\n",
    "* AdaMax is an adaptation of Adam optimization designed to mitigate certain drawbacks of Adam, especially when adaptive learning rates become too aggressive. \n",
    "* In AdaMax, instead of computing the second moment of the gradients using squared gradients as in Adam, AdaMax uses the ℓ∞ norm of the gradients. The ℓ∞ norm, also known as the maximum norm, calculates the maximum absolute value of the elements in a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Nadam Optimization\n",
    "* Nadam, short for Nesterov-accelerated Adaptive Moment Estimation, is an optimization algorithm that `combines` the benefits of `Nesterov momentum` and the adaptive learning rates of `Adam`.\n",
    "* Nadam is particularly effective in optimizing deep neural networks, where it can achieve faster convergence and better generalization compared to traditional gradient-based optimization algorithms. Its ability to combine the benefits of adaptive learning rates and Nesterov momentum makes it a popular choice for training deep neural networks in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
