{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `The Vanishing/Exploding Gradients Problems`\n",
    " * The vanishing gradients problem occurs when the gradients of the loss function with respect to the parameters become extremely small during backpropagation.\n",
    " * Conversely, the exploding gradients problem arises when the gradients of the loss function grow exponentially as they propagate backward through the layers during backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    " * Proper initialization of network weights can help alleviate both vanishing and exploding gradients problems. \n",
    " * Techniques like __Xavier__ initialization and __He__ initialization are commonly used to ensure that the weights are initialized in a way that keeps the signal propagated through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Constant', 'Identity', 'Initializer', 'Ones', 'Orthogonal', 'RandomNormal', 'RandomUniform', 'TruncatedNormal', 'VarianceScaling', 'Zeros', 'constant', 'deserialize', 'get', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform', 'identity', 'lecun_normal', 'lecun_uniform', 'normal', 'ones', 'orthogonal', 'random_normal', 'random_uniform', 'serialize', 'truncated_normal', 'uniform', 'zeros']\n"
     ]
    }
   ],
   "source": [
    "# Importing essential package\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# List comprehension to get the names of initializers in Keras without underscores\n",
    "initializer_names = [name for name in dir(keras.initializers) if not name.startswith(\"_\")]\n",
    "\n",
    "# Printing the initializer names\n",
    "print(initializer_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating a dense layer with built-in initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x283c36c1720>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dense layer with 10 units, ReLU activation, and He normal weight initialization\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Defining an initializer with Variance Scaling (custom kernel initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an initializer with Variance Scaling\n",
    "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
    "\n",
    "# Creating a dense layer with 10 units, ReLU activation, and custom kernel initializer\n",
    "dense_layer = keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions\n",
    " * Non-saturating activation functions refer to activation functions that do not suffer from the vanishing gradient problem to the same extent as traditional saturating activation functions like sigmoid and tanh. These functions typically have gradients that do not diminish as quickly as the __sigmoid__ or __tanh__ functions for large input values, thereby alleviating the vanishing gradient problem. \n",
    " * One prominent example of a non-saturating activation function is the Rectified Linear Unit (__ReLU__) and its variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Rectified Linear Unit (ReLU):__\n",
    "    * ReLU is a popular activation function in deep learning, defined as __f(x)=max(0,x)__. It outputs zero for negative inputs and passes positive inputs unchanged. \n",
    "    * It's computationally efficient and doesn't suffer from vanishing gradients for positive inputs. However, it may lead to the __\"dying ReLU\"__ problem, causing neurons to become inactive for negative inputs. \n",
    "    * Variants like __Leaky ReLU__, __Parametric ReLU (PReLU)__, and __Exponential Linear Unit (ELU)__ address this issue by allowing a small, non-zero gradient for negative inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing ReLU variants in Keras.layers module\n",
    "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Leaky ReLU:__\n",
    "    * Leaky ReLU is an activation function similar to ReLU but with a small, non-zero gradient for negative input values. \n",
    "    * It's defined as __f(x)=max(αx,x)__, where α is a small constant (e.g., 0.01). \n",
    "    * Leaky ReLU addresses the __\"dying ReLU\"__ problem by preventing neurons from becoming inactive for negative inputs during training. It retains the efficiency of ReLU while mitigating its limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 1.2537 - accuracy: 0.5988 - val_loss: 0.8649 - val_accuracy: 0.7186\n",
      "Epoch 2/10\n",
      " 333/1719 [====>.........................] - ETA: 3s - loss: 0.8565 - accuracy: 0.7220"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using the Leaky ReLU:\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Defining the neural network model with Leaky ReLU activation\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),  # Leaky ReLU activation\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),  # Leaky ReLU activation\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __Parametric ReLU:__\n",
    "    * Parametric ReLU (PReLU) extends Leaky ReLU by allowing the coefficient α to be learned during training rather than fixed. PReLU can adaptively adjust the leakage for negative inputs based on data, potentially improving performance over fixed coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 1.3504 - accuracy: 0.5968 - val_loss: 0.9022 - val_accuracy: 0.7100\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.8050 - accuracy: 0.7419 - val_loss: 0.7154 - val_accuracy: 0.7670\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.6840 - accuracy: 0.7787 - val_loss: 0.6437 - val_accuracy: 0.7876\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6213 - accuracy: 0.7953 - val_loss: 0.5873 - val_accuracy: 0.8026\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5810 - accuracy: 0.8070 - val_loss: 0.5535 - val_accuracy: 0.8164\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5522 - accuracy: 0.8139 - val_loss: 0.5298 - val_accuracy: 0.8234\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5306 - accuracy: 0.8199 - val_loss: 0.5099 - val_accuracy: 0.8328\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5140 - accuracy: 0.8249 - val_loss: 0.5022 - val_accuracy: 0.8326\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5008 - accuracy: 0.8285 - val_loss: 0.4846 - val_accuracy: 0.8398\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4894 - accuracy: 0.8319 - val_loss: 0.4763 - val_accuracy: 0.8410\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using the Leaky PReLU:\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Defining the neural network model with Leaky ReLU activation\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. __Exponential Linear Unit (ELU):__\n",
    "    * ELU is defined as f(x)=x if x≥0 and f(x)=α(e^x−1) if x<0, where α is a small positive constant.\n",
    "    * ELU behaves similarly to ReLU for positive input values but has a smooth curve for negative input values, allowing it to handle negative inputs more gracefully than ReLU. \n",
    "    * ELU also prevents the \"dying ReLU\" problem by ensuring a non-zero gradient for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.1114 - accuracy: 0.6441 - val_loss: 0.7819 - val_accuracy: 0.7482\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.7236 - accuracy: 0.7634 - val_loss: 0.6526 - val_accuracy: 0.7878\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.6358 - accuracy: 0.7896 - val_loss: 0.6006 - val_accuracy: 0.8024\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5887 - accuracy: 0.8041 - val_loss: 0.5570 - val_accuracy: 0.8140\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5581 - accuracy: 0.8128 - val_loss: 0.5321 - val_accuracy: 0.8280\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5357 - accuracy: 0.8192 - val_loss: 0.5130 - val_accuracy: 0.8320\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5183 - accuracy: 0.8246 - val_loss: 0.4969 - val_accuracy: 0.8386\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5047 - accuracy: 0.8274 - val_loss: 0.4901 - val_accuracy: 0.8358\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4935 - accuracy: 0.8311 - val_loss: 0.4758 - val_accuracy: 0.8448\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4837 - accuracy: 0.8328 - val_loss: 0.4691 - val_accuracy: 0.8428\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using the Leaky PReLU:\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Defining the neural network model with Leaky ReLU activation\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", activation=\"elu\"),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", activation=\"elu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. __Scaled Exponential Linear Unit (SELU):__\n",
    "    * It's an activation function introduced to address the vanishing/exploding gradient problem while promoting self-normalizing properties in neural networks.\n",
    "    * SELU has a self-normalizing property, meaning the output distribution remains close to mean 0 and standard deviation 1, which helps stabilize training.\n",
    "    * It overcomes the vanishing/exploding gradient problem by promoting stable mean and variance propagation through the network.\n",
    "    * It has an exponential component for negative inputs, ensuring non-zero gradients, thus avoiding the \"dying ReLU\" problem.\n",
    "    * SELU is specifically designed for feedforward neural networks, and its performance may vary depending on the architecture and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 36s 20ms/step - loss: 1.3252 - accuracy: 0.4923 - val_loss: 1.0440 - val_accuracy: 0.5922\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 37s 22ms/step - loss: 0.8742 - accuracy: 0.6761 - val_loss: 0.6652 - val_accuracy: 0.7656\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 38s 22ms/step - loss: 1.0586 - accuracy: 0.6026 - val_loss: 0.8854 - val_accuracy: 0.6582\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 38s 22ms/step - loss: 0.8120 - accuracy: 0.6931 - val_loss: 0.7537 - val_accuracy: 0.7324\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 38s 22ms/step - loss: 0.7184 - accuracy: 0.7357 - val_loss: 0.6326 - val_accuracy: 0.7668\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using the Leaky SeLU:\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Calculating mean and standard deviation of training pixel values\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "# Standardizing the input features using mean and standard deviation\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Constructing the neural network model with SELU activation\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "# Adding 99 hidden layers with SELU activation\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "# Adding output layer with softmax activation for 10 classes\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Compiling the model with sparse categorical crossentropy loss and SGD optimizer\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model on scaled training data for 5 epochs with validation data for monitoring\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    " * Batch Normalization is a technique used in deep learning to enhance the training process of neural networks. \n",
    " * This means that it adjusts the activations of each layer so that they have a mean of zero and a standard deviation of one. By doing this normalization, Batch Normalization helps to stabilize the training process and improve the speed at which neural networks converge to an optimal solution.\n",
    "\n",
    " * Batch Normalization has become a standard technique in deep learning because it contributes to __faster training__, __improved model performance__, and __greater stability__ during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.8471 - accuracy: 0.7150 - val_loss: 0.5629 - val_accuracy: 0.8084\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5730 - accuracy: 0.8020 - val_loss: 0.4867 - val_accuracy: 0.8360\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5208 - accuracy: 0.8177 - val_loss: 0.4511 - val_accuracy: 0.8460\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4812 - accuracy: 0.8330 - val_loss: 0.4282 - val_accuracy: 0.8500\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4570 - accuracy: 0.8395 - val_loss: 0.4129 - val_accuracy: 0.8550\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4399 - accuracy: 0.8444 - val_loss: 0.3995 - val_accuracy: 0.8590\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4237 - accuracy: 0.8519 - val_loss: 0.3885 - val_accuracy: 0.8628\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4138 - accuracy: 0.8530 - val_loss: 0.3829 - val_accuracy: 0.8668\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4031 - accuracy: 0.8573 - val_loss: 0.3756 - val_accuracy: 0.8690\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3907 - accuracy: 0.8615 - val_loss: 0.3692 - val_accuracy: 0.8708\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using the Batch Normalization:\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a Sequential model\n",
    "model = keras.models.Sequential([\n",
    "    # Flatten layer to convert 2D input into 1D\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # BatchNormalization layer applied after the Flatten layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # Dense layer with 300 neurons and ReLU activation function\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    # BatchNormalization layer applied after the first Dense layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # Dense layer with 100 neurons and ReLU activation function\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    # BatchNormalization layer applied after the second Dense layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # Dense layer with 10 neurons and softmax activation function for classification\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compiling the model with sparse categorical crossentropy loss and SGD optimizer\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model on training data for 10 epochs with validation data for monitoring\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_14 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 784)              3136      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_256 (Dense)           (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_257 (Dense)           (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_258 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display a summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization_6/gamma:0', True),\n",
       " ('batch_normalization_6/beta:0', True),\n",
       " ('batch_normalization_6/moving_mean:0', False),\n",
       " ('batch_normalization_6/moving_variance:0', False)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the BatchNormalization layer after the first layer in the model\n",
    "bn1 = model.layers[1]\n",
    "\n",
    "# List the names and trainability of the variables in the BatchNormalization layer\n",
    "# This can be useful for inspecting the variables of the BatchNormalization layer\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization: Before or After Activation?\n",
    "* Applying Batch Normalization (BN) __before__ the __activation function__ is a debated topic in deep learning. While sometimes it yields __better results__, it's subject to discussion. Additionally, it's common practice to omit bias terms in layers preceding BatchNormalization. Since BatchNormalization introduces its own bias parameters, setting __use_bias=False__ for these preceding layers avoids redundant parameters and optimizes model efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 1.0467 - accuracy: 0.6695 - val_loss: 0.6839 - val_accuracy: 0.7888\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6864 - accuracy: 0.7788 - val_loss: 0.5597 - val_accuracy: 0.8242\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5994 - accuracy: 0.8026 - val_loss: 0.5008 - val_accuracy: 0.8384\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5453 - accuracy: 0.8191 - val_loss: 0.4648 - val_accuracy: 0.8500\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5108 - accuracy: 0.8269 - val_loss: 0.4413 - val_accuracy: 0.8546\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4889 - accuracy: 0.8337 - val_loss: 0.4242 - val_accuracy: 0.8578\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4717 - accuracy: 0.8372 - val_loss: 0.4099 - val_accuracy: 0.8614\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4547 - accuracy: 0.8434 - val_loss: 0.3995 - val_accuracy: 0.8656\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4432 - accuracy: 0.8466 - val_loss: 0.3900 - val_accuracy: 0.8662\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4309 - accuracy: 0.8508 - val_loss: 0.3824 - val_accuracy: 0.8688\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using the Batch Normalization before the activation function:\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a Sequential model\n",
    "model = keras.models.Sequential([\n",
    "    # Flatten layer to convert 2D input into 1D\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # BatchNormalization layer applied before the activation function\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # Dense layer with 300 neurons and no bias term\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    # BatchNormalization layer applied before the activation function\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # Activation function ReLU applied\n",
    "    keras.layers.Activation(\"relu\"), \n",
    "    # Dense layer with 100 neurons and no bias term\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    # BatchNormalization layer applied before the activation function\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # Activation function ReLU applied\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    # Dense layer with 10 neurons and softmax activation function for classification\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compiling the model with sparse categorical crossentropy loss and SGD optimizer\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model on training data for 10 epochs with validation data for monitoring\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping\n",
    " * Gradient clipping addresses exploding gradients problem by imposing a constraint on the gradients during training. Specifically, it involves scaling the gradients if their norm exceeds a certain threshold.\n",
    " \n",
    " \n",
    " * __Gradient Clipping With clipvalue__: Each component of the gradient vector is individually scaled if it falls outside a specified range. This technique restricts the magnitude of each gradient component without necessarily preserving the overall direction of the gradient vector.\n",
    " \n",
    " * __Gradient Clipping With clipnorm__: The entire gradient vector is scaled down if its norm exceeds a specified threshold. This ensures that the direction of the gradient vector is preserved while preventing it from becoming too large.\n",
    "\n",
    "  * If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting clipnorm instead of clipvalue.\n",
    "\n",
    " * Gradient clipping is commonly used in recurrent neural networks (RNNs) because Batch Normalization (BN) is difficult to implement in RNNs. However, for other network types, like feedforward or convolutional neural networks (CNNs), Batch Normalization is usually sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer with gradient clipping by clipvalue\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "\n",
    "# Set optimizer with gradient clipping by clipnorm\n",
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Reusing Pretrained Layers`\n",
    "* Reusing pretrained layers, often referred to as transfer learning, is a common technique in machine learning where a model trained on one task is adapted for use on a new, related task. \n",
    "* This approach is particularly effective when the pretrained model has been trained on a large dataset, as it has learned general features that can be useful for other tasks.\n",
    "* Transfer learning does __`not`__ work as well with __`small`__, densely connected neural networks. This is because these networks learn only a few specific patterns, which aren't very useful for other tasks. On the other hand, transfer learning works __`best`__ with __`deep convolutional neural networks (CNN)`__. These networks are better at learning general features, especially in the lower layers, making them more useful for different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the fashion MNIST training set in two:\n",
    "* `X_train_A`: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "* `X_train_B`: a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "\n",
    "The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "\n",
    "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using `Dense` layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the CNN chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Creating set A and set B\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Splitter of the dataset into two subsets\n",
    "def split_dataset(X, y):\n",
    "\n",
    "    # Creating masks for classes 5 (sandals) and 6 (shirts)\n",
    "    y_5_or_6 = (y == 5) | (y == 6)\n",
    "    # Subsetting dataset into classes A (excluding classes 5 and 6) and B (only classes 5 and 6)\n",
    "    y_A = y[~y_5_or_6]\n",
    "    # Adjusting class indices for class A to maintain continuity\n",
    "    y_A[y_A > 6] -= 2  # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    # For class B, creating a binary classification task: 1 for shirts (class 6), 0 otherwise\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32)\n",
    "    \n",
    "    return ((X[~y_5_or_6], y_A), (X[y_5_or_6], y_B))\n",
    "\n",
    "# Splitting datasets into subsets for classes A and B\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "\n",
    "# Limiting class B training set to 200 samples for efficiency\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]\n",
    "\n",
    "# Print the shape of the training data for the model B\n",
    "print(X_train_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.5989 - accuracy: 0.8164 - val_loss: 0.3967 - val_accuracy: 0.8642\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.3614 - accuracy: 0.8765 - val_loss: 0.3296 - val_accuracy: 0.8832\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.3221 - accuracy: 0.8887 - val_loss: 0.3016 - val_accuracy: 0.8931\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.3018 - accuracy: 0.8961 - val_loss: 0.2878 - val_accuracy: 0.8971\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2880 - accuracy: 0.9009 - val_loss: 0.2764 - val_accuracy: 0.9033\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2774 - accuracy: 0.9043 - val_loss: 0.2725 - val_accuracy: 0.9016\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2688 - accuracy: 0.9081 - val_loss: 0.2662 - val_accuracy: 0.9048\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2617 - accuracy: 0.9107 - val_loss: 0.2607 - val_accuracy: 0.9051\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2561 - accuracy: 0.9118 - val_loss: 0.2561 - val_accuracy: 0.9083\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2509 - accuracy: 0.9136 - val_loss: 0.2514 - val_accuracy: 0.9101\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2462 - accuracy: 0.9150 - val_loss: 0.2480 - val_accuracy: 0.9091\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2419 - accuracy: 0.9167 - val_loss: 0.2464 - val_accuracy: 0.9123\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2384 - accuracy: 0.9186 - val_loss: 0.2420 - val_accuracy: 0.9111\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2348 - accuracy: 0.9197 - val_loss: 0.2405 - val_accuracy: 0.9113\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2318 - accuracy: 0.9202 - val_loss: 0.2432 - val_accuracy: 0.9136\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2286 - accuracy: 0.9219 - val_loss: 0.2389 - val_accuracy: 0.9143\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2261 - accuracy: 0.9222 - val_loss: 0.2408 - val_accuracy: 0.9150\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2231 - accuracy: 0.9237 - val_loss: 0.2444 - val_accuracy: 0.9111\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2207 - accuracy: 0.9249 - val_loss: 0.2320 - val_accuracy: 0.9158\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2184 - accuracy: 0.9256 - val_loss: 0.2329 - val_accuracy: 0.9145\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on set A (Fashion MNIST)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the architecture of model_A\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model with loss function, optimizer, and metrics\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model on training data and validate on validation data\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))\n",
    "\n",
    "# Save the trained model\n",
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_16 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_262 (Dense)           (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_263 (Dense)           (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_264 (Dense)           (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_265 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_266 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_267 (Dense)           (None, 8)                 408       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 276,158\n",
      "Trainable params: 276,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Displaying model A summary\n",
    "model_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 33ms/step - loss: 0.6229 - accuracy: 0.6900 - val_loss: 0.4415 - val_accuracy: 0.8387\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.3835 - accuracy: 0.8900 - val_loss: 0.3447 - val_accuracy: 0.8945\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2951 - accuracy: 0.9550 - val_loss: 0.2833 - val_accuracy: 0.9331\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2382 - accuracy: 0.9750 - val_loss: 0.2401 - val_accuracy: 0.9473\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1990 - accuracy: 0.9900 - val_loss: 0.2098 - val_accuracy: 0.9625\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1701 - accuracy: 0.9900 - val_loss: 0.1866 - val_accuracy: 0.9655\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1491 - accuracy: 0.9950 - val_loss: 0.1690 - val_accuracy: 0.9726\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1324 - accuracy: 0.9950 - val_loss: 0.1547 - val_accuracy: 0.9767\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1193 - accuracy: 0.9950 - val_loss: 0.1424 - val_accuracy: 0.9757\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1082 - accuracy: 0.9950 - val_loss: 0.1325 - val_accuracy: 0.9777\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0992 - accuracy: 0.9950 - val_loss: 0.1240 - val_accuracy: 0.9817\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0913 - accuracy: 0.9950 - val_loss: 0.1172 - val_accuracy: 0.9828\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0850 - accuracy: 0.9950 - val_loss: 0.1112 - val_accuracy: 0.9828\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0793 - accuracy: 0.9950 - val_loss: 0.1059 - val_accuracy: 0.9848\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0747 - accuracy: 0.9950 - val_loss: 0.1013 - val_accuracy: 0.9858\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0707 - accuracy: 0.9950 - val_loss: 0.0974 - val_accuracy: 0.9858\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0671 - accuracy: 0.9950 - val_loss: 0.0937 - val_accuracy: 0.9858\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0638 - accuracy: 0.9950 - val_loss: 0.0905 - val_accuracy: 0.9858\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0609 - accuracy: 0.9950 - val_loss: 0.0874 - val_accuracy: 0.9858\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0581 - accuracy: 0.9950 - val_loss: 0.0847 - val_accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on set B (Fashion MNIST)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the architecture of model_B\n",
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compiling the model with binary_crossentropy loss and SGD optimizer\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model with training data and validating it with validation data\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_17 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_268 (Dense)           (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_269 (Dense)           (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_270 (Dense)           (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_271 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_272 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_273 (Dense)           (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Displaying model B summary\n",
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.9930 - accuracy: 0.1250 - val_loss: 1.8582 - val_accuracy: 0.1562\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.8310 - accuracy: 0.1400 - val_loss: 1.7122 - val_accuracy: 0.1917\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.6840 - accuracy: 0.1900 - val_loss: 1.5796 - val_accuracy: 0.2323\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.5506 - accuracy: 0.2250 - val_loss: 1.4527 - val_accuracy: 0.2667\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 33ms/step - loss: 1.1641 - accuracy: 0.3650 - val_loss: 0.7693 - val_accuracy: 0.5243\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6407 - accuracy: 0.6500 - val_loss: 0.5100 - val_accuracy: 0.7698\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.4365 - accuracy: 0.8300 - val_loss: 0.3798 - val_accuracy: 0.8692\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.3297 - accuracy: 0.8900 - val_loss: 0.2997 - val_accuracy: 0.9108\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2607 - accuracy: 0.9200 - val_loss: 0.2481 - val_accuracy: 0.9371\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2147 - accuracy: 0.9400 - val_loss: 0.2130 - val_accuracy: 0.9513\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1829 - accuracy: 0.9600 - val_loss: 0.1869 - val_accuracy: 0.9574\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1590 - accuracy: 0.9750 - val_loss: 0.1662 - val_accuracy: 0.9635\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1399 - accuracy: 0.9800 - val_loss: 0.1483 - val_accuracy: 0.9686\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1239 - accuracy: 0.9800 - val_loss: 0.1373 - val_accuracy: 0.9706\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1132 - accuracy: 0.9800 - val_loss: 0.1277 - val_accuracy: 0.9736\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1041 - accuracy: 0.9850 - val_loss: 0.1200 - val_accuracy: 0.9746\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0965 - accuracy: 0.9850 - val_loss: 0.1131 - val_accuracy: 0.9767\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0900 - accuracy: 0.9850 - val_loss: 0.1071 - val_accuracy: 0.9807\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0837 - accuracy: 0.9900 - val_loss: 0.1016 - val_accuracy: 0.9817\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0787 - accuracy: 0.9900 - val_loss: 0.0971 - val_accuracy: 0.9828\n"
     ]
    }
   ],
   "source": [
    "# Utilizing pre-trained layers from model A to initialize and enhance the \n",
    "# training of a new neural network on dataset B.\n",
    "\n",
    "# Loading model A\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "\n",
    "# Cloning model A to avoid shared layers during training\n",
    "model_A_clone = keras.models.clone_model(model_A) # Clone model A to preserve its architecture\n",
    "model_A_clone.set_weights(model_A.get_weights()) # Set the weights of the cloned model to match those of model A\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1]) # Create model B based on model A by using all layers except the last one\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\")) # Add a new Dense layer with sigmoid activation to model B\n",
    "\n",
    "# Freezing layers of model B on A for initial training\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compiling and training model B on A\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "# Unfreezing layers of model B on A for further training\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compiling and continuing training of model B on A\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9825\n",
      "[0.08476961404085159, 0.9825000166893005]\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.9835\n",
      "[0.09095914661884308, 0.9835000038146973]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating model B and model B on A on test set B\n",
    "print(model_B.evaluate(X_test_B, y_test_B))\n",
    "print(model_B_on_A.evaluate(X_test_B, y_test_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Faster Optimizers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Momentum optimization\n",
    " * Momentum optimization `accelerates` gradient descent by accumulating past gradients, helping to navigate towards the minimum loss faster.\n",
    " * Momentum optimization requires careful tuning of hyperparameters such as the momentum coefficient Poorly chosen values can lead to suboptimal performance or instability in the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Nesterov Accelerated Gradient\n",
    "* Nesterov Accelerated Gradient, also known as Nesterov momentum or Nesterov `accelerated` gradient descent (NAG), is an enhancement of the standard momentum optimization algorithm.\n",
    "* In essence, Nesterov Accelerated Gradient computes the gradient not at the current position but slightly ahead in the direction of the momentum. This anticipatory step helps to provide a more accurate estimate of the gradient, allowing for smoother and faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. AdaGrad\n",
    "* In AdaGrad, the adjustment of learning rates for each parameter is based on the accumulated squared gradients, effectively reducing the learning rate in directions where gradients historically have been large. This adaptivity enables smaller steps in steep directions, such as those encountered in the elongated bowl problem, where traditional gradient descent may struggle. By decreasing step sizes along steeper gradients, AdaGrad facilitates more efficient progress towards the global optimum, addressing directional challenges early in the optimization process.\n",
    "* AdaGrad works well for simple problems but often stops too early in training neural networks because it reduces the learning rate too much, preventing further progress toward the best solution.\n",
    "* Despite Keras offering the Adagrad optimizer, it's advisable `not` to employ it for training deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RMSProp\n",
    "* As observed, AdaGrad's tendency to slow down too quickly can prevent convergence to the global optimum. RMSProp addresses this issue by accumulating gradients only from the most recent iterations.\n",
    "* The decay rate (rho), often set to 0.9, is a new hyperparameter in RMSProp. However, this default value often performs well, eliminating the need for tuning in many cases.\n",
    "* This optimizer almost always performs much `better` than AdaGrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Adam Optimization\n",
    "* Adam `combines` the advantages of both `AdaGrad` and `RMSProp`. \n",
    "* It maintains adaptive learning rates for each parameter and also keeps track of exponentially decaying average of past gradients and their squares. \n",
    "* This allows Adam to converge quickly and efficiently, even in the presence of noisy gradients, and it is widely used in practice due to its robustness and ease of use.\n",
    "* The momentum decay hyperparameter __β1__ is typically initialized to __0.9__, while the scaling decay hyperparameter __β2__ is often initialized to __0.999__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. AdaMax Optimization\n",
    "* AdaMax is an adaptation of Adam optimization designed to mitigate certain drawbacks of Adam, especially when adaptive learning rates become too aggressive. \n",
    "* In AdaMax, instead of computing the second moment of the gradients using squared gradients as in Adam, AdaMax uses the ℓ∞ norm of the gradients. The ℓ∞ norm, also known as the maximum norm, calculates the maximum absolute value of the elements in a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Nadam Optimization\n",
    "* Nadam, short for Nesterov-accelerated Adaptive Moment Estimation, is an optimization algorithm that `combines` the benefits of `Nesterov momentum` and the adaptive learning rates of `Adam`.\n",
    "* Nadam is particularly effective in optimizing deep neural networks, where it can achieve faster convergence and better generalization compared to traditional gradient-based optimization algorithms. Its ability to combine the benefits of adaptive learning rates and Nesterov momentum makes it a popular choice for training deep neural networks in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Learning Rate Scheduling\n",
    "* The learning schedule, or learning rate schedule, is a strategy used in training machine learning models, like neural networks. \n",
    "* It involves adjusting the learning rate during training to strike a balance between exploration and fine-tuning, and between stability and speed. \n",
    "* There are various types of learning rate schedules, including fixed, time-based, step-based, exponential, piecewise, and cyclical schedules.\n",
    "* The choice of schedule depends on factors such as model complexity and dataset size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Power Scheduling\n",
    "* It involves reducing the learning rate according to a `power-law` schedule over time. The formula for power scheduling is:\n",
    "$$ \n",
    "lr = \\frac{lr_0}{(1 + \\frac{t}{s})^{\\text{c}}} \n",
    "$$\n",
    "* Power scheduling gradually decreases the learning rate over time, allowing the model to converge more smoothly as training progresses. The decay and power parameters control how quickly and steeply the learning rate decreases.\n",
    "* Keras uses `c=1` and `s = 1 / decay`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.01\n",
    "decay = 1e-4\n",
    "batch_size = 32\n",
    "n_epochs = 25\n",
    "n_steps_per_epoch = math.ceil(55000 / batch_size)\n",
    "epochs = np.arange(n_epochs)\n",
    "lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))  # Width: 8 inches, Height: 6 inches\n",
    "plt.plot(epochs, lrs,  \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.01])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Power Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4870 - accuracy: 0.8293 - val_loss: 0.4046 - val_accuracy: 0.8624\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3787 - accuracy: 0.8654 - val_loss: 0.3708 - val_accuracy: 0.8718\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3458 - accuracy: 0.8775 - val_loss: 0.3738 - val_accuracy: 0.8712\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3244 - accuracy: 0.8841 - val_loss: 0.3486 - val_accuracy: 0.8792\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3096 - accuracy: 0.8902 - val_loss: 0.3438 - val_accuracy: 0.8796\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2958 - accuracy: 0.8941 - val_loss: 0.3405 - val_accuracy: 0.8826\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2860 - accuracy: 0.8987 - val_loss: 0.3368 - val_accuracy: 0.8806\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2770 - accuracy: 0.9013 - val_loss: 0.3408 - val_accuracy: 0.8776\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2689 - accuracy: 0.9044 - val_loss: 0.3290 - val_accuracy: 0.8820\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2616 - accuracy: 0.9062 - val_loss: 0.3263 - val_accuracy: 0.8828\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2556 - accuracy: 0.9092 - val_loss: 0.3264 - val_accuracy: 0.8830\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2498 - accuracy: 0.9119 - val_loss: 0.3338 - val_accuracy: 0.8786\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2445 - accuracy: 0.9140 - val_loss: 0.3231 - val_accuracy: 0.8858\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2406 - accuracy: 0.9149 - val_loss: 0.3280 - val_accuracy: 0.8820\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2357 - accuracy: 0.9166 - val_loss: 0.3229 - val_accuracy: 0.8854\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2316 - accuracy: 0.9175 - val_loss: 0.3207 - val_accuracy: 0.8862\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2281 - accuracy: 0.9192 - val_loss: 0.3238 - val_accuracy: 0.8866\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2243 - accuracy: 0.9211 - val_loss: 0.3202 - val_accuracy: 0.8874\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2212 - accuracy: 0.9225 - val_loss: 0.3225 - val_accuracy: 0.8874\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2180 - accuracy: 0.9245 - val_loss: 0.3201 - val_accuracy: 0.8872\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2146 - accuracy: 0.9245 - val_loss: 0.3214 - val_accuracy: 0.8860\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2122 - accuracy: 0.9256 - val_loss: 0.3189 - val_accuracy: 0.8886\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2093 - accuracy: 0.9268 - val_loss: 0.3205 - val_accuracy: 0.8870\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2066 - accuracy: 0.9275 - val_loss: 0.3212 - val_accuracy: 0.8888\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2044 - accuracy: 0.9289 - val_loss: 0.3215 - val_accuracy: 0.8882\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using Power scheduling:\n",
    "\n",
    "# Importing essential package\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "# Calculating mean and standard deviation of training pixel values\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "# Standardizing the input features using mean and standard deviation\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the optimizer with power scheduling\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)\n",
    "\n",
    "# Define a Sequential model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model with sparse categorical crossentropy loss and the optimizer\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Number of epochs for training\n",
    "n_epochs = 25\n",
    "\n",
    "# Train the model using the training set and validate using the validation set\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Exponential Scheduling\n",
    "* The learning rate decreases gradually, decreasing by a factor of 10 after every s steps.\n",
    "$$ \n",
    "lr = lr_0 \\ 0.1^{\\frac{t}{s}} \n",
    "$$\n",
    "*  In contrast to power scheduling, which reduces the learning rate at a slower rate over time, exponential scheduling consistently reduces it by a factor of 10 after every s steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for exponential learning rate decay\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "# Set up exponential decay function with initial learning rate lr0=0.01 and s=20\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.8543 - accuracy: 0.7553 - val_loss: 1.0417 - val_accuracy: 0.7806 - lr: 0.0100\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.8102 - accuracy: 0.7704 - val_loss: 0.5637 - val_accuracy: 0.8296 - lr: 0.0089\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6050 - accuracy: 0.8212 - val_loss: 0.7087 - val_accuracy: 0.8094 - lr: 0.0079\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5567 - accuracy: 0.8372 - val_loss: 0.5517 - val_accuracy: 0.8480 - lr: 0.0071\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5164 - accuracy: 0.8473 - val_loss: 0.5111 - val_accuracy: 0.8494 - lr: 0.0063\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4650 - accuracy: 0.8612 - val_loss: 0.5687 - val_accuracy: 0.8626 - lr: 0.0056\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4372 - accuracy: 0.8689 - val_loss: 0.5081 - val_accuracy: 0.8598 - lr: 0.0050\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3984 - accuracy: 0.8788 - val_loss: 0.5019 - val_accuracy: 0.8542 - lr: 0.0045\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3703 - accuracy: 0.8841 - val_loss: 0.4424 - val_accuracy: 0.8720 - lr: 0.0040\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3469 - accuracy: 0.8916 - val_loss: 0.4532 - val_accuracy: 0.8768 - lr: 0.0035\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3202 - accuracy: 0.8983 - val_loss: 0.4204 - val_accuracy: 0.8776 - lr: 0.0032\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2934 - accuracy: 0.9047 - val_loss: 0.4301 - val_accuracy: 0.8758 - lr: 0.0028\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2793 - accuracy: 0.9104 - val_loss: 0.4757 - val_accuracy: 0.8762 - lr: 0.0025\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2615 - accuracy: 0.9141 - val_loss: 0.4760 - val_accuracy: 0.8752 - lr: 0.0022\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2431 - accuracy: 0.9187 - val_loss: 0.4428 - val_accuracy: 0.8842 - lr: 0.0020\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2190 - accuracy: 0.9252 - val_loss: 0.4418 - val_accuracy: 0.8906 - lr: 0.0018\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1999 - accuracy: 0.9321 - val_loss: 0.4830 - val_accuracy: 0.8812 - lr: 0.0016\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1895 - accuracy: 0.9350 - val_loss: 0.4338 - val_accuracy: 0.8874 - lr: 0.0014\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1726 - accuracy: 0.9403 - val_loss: 0.4572 - val_accuracy: 0.8898 - lr: 0.0013\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1588 - accuracy: 0.9441 - val_loss: 0.4696 - val_accuracy: 0.8878 - lr: 0.0011\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1506 - accuracy: 0.9482 - val_loss: 0.4807 - val_accuracy: 0.8864 - lr: 0.0010\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1400 - accuracy: 0.9517 - val_loss: 0.4857 - val_accuracy: 0.8884 - lr: 8.9125e-04\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1290 - accuracy: 0.9561 - val_loss: 0.5249 - val_accuracy: 0.8832 - lr: 7.9433e-04\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1228 - accuracy: 0.9573 - val_loss: 0.5329 - val_accuracy: 0.8862 - lr: 7.0795e-04\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1156 - accuracy: 0.9604 - val_loss: 0.5456 - val_accuracy: 0.8868 - lr: 6.3096e-04\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using Exponential scheduling:\n",
    "\n",
    "# Importing essential package\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "# Calculating mean and standard deviation of training pixel values\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "# Standardizing the input features using mean and standard deviation\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a function for exponential learning rate decay\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "# Set up exponential decay function with initial learning rate lr0=0.01 and s=20\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Set number of epochs\n",
    "n_epochs = 25\n",
    "\n",
    "# Set up LearningRateScheduler callback using exponential decay function\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "\n",
    "# Train the model and record history\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Monitoring learning rate decay over epochs\n",
    "\n",
    "# Importing essential package\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "e = np.asarray(history.epoch)\n",
    "h = np.asarray(history.history[\"lr\"])\n",
    "\n",
    "# Plot learning rate decay over epochs\n",
    "fig = plt.figure(figsize=(4, 3))  # Width: 4 inches, Height: 3 inches\n",
    "plt.plot(e, h, \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  `tf.keras` Exponential Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.4910 - accuracy: 0.8286 - val_loss: 0.4107 - val_accuracy: 0.8616\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3826 - accuracy: 0.8653 - val_loss: 0.3725 - val_accuracy: 0.8752\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3479 - accuracy: 0.8771 - val_loss: 0.3734 - val_accuracy: 0.8714\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3256 - accuracy: 0.8839 - val_loss: 0.3502 - val_accuracy: 0.8810\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3090 - accuracy: 0.8909 - val_loss: 0.3429 - val_accuracy: 0.8798\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2947 - accuracy: 0.8963 - val_loss: 0.3418 - val_accuracy: 0.8820\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2841 - accuracy: 0.8999 - val_loss: 0.3356 - val_accuracy: 0.8814\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2748 - accuracy: 0.9027 - val_loss: 0.3369 - val_accuracy: 0.8812\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2662 - accuracy: 0.9049 - val_loss: 0.3274 - val_accuracy: 0.8872\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2592 - accuracy: 0.9074 - val_loss: 0.3256 - val_accuracy: 0.8868\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2536 - accuracy: 0.9097 - val_loss: 0.3260 - val_accuracy: 0.8862\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2480 - accuracy: 0.9126 - val_loss: 0.3301 - val_accuracy: 0.8834\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2432 - accuracy: 0.9148 - val_loss: 0.3222 - val_accuracy: 0.8888\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2397 - accuracy: 0.9153 - val_loss: 0.3227 - val_accuracy: 0.8876\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2357 - accuracy: 0.9170 - val_loss: 0.3213 - val_accuracy: 0.8884\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2325 - accuracy: 0.9179 - val_loss: 0.3192 - val_accuracy: 0.8886\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2297 - accuracy: 0.9195 - val_loss: 0.3205 - val_accuracy: 0.8898\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2271 - accuracy: 0.9206 - val_loss: 0.3177 - val_accuracy: 0.8904\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2250 - accuracy: 0.9222 - val_loss: 0.3202 - val_accuracy: 0.8880\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2230 - accuracy: 0.9223 - val_loss: 0.3174 - val_accuracy: 0.8906\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2209 - accuracy: 0.9233 - val_loss: 0.3183 - val_accuracy: 0.8890\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2195 - accuracy: 0.9241 - val_loss: 0.3176 - val_accuracy: 0.8904\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2180 - accuracy: 0.9247 - val_loss: 0.3175 - val_accuracy: 0.8894\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2167 - accuracy: 0.9255 - val_loss: 0.3177 - val_accuracy: 0.8902\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2158 - accuracy: 0.9255 - val_loss: 0.3180 - val_accuracy: 0.8908\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using tf.keras Exponential scheduling\n",
    "\n",
    "# Importing essential package\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "# Calculating mean and standard deviation of training pixel values\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "# Standardizing the input features using mean and standard deviation\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the neural network architecture using a Sequential model.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),  # Input layer: Flatten the 28x28 input images into a 1D array.\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),  # Hidden layer with 300 neurons, SELU activation function, and LeCun normal initialization.\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),  # Hidden layer with 100 neurons, SELU activation function, and LeCun normal initialization.\n",
    "    keras.layers.Dense(10, activation=\"softmax\")  # Output layer with 10 neurons (for class probabilities) and softmax activation function.\n",
    "])\n",
    "\n",
    "# Calculate the number of steps in 20 epochs (assuming batch size = 32).\n",
    "s = 20 * len(X_train) // 32  \n",
    "# Define a learning rate schedule with exponential decay starting at 0.01, decayed over 's' steps with a decay rate of 0.1.\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "# Define the optimizer with SGD (Stochastic Gradient Descent) and using the defined learning rate schedule.\n",
    "optimizer = keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Compile the model with sparse categorical crossentropy loss function, SGD optimizer, and accuracy metric.\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])  \n",
    "\n",
    "# Set the number of epochs for training.\n",
    "n_epochs = 25  \n",
    "\n",
    "# Fit the model to the training data without specifying any callbacks for dynamic learning rate adjustments.\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Updating the learning rate at each iteration rather than at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 9s 4ms/step - loss: 0.7900 - accuracy: 0.7694 - val_loss: 0.9446 - val_accuracy: 0.7268 - lr: 0.0089\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.6647 - accuracy: 0.7993 - val_loss: 0.5632 - val_accuracy: 0.8362 - lr: 0.0079\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5896 - accuracy: 0.8179 - val_loss: 0.6082 - val_accuracy: 0.8132 - lr: 0.0071\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5339 - accuracy: 0.8362 - val_loss: 0.4857 - val_accuracy: 0.8472 - lr: 0.0063\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4639 - accuracy: 0.8536 - val_loss: 0.5046 - val_accuracy: 0.8614 - lr: 0.0056\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.4153 - accuracy: 0.8684 - val_loss: 0.4547 - val_accuracy: 0.8580 - lr: 0.0050\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.3940 - accuracy: 0.8754 - val_loss: 0.4800 - val_accuracy: 0.8620 - lr: 0.0045\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3510 - accuracy: 0.8845 - val_loss: 0.4737 - val_accuracy: 0.8604 - lr: 0.0040\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3295 - accuracy: 0.8906 - val_loss: 0.4984 - val_accuracy: 0.8646 - lr: 0.0035\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3085 - accuracy: 0.8969 - val_loss: 0.4377 - val_accuracy: 0.8888 - lr: 0.0032\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2914 - accuracy: 0.9021 - val_loss: 0.4372 - val_accuracy: 0.8784 - lr: 0.0028\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2612 - accuracy: 0.9119 - val_loss: 0.4019 - val_accuracy: 0.8826 - lr: 0.0025\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2464 - accuracy: 0.9161 - val_loss: 0.4560 - val_accuracy: 0.8826 - lr: 0.0022\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2267 - accuracy: 0.9221 - val_loss: 0.4625 - val_accuracy: 0.8782 - lr: 0.0020\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2129 - accuracy: 0.9270 - val_loss: 0.4490 - val_accuracy: 0.8884 - lr: 0.0018\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1950 - accuracy: 0.9330 - val_loss: 0.4545 - val_accuracy: 0.8910 - lr: 0.0016\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1803 - accuracy: 0.9376 - val_loss: 0.4684 - val_accuracy: 0.8890 - lr: 0.0014\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1668 - accuracy: 0.9418 - val_loss: 0.4857 - val_accuracy: 0.8886 - lr: 0.0013\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1574 - accuracy: 0.9472 - val_loss: 0.4844 - val_accuracy: 0.8946 - lr: 0.0011\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1462 - accuracy: 0.9508 - val_loss: 0.4963 - val_accuracy: 0.8890 - lr: 9.9967e-04\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1345 - accuracy: 0.9547 - val_loss: 0.5307 - val_accuracy: 0.8900 - lr: 8.9094e-04\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1283 - accuracy: 0.9569 - val_loss: 0.5469 - val_accuracy: 0.8918 - lr: 7.9404e-04\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1181 - accuracy: 0.9610 - val_loss: 0.5442 - val_accuracy: 0.8922 - lr: 7.0767e-04\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1100 - accuracy: 0.9635 - val_loss: 0.5996 - val_accuracy: 0.8914 - lr: 6.3071e-04\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1046 - accuracy: 0.9655 - val_loss: 0.6429 - val_accuracy: 0.8936 - lr: 5.6211e-04\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using Exponential scheduling\n",
    "\n",
    "# Importing essential package\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "# Calculating mean and standard deviation of training pixel values\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "# Standardizing the input features using mean and standard deviation\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#  Defining callback class for ExponentialDecay\n",
    "K = keras.backend  # Importing Keras backend for tensor operations\n",
    "\n",
    "class ExponentialDecay(keras.callbacks.Callback):\n",
    "    def __init__(self, s=40000):\n",
    "        super().__init__()\n",
    "        self.s = s  # Number of steps for exponential decay calculation\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # Note: the `batch` argument is reset at each epoch\n",
    "        lr = K.get_value(self.model.optimizer.learning_rate)  # Get current learning rate\n",
    "        # Apply exponential decay to the learning rate\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr * 0.1**(1 / self.s))  \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.learning_rate)  # Log the current learning rate\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "lr0 = 0.01  # Initial learning rate\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=lr0)  # Nadam optimizer with initial learning rate\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])  # Compile the model\n",
    "\n",
    "n_epochs = 25  # Number of epochs for training\n",
    "\n",
    "s = 20 * len(X_train) // 32  # Total number of steps in 20 epochs (assuming batch size = 32)\n",
    "exp_decay = ExponentialDecay(s)  # Initialize ExponentialDecay callback\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[exp_decay])  # Train the model with ExponentialDecay callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. Piecewise Constant Scheduling\n",
    "*  It involves defining a sequence of learning rates and corresponding epochs, where each learning rate is used for a certain number of epochs before switching to the next one. \n",
    "* This method provides finer control over the optimization process, potentially leading to more stable convergence and improved performance. \n",
    "* By utilizing a learning rate scheduler callback, the model automatically adjusts the learning rate based on the predefined schedule during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for Simple Piecewise Constant Scheduling\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for Piecewise Constant Scheduling\n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.8401 - accuracy: 0.7585 - val_loss: 0.9527 - val_accuracy: 0.7048 - lr: 0.0100\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.9147 - accuracy: 0.7517 - val_loss: 0.6803 - val_accuracy: 0.8130 - lr: 0.0100\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.8218 - accuracy: 0.7722 - val_loss: 1.0829 - val_accuracy: 0.7214 - lr: 0.0100\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.8546 - accuracy: 0.7788 - val_loss: 0.9399 - val_accuracy: 0.8018 - lr: 0.0100\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.8947 - accuracy: 0.7605 - val_loss: 1.5907 - val_accuracy: 0.6798 - lr: 0.0100\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5781 - accuracy: 0.8311 - val_loss: 0.6291 - val_accuracy: 0.8380 - lr: 0.0050\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5121 - accuracy: 0.8497 - val_loss: 0.5681 - val_accuracy: 0.8514 - lr: 0.0050\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4988 - accuracy: 0.8551 - val_loss: 0.6624 - val_accuracy: 0.8196 - lr: 0.0050\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4955 - accuracy: 0.8578 - val_loss: 0.6233 - val_accuracy: 0.8322 - lr: 0.0050\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4871 - accuracy: 0.8602 - val_loss: 0.5602 - val_accuracy: 0.8578 - lr: 0.0050\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4681 - accuracy: 0.8626 - val_loss: 0.4938 - val_accuracy: 0.8622 - lr: 0.0050\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4609 - accuracy: 0.8689 - val_loss: 0.6297 - val_accuracy: 0.8544 - lr: 0.0050\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4452 - accuracy: 0.8702 - val_loss: 0.5238 - val_accuracy: 0.8642 - lr: 0.0050\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4448 - accuracy: 0.8734 - val_loss: 0.5327 - val_accuracy: 0.8608 - lr: 0.0050\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4465 - accuracy: 0.8765 - val_loss: 0.6011 - val_accuracy: 0.8618 - lr: 0.0050\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3126 - accuracy: 0.9038 - val_loss: 0.4902 - val_accuracy: 0.8814 - lr: 0.0010\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2792 - accuracy: 0.9122 - val_loss: 0.4616 - val_accuracy: 0.8790 - lr: 0.0010\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2624 - accuracy: 0.9145 - val_loss: 0.4674 - val_accuracy: 0.8840 - lr: 0.0010\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2526 - accuracy: 0.9173 - val_loss: 0.4506 - val_accuracy: 0.8902 - lr: 0.0010\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2421 - accuracy: 0.9214 - val_loss: 0.4758 - val_accuracy: 0.8830 - lr: 0.0010\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2349 - accuracy: 0.9243 - val_loss: 0.4957 - val_accuracy: 0.8826 - lr: 0.0010\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2259 - accuracy: 0.9267 - val_loss: 0.5397 - val_accuracy: 0.8864 - lr: 0.0010\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2219 - accuracy: 0.9290 - val_loss: 0.5000 - val_accuracy: 0.8816 - lr: 0.0010\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2168 - accuracy: 0.9309 - val_loss: 0.5353 - val_accuracy: 0.8852 - lr: 0.0010\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2093 - accuracy: 0.9338 - val_loss: 0.5317 - val_accuracy: 0.8874 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using Piecewise Constant Scheduling\n",
    "\n",
    "# Importing essential package\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "# Calculating mean and standard deviation of training pixel values\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "# Standardizing the input features using mean and standard deviation\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a function for Piecewise Constant Scheduling\n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "# Define a piecewise constant function for learning rate scheduling\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])\n",
    "\n",
    "# Create a LearningRateScheduler callback using the defined piecewise constant function\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "\n",
    "# Define the neural network model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model with specified loss, optimizer, and metrics\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Set number of epochs for training\n",
    "n_epochs = 25\n",
    "\n",
    "# Train the model with specified training and validation data and the learning rate scheduler callback\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning rate schedule over epochs using the piecewise constant function\n",
    "plt.figure(figsize=(4, 3))  # Width: 4 inches, Height: 3 inches\n",
    "plt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], \"o-\")\n",
    "# Set the axis limits for the plot\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "# Set labels for the x and y axes\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "# Set the title of the plot\n",
    "plt.title(\"Piecewise Constant Scheduling\", fontsize=14)\n",
    "# Enable grid on the plot\n",
    "plt.grid(True)\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.4861 - accuracy: 0.8297 - val_loss: 0.4008 - val_accuracy: 0.8604\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3788 - accuracy: 0.8659 - val_loss: 0.3706 - val_accuracy: 0.8696\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3443 - accuracy: 0.8779 - val_loss: 0.3748 - val_accuracy: 0.8654\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3211 - accuracy: 0.8850 - val_loss: 0.3440 - val_accuracy: 0.8768\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3036 - accuracy: 0.8915 - val_loss: 0.3361 - val_accuracy: 0.8776\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2759 - accuracy: 0.9019 - val_loss: 0.3296 - val_accuracy: 0.8844\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2683 - accuracy: 0.9055 - val_loss: 0.3266 - val_accuracy: 0.8814\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2609 - accuracy: 0.9077 - val_loss: 0.3313 - val_accuracy: 0.8770\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2539 - accuracy: 0.9089 - val_loss: 0.3215 - val_accuracy: 0.8854\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2475 - accuracy: 0.9108 - val_loss: 0.3220 - val_accuracy: 0.8828\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2416 - accuracy: 0.9132 - val_loss: 0.3211 - val_accuracy: 0.8866\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2356 - accuracy: 0.9160 - val_loss: 0.3341 - val_accuracy: 0.8782\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2299 - accuracy: 0.9182 - val_loss: 0.3223 - val_accuracy: 0.8836\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2251 - accuracy: 0.9202 - val_loss: 0.3289 - val_accuracy: 0.8818\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2189 - accuracy: 0.9224 - val_loss: 0.3191 - val_accuracy: 0.8860\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2034 - accuracy: 0.9298 - val_loss: 0.3147 - val_accuracy: 0.8874\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2016 - accuracy: 0.9311 - val_loss: 0.3152 - val_accuracy: 0.8876\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2001 - accuracy: 0.9315 - val_loss: 0.3142 - val_accuracy: 0.8888\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1990 - accuracy: 0.9314 - val_loss: 0.3161 - val_accuracy: 0.8870\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1980 - accuracy: 0.9315 - val_loss: 0.3147 - val_accuracy: 0.8880\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1964 - accuracy: 0.9330 - val_loss: 0.3166 - val_accuracy: 0.8866\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1956 - accuracy: 0.9331 - val_loss: 0.3154 - val_accuracy: 0.8882\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1943 - accuracy: 0.9330 - val_loss: 0.3164 - val_accuracy: 0.8858\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1932 - accuracy: 0.9342 - val_loss: 0.3172 - val_accuracy: 0.8854\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1923 - accuracy: 0.9339 - val_loss: 0.3175 - val_accuracy: 0.8862\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using tf.keras Piecewise Constant Scheduling\n",
    "\n",
    "# Importing essential package\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "# Calculating mean and standard deviation of training pixel values\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "# Standardizing the input features using mean and standard deviation\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the neural network architecture using a Sequential model.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),  # Input layer: Flatten the 28x28 input images into a 1D array.\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),  # Hidden layer with 300 neurons, SELU activation function, and LeCun normal initialization.\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),  # Hidden layer with 100 neurons, SELU activation function, and LeCun normal initialization.\n",
    "    keras.layers.Dense(10, activation=\"softmax\")  # Output layer with 10 neurons (for class probabilities) and softmax activation function.\n",
    "])\n",
    "\n",
    "# Define a piecewise constant learning rate schedule with specified boundaries and corresponding values.\n",
    "learning_rate = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=[5. * n_steps_per_epoch, 15. * n_steps_per_epoch],  # Learning rate boundaries at 5 and 15 epochs.\n",
    "    values=[0.01, 0.005, 0.001])  # Learning rate values for different intervals.\n",
    "# Define the optimizer with SGD (Stochastic Gradient Descent) and using the defined learning rate schedule.\n",
    "optimizer = keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Compile the model with sparse categorical crossentropy loss function, SGD optimizer, and accuracy metric.\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])  \n",
    "\n",
    "# Set the number of epochs for training.\n",
    "n_epochs = 25  \n",
    "\n",
    "# Fit the model to the training data without specifying any callbacks for dynamic learning rate adjustments.\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4. Performance scheduling\n",
    "*  Performance scheduling involves dynamically adjusting the learning rate during training based on the validation error. \n",
    "* At intervals defined by parameter N, the validation error is measured, and if it ceases to decrease for a set number of consecutive checks, the learning rate is reduced by a specified factor λ. \n",
    "* This adaptive strategy aims to prevent overshooting of the optimal model parameters by iteratively fine-tuning the learning rate according to the observed performance, potentially enhancing convergence and model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001B506CFDEE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001B506CFDEE8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.6099 - accuracy: 0.8024 - val_loss: 0.5381 - val_accuracy: 0.8150\n",
      "Epoch 2/25\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.5153 - accuracy: 0.8370 - val_loss: 0.5442 - val_accuracy: 0.8260\n",
      "Epoch 3/25\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.5054 - accuracy: 0.8436 - val_loss: 0.5000 - val_accuracy: 0.8414\n",
      "Epoch 4/25\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.5057 - accuracy: 0.8471 - val_loss: 0.6094 - val_accuracy: 0.8330\n",
      "Epoch 5/25\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.5312 - accuracy: 0.8487 - val_loss: 0.5439 - val_accuracy: 0.8536\n",
      "Epoch 6/25\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.5100 - accuracy: 0.8543 - val_loss: 0.6526 - val_accuracy: 0.8286\n",
      "Epoch 7/25\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.5495 - accuracy: 0.8477 - val_loss: 0.6644 - val_accuracy: 0.8306\n",
      "Epoch 8/25\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.5066 - accuracy: 0.8577 - val_loss: 0.8381 - val_accuracy: 0.8174\n",
      "Epoch 9/25\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.3027 - accuracy: 0.8947 - val_loss: 0.4184 - val_accuracy: 0.8726\n",
      "Epoch 10/25\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.2514 - accuracy: 0.9079 - val_loss: 0.4376 - val_accuracy: 0.8776\n",
      "Epoch 11/25\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2364 - accuracy: 0.9146 - val_loss: 0.4219 - val_accuracy: 0.8896\n",
      "Epoch 12/25\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2148 - accuracy: 0.9204 - val_loss: 0.4559 - val_accuracy: 0.8864\n",
      "Epoch 13/25\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2032 - accuracy: 0.9235 - val_loss: 0.4425 - val_accuracy: 0.8858\n",
      "Epoch 14/25\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.1908 - accuracy: 0.9286 - val_loss: 0.4969 - val_accuracy: 0.8784\n",
      "Epoch 15/25\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.1383 - accuracy: 0.9459 - val_loss: 0.4380 - val_accuracy: 0.8936\n",
      "Epoch 16/25\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.1238 - accuracy: 0.9516 - val_loss: 0.4566 - val_accuracy: 0.8934\n",
      "Epoch 17/25\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.1166 - accuracy: 0.9537 - val_loss: 0.4906 - val_accuracy: 0.8924\n",
      "Epoch 18/25\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.1099 - accuracy: 0.9572 - val_loss: 0.4869 - val_accuracy: 0.8956\n",
      "Epoch 19/25\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.1029 - accuracy: 0.9604 - val_loss: 0.5072 - val_accuracy: 0.8960\n",
      "Epoch 20/25\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.0831 - accuracy: 0.9687 - val_loss: 0.5065 - val_accuracy: 0.8976\n",
      "Epoch 21/25\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.0770 - accuracy: 0.9711 - val_loss: 0.5159 - val_accuracy: 0.8958\n",
      "Epoch 22/25\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.0729 - accuracy: 0.9725 - val_loss: 0.5399 - val_accuracy: 0.8946\n",
      "Epoch 23/25\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.0699 - accuracy: 0.9743 - val_loss: 0.5447 - val_accuracy: 0.8962\n",
      "Epoch 24/25\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.0670 - accuracy: 0.9746 - val_loss: 0.5568 - val_accuracy: 0.8938\n",
      "Epoch 25/25\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.0576 - accuracy: 0.9794 - val_loss: 0.5652 - val_accuracy: 0.8956\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using Performance Scheduling\n",
    "\n",
    "# Importing essential package\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "# Calculating mean and standard deviation of training pixel values\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "# Standardizing the input features using mean and standard deviation\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a learning rate scheduler to dynamically adjust the learning rate during training based on validation error.\n",
    "# The learning rate is reduced by a factor of 0.5 if the validation error does not decrease for 5 consecutive epochs.\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)  \n",
    "\n",
    "# Define the neural network architecture using a Sequential model.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),  # Input layer: Flatten the 28x28 input images into a 1D array.\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),  # Hidden layer with 300 neurons, SELU activation function, and LeCun normal initialization.\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),  # Hidden layer with 100 neurons, SELU activation function, and LeCun normal initialization.\n",
    "    keras.layers.Dense(10, activation=\"softmax\")  # Output layer with 10 neurons (for class probabilities) and softmax activation function.\n",
    "])\n",
    "\n",
    "# Define the optimizer with SGD (Stochastic Gradient Descent) and set learning rate to 0.02 and momentum to 0.9.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)  \n",
    "\n",
    "# Compile the model with sparse categorical crossentropy loss function and accuracy metric.\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])  \n",
    "\n",
    "# Set the number of epochs for training.\n",
    "n_epochs = 25  \n",
    "\n",
    "# Fit the model to the training data while utilizing the defined learning rate scheduler callback for dynamic adjustment \n",
    "# of the learning rate during training based on validation performance.\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning rate over epochs with blue circles and solid line.\n",
    "plt.figure(figsize=(4, 3))  # Width: 4 inches, Height: 3 inches\n",
    "plt.plot(history.epoch, history.history[\"lr\"], \"bo-\")  \n",
    "plt.xlabel(\"Epoch\")  # Label x-axis as 'Epoch'.\n",
    "plt.ylabel(\"Learning Rate\", color='b')  # Label left y-axis as 'Learning Rate' in blue.\n",
    "plt.tick_params('y', colors='b')  # Set tick color of left y-axis to blue.\n",
    "plt.gca().set_xlim(0, n_epochs - 1)  # Set x-axis limits from 0 to (n_epochs - 1).\n",
    "plt.grid(True)  # Show grid lines.\n",
    "\n",
    "ax2 = plt.gca().twinx()  # Create a twin Axes sharing the same x-axis.\n",
    "ax2.plot(history.epoch, history.history[\"val_loss\"], \"r^-\")  # Plot validation loss over epochs with red triangles and solid line.\n",
    "ax2.set_ylabel('Validation Loss', color='r')  # Label right y-axis as 'Validation Loss' in red.\n",
    "ax2.tick_params('y', colors='r')  # Set tick color of right y-axis to red.\n",
    "\n",
    "plt.title(\"Reduce LR on Plateau\", fontsize=14)  # Set plot title as \"Reduce LR on Plateau\" with font size 14.\n",
    "plt.show()  # Display the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5. One Cycle scheduling\n",
    "*  The 1cycle learning rate schedule, introduced by Leslie Smith in 2018, differs from other approaches by initially increasing the learning rate linearly up to a maximum value halfway through training, then decreasing it linearly back to the initial value for the second half. In the final epochs, the learning rate is decreased significantly, still following a linear pattern. \n",
    "* The maximum learning rate is determined using the same method as finding the optimal learning rate, while the initial learning rate is set to be approximately 10 times lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for OneCycleScheduler callback.\n",
    "\n",
    "# Importing essential package\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "K = keras.backend\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Initializes the OneCycleScheduler callback.\n",
    "    Parameters:\n",
    "    iterations (int): Total number of iterations.\n",
    "    max_rate (float): Maximum learning rate.\n",
    "    start_rate (float, optional): Initial learning rate. Defaults to max_rate / 10.\n",
    "    last_iterations (int, optional): Number of iterations for the last phase. Defaults to iterations // 10 + 1.\n",
    "    last_rate (float, optional): Final learning rate. Defaults to start_rate / 1000.\n",
    "    \"\"\"    \n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    \"\"\"\n",
    "    Interpolates the learning rate linearly between two points.\n",
    "    Parameters:\n",
    "    iter1 (int): Start iteration.\n",
    "    iter2 (int): End iteration.\n",
    "    rate1 (float): Learning rate at iter1.\n",
    "    rate2 (float): Learning rate at iter2.\n",
    "\n",
    "    Returns:\n",
    "    float: Interpolated learning rate.\n",
    "    \"\"\"\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    \"\"\"\n",
    "    Callback called at the beginning of each batch.\n",
    "    Adjusts the learning rate based on the current iteration.\n",
    "    \"\"\"\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/430 [==============================] - 1s 2ms/step - loss: nan - accuracy: 0.3859\n",
      "Epoch 1/25\n",
      "  1/430 [..............................] - ETA: 0s - loss: 2.6624 - accuracy: 0.1406WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0011s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0008s). Check your callbacks.\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.6572 - accuracy: 0.7740 - val_loss: 0.4872 - val_accuracy: 0.8336\n",
      "Epoch 2/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.4581 - accuracy: 0.8395 - val_loss: 0.4275 - val_accuracy: 0.8520\n",
      "Epoch 3/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.4122 - accuracy: 0.8546 - val_loss: 0.4116 - val_accuracy: 0.8586\n",
      "Epoch 4/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3837 - accuracy: 0.8640 - val_loss: 0.3869 - val_accuracy: 0.8686\n",
      "Epoch 5/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3640 - accuracy: 0.8716 - val_loss: 0.3764 - val_accuracy: 0.8686\n",
      "Epoch 6/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3457 - accuracy: 0.8775 - val_loss: 0.3746 - val_accuracy: 0.8708\n",
      "Epoch 7/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3330 - accuracy: 0.8809 - val_loss: 0.3632 - val_accuracy: 0.8710\n",
      "Epoch 8/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3184 - accuracy: 0.8861 - val_loss: 0.3953 - val_accuracy: 0.8624\n",
      "Epoch 9/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3065 - accuracy: 0.8891 - val_loss: 0.3486 - val_accuracy: 0.8770\n",
      "Epoch 10/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2944 - accuracy: 0.8923 - val_loss: 0.3396 - val_accuracy: 0.8804\n",
      "Epoch 11/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2838 - accuracy: 0.8959 - val_loss: 0.3453 - val_accuracy: 0.8818\n",
      "Epoch 12/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2709 - accuracy: 0.9027 - val_loss: 0.3652 - val_accuracy: 0.8690\n",
      "Epoch 13/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2537 - accuracy: 0.9082 - val_loss: 0.3354 - val_accuracy: 0.8834\n",
      "Epoch 14/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2404 - accuracy: 0.9133 - val_loss: 0.3469 - val_accuracy: 0.8796\n",
      "Epoch 15/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2280 - accuracy: 0.9184 - val_loss: 0.3262 - val_accuracy: 0.8840\n",
      "Epoch 16/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2159 - accuracy: 0.9235 - val_loss: 0.3297 - val_accuracy: 0.8832\n",
      "Epoch 17/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2063 - accuracy: 0.9262 - val_loss: 0.3352 - val_accuracy: 0.8872\n",
      "Epoch 18/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1978 - accuracy: 0.9303 - val_loss: 0.3242 - val_accuracy: 0.8908\n",
      "Epoch 19/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1892 - accuracy: 0.9340 - val_loss: 0.3229 - val_accuracy: 0.8924\n",
      "Epoch 20/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1821 - accuracy: 0.9370 - val_loss: 0.3231 - val_accuracy: 0.8926\n",
      "Epoch 21/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1752 - accuracy: 0.9402 - val_loss: 0.3227 - val_accuracy: 0.8922\n",
      "Epoch 22/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1700 - accuracy: 0.9417 - val_loss: 0.3188 - val_accuracy: 0.8950\n",
      "Epoch 23/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1655 - accuracy: 0.9441 - val_loss: 0.3193 - val_accuracy: 0.8940\n",
      "Epoch 24/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1626 - accuracy: 0.9458 - val_loss: 0.3183 - val_accuracy: 0.8936\n",
      "Epoch 25/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1609 - accuracy: 0.9463 - val_loss: 0.3176 - val_accuracy: 0.8934\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using OneCycle Scheduler\n",
    "\n",
    "# Importing essential package\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "# print(tf.__version__)\n",
    "\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "# Calculating mean and standard deviation of training pixel values\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "# Standardizing the input features using mean and standard deviation\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# Creating ExponentialLearningRate callback\n",
    "K = keras.backend  # Importing Keras backend for tensor operations\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        \"\"\"\n",
    "        Initializes the ExponentialLearningRate callback.\n",
    "        Parameters:\n",
    "        - factor: The factor by which the learning rate will be adjusted after each batch.\n",
    "        \"\"\"\n",
    "        self.factor = factor\n",
    "        self.rates = []  # List to store learning rates\n",
    "        self.losses = []  # List to store losses\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        \"\"\"\n",
    "        Callback function called at the end of each batch.\n",
    "        Parameters:\n",
    "        - batch: Index of the current batch.\n",
    "        - logs: Dictionary containing the training loss for the current batch.\n",
    "        \"\"\"\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))  # Store the current learning rate\n",
    "        self.losses.append(logs[\"loss\"])  # Store the training loss\n",
    "        # Update the learning rate for the next batch\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "# Defining find_learning_rate function to find the optimal learning rate using exponential learning rate search.\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    \"\"\"\n",
    "    Find the optimal learning rate using exponential learning rate search.\n",
    "    Parameters:\n",
    "    - model: The Keras model to find the learning rate for.\n",
    "    - X: The input data.\n",
    "    - y: The target labels.\n",
    "    - epochs: Number of epochs for training.\n",
    "    - batch_size: Batch size for training.\n",
    "    - min_rate: Minimum learning rate to search from.\n",
    "    - max_rate: Maximum learning rate to search to.\n",
    "    Returns:\n",
    "    - rates: List of learning rates tested during the search.\n",
    "    - losses: Corresponding training losses.\n",
    "    \"\"\"\n",
    "    init_weights = model.get_weights()  # Initial model weights\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs  # Total number of iterations\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)  # Calculate the factor for adjusting the learning rate\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)  # Initial learning rate\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)  # Set the initial learning rate\n",
    "    exp_lr = ExponentialLearningRate(factor)  # Initialize ExponentialLearningRate callback\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])  # Train the model with the exponential learning rate\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)  # Reset the learning rate to its initial value\n",
    "    model.set_weights(init_weights)  # Reset the model weights\n",
    "    return exp_lr.rates, exp_lr.losses  # Return the learning rates and losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    \"\"\"\n",
    "    Plot the learning rate against the training loss.\n",
    "\n",
    "    Parameters:\n",
    "    - rates: List of learning rates.\n",
    "    - losses: Corresponding training losses.\n",
    "    \"\"\"\n",
    "    plt.plot(rates, losses)  # Plot learning rates against losses\n",
    "    plt.gca().set_xscale('log')  # Set x-axis to logarithmic scale for better visualization\n",
    "    plt.hlines(min(losses), min(rates), max(rates))  # Add horizontal line at the minimum loss\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])  # Set axis limits\n",
    "    plt.xlabel(\"Learning rate\")  # Set x-axis label\n",
    "    plt.ylabel(\"Loss\")  # Set y-axis label\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model with SGD optimizer and fixed learning rate\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "batch_size = 128  # Batch size for training\n",
    "\n",
    "# Find optimal learning rate using exponential learning rate search\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "\n",
    "# Plot learning rate vs. loss\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "\n",
    "\n",
    "# Define the number of epochs for training\n",
    "n_epochs = 25\n",
    "\n",
    "# Define a class for OneCycleScheduler callback.\n",
    "K = keras.backend\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Initializes the OneCycleScheduler callback.\n",
    "    Parameters:\n",
    "    iterations (int): Total number of iterations.\n",
    "    max_rate (float): Maximum learning rate.\n",
    "    start_rate (float, optional): Initial learning rate. Defaults to max_rate / 10.\n",
    "    last_iterations (int, optional): Number of iterations for the last phase. Defaults to iterations // 10 + 1.\n",
    "    last_rate (float, optional): Final learning rate. Defaults to start_rate / 1000.\n",
    "    \"\"\"    \n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    \"\"\"\n",
    "    Interpolates the learning rate linearly between two points.\n",
    "    Parameters:\n",
    "    iter1 (int): Start iteration.\n",
    "    iter2 (int): End iteration.\n",
    "    rate1 (float): Learning rate at iter1.\n",
    "    rate2 (float): Learning rate at iter2.\n",
    "\n",
    "    Returns:\n",
    "    float: Interpolated learning rate.\n",
    "    \"\"\"\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    \"\"\"\n",
    "    Callback called at the beginning of each batch.\n",
    "    Adjusts the learning rate based on the current iteration.\n",
    "    \"\"\"\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)\n",
    "\n",
    "# Create a OneCycleScheduler instance to adjust learning rate during training\n",
    "# Total number of iterations is calculated based on the length of training data\n",
    "# multiplied by the number of epochs, ensuring each sample is seen once per epoch\n",
    "# max_rate defines the maximum learning rate to be used\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs, max_rate=0.05)\n",
    "\n",
    "# Train the model using fit method\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Avoiding Overfitting Through Regularization`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. $\\ell_1$ and $\\ell_2$ regularization\n",
    "* Both $\\ell_2$ and $\\ell_1$ regularization techniques aim to prevent overfitting by penalizing large weight values, they achieve this in slightly different ways. \n",
    "* $\\ell_2$ regularization encourages smaller but non-zero weights, whereas $\\ell_1$ regularization promotes sparsity by driving many weights to zero. \n",
    "* The choice between these regularization techniques depends on the specific requirements of the problem at hand, such as the desired model complexity and interpretability. Additionally, as mentioned earlier, a combination of both techniques, known as `elastic net regularization`, can be employed to leverage their respective benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. $\\ell_2$ Regularization (Ridge Regression):\n",
    "\n",
    "* $\\ell_2$ regularization adds a penalty term to the loss function proportional to the squared magnitude of the weights.\n",
    "* The regularization term is λ * ||w||₂², where λ is the regularization strength and ||w||₂ is the Euclidean norm of the weight vector.\n",
    "* The effect of $\\ell_2$ regularization is to push the weights towards smaller values but not necessarily to zero. It penalizes large weights more heavily, preventing them from dominating the optimization process.\n",
    "* In neural networks, applying $\\ell_2$ regularization encourages the network to generalize better to unseen data by preventing overfitting. It effectively acts as a form of smoothness constraint on the learned function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. $\\ell_1$ Regularization (Lasso Regression):\n",
    "\n",
    "* $\\ell_1$ regularization adds a penalty term to the loss function proportional to the absolute magnitude of the weights.\n",
    "* The regularization term is λ * ||w||₁, where λ is the regularization strength and ||w||₁ is the $\\ell_1$ norm of the weight vector.\n",
    "* The key characteristic of $\\ell_1$ regularization is its ability to induce sparsity. It tends to push many weights to exactly zero, effectively performing feature selection.\n",
    "* In the context of neural networks, $\\ell_1$ regularization can be useful for reducing the model's complexity by eliminating irrelevant features or connections. It leads to more interpretable models and can improve generalization by simplifying the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "# or l1(0.1) for ℓ1 regularization with a factor of 0.1\n",
    "# or l1_l2(0.1, 0.01) for both ℓ1 and ℓ2 regularization, with factors 0.1 and 0.01 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 5s 2ms/step - loss: 1.6119 - accuracy: 0.8110 - val_loss: 0.7176 - val_accuracy: 0.8316\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7152 - accuracy: 0.8292 - val_loss: 0.6805 - val_accuracy: 0.8392\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using ℓ2 Regularization\n",
    "\n",
    "# Importing essential package\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "# Calculating mean and standard deviation of training pixel values\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "# Standardizing the input features using mean and standard deviation\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Creating a Sequential model\n",
    "model = keras.models.Sequential([\n",
    "    # Input layer: Flatten layer to convert 2D input (28x28) to 1D array\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # Hidden layer 1: Dense layer with 300 neurons, ELU activation function,\n",
    "    # He initialization, and ℓ2 regularization with a regularization strength of 0.01\n",
    "    keras.layers.Dense(300, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    # Hidden layer 2: Dense layer with 100 neurons, ELU activation function,\n",
    "    # He initialization, and ℓ2 regularization with a regularization strength of 0.01\n",
    "    keras.layers.Dense(100, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    # Output layer: Dense layer with 10 neurons (for 10 classes), softmax activation function,\n",
    "    # and ℓ2 regularization with a regularization strength of 0.01\n",
    "    keras.layers.Dense(10, activation=\"softmax\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Defining number of epochs\n",
    "n_epochs = 2\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creatign a partial function\n",
    "* To adjust the default configurations for layers (e.g., changing the activation function or regularization strength), you can do so in one central location rather than modifying each layer individually throughout your codebase. \n",
    "* This simplifies the process of making global changes to the neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'partial' function from the functools module\n",
    "from functools import partial\n",
    "\n",
    "# Create a partial function called RegularizedDense using 'partial'\n",
    "# This partial function acts as a wrapper for keras.layers.Dense with default arguments\n",
    "# Default arguments include activation function, kernel initializer, and kernel regularizer\n",
    "RegularizedDense = partial(\n",
    "    keras.layers.Dense,\n",
    "    activation=\"elu\",\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    kernel_regularizer=keras.regularizers.l2(0.01)\n",
    ")\n",
    "\n",
    "# Define the neural network model using the Sequential API\n",
    "model = keras.models.Sequential([\n",
    "    # Input layer: Flatten the input images to a 1D array\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # Hidden layers: Dense layers with default configurations defined by RegularizedDense\n",
    "    RegularizedDense(300),  # First hidden layer with 300 units\n",
    "    RegularizedDense(100),  # Second hidden layer with 100 units\n",
    "    # Output layer: Dense layer with 10 units and softmax activation for multi-class classification\n",
    "    RegularizedDense(10, activation=\"softmax\")  # Output layer with 10 units for 10 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 15s 7ms/step - loss: 1.6389 - accuracy: 0.8116 - val_loss: 0.7182 - val_accuracy: 0.8324\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.7185 - accuracy: 0.8280 - val_loss: 0.6828 - val_accuracy: 0.8382\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using 'partial' function wrapper\n",
    "\n",
    "# Importing essential package\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "# Calculating mean and standard deviation of training pixel values\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "# Standardizing the input features using mean and standard deviation\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import the 'partial' function from the functools module\n",
    "from functools import partial\n",
    "\n",
    "# Create a partial function called RegularizedDense using 'partial'\n",
    "# This partial function acts as a wrapper for keras.layers.Dense with default arguments\n",
    "# Default arguments include activation function, kernel initializer, and kernel regularizer\n",
    "RegularizedDense = partial(\n",
    "    keras.layers.Dense,\n",
    "    activation=\"elu\",\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    kernel_regularizer=keras.regularizers.l2(0.01)\n",
    ")\n",
    "\n",
    "# Define the neural network model using the Sequential API\n",
    "model = keras.models.Sequential([\n",
    "    # Input layer: Flatten the input images to a 1D array\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # Hidden layers: Dense layers with default configurations defined by RegularizedDense\n",
    "    RegularizedDense(300),  # First hidden layer with 300 units\n",
    "    RegularizedDense(100),  # Second hidden layer with 100 units\n",
    "    # Output layer: Dense layer with 10 units and softmax activation for multi-class classification\n",
    "    RegularizedDense(10, activation=\"softmax\")  # Output layer with 10 units for 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model with specified loss function, optimizer, and evaluation metric\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model on the training data for a specified number of epochs\n",
    "# Use validation data for evaluating model performance during training\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dropout\n",
    "\n",
    "* During each training iteration, dropout randomly removes a proportion of neurons from the neural network with a certain probability, often denoted as p. \n",
    "\n",
    "* The dropout rate (p) is a hyperparameter that controls the probability of dropping out a neuron during each training iteration.\n",
    "\n",
    "* Dropout prevents the co-adaptation of neurons. When neurons are randomly dropped out during training, other neurons must compensate for their absence to make accurate predictions. This prevents neurons from relying too heavily on the presence of specific other neurons for making predictions, which encourages the network to learn more robust and generalizable features.\n",
    "\n",
    "* Co-adaptation\" in the context of neural networks refers to a phenomenon where neurons in the network become overly specialized and reliant on each other to make predictions. \n",
    "\n",
    "\n",
    "**Compensating for Dropout Regularization: Adjusting Weights During Training**\n",
    "\n",
    "\n",
    "* This adjustment ensures that the network operates consistently during both training and testing phases.\n",
    "\n",
    "* **Compensation for Dropout during Testing:** During training, dropout randomly drops out neurons with a certain probability p. However, during testing, all neurons are active to make predictions. If p=0.5 (i.e., 50% dropout rate), on average, each neuron would be connected to only half the number of input neurons during training compared to testing.\n",
    "\n",
    "* **Weight Adjustment:** To compensate for the difference in connectivity between training and testing, the weights of the connections in the neural network need to be adjusted. Specifically, each **input connection weight** needs to be **multiplied** by the **\"keep probability,\"** which is **1−p**, after training.\n",
    "\n",
    "* **Maintaining Consistency:** Without this adjustment, each neuron would receive a total input signal that is roughly twice as large during testing compared to what the network was trained on. This inconsistency could lead to poor performance during testing, as the network might not generalize well to unseen data.\n",
    "\n",
    "* **Alternative Approach:** Instead of adjusting the weights after training, an alternative approach is to **divide** each **neuron's output** by the **keep probability** during training. While not perfectly equivalent, this approach achieves the same goal of maintaining consistency between training and testing phases.\n",
    "\n",
    "* Overall, this adjustment ensures that the network operates consistently and effectively during both training and testing, despite the dropout regularization applied during training. It helps maintain the performance of the network on unseen data and improves its generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 7s 3ms/step - loss: 0.5699 - accuracy: 0.8038 - val_loss: 0.3663 - val_accuracy: 0.8650\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4227 - accuracy: 0.8446 - val_loss: 0.3415 - val_accuracy: 0.8714\n"
     ]
    }
   ],
   "source": [
    "# Training a neural network on Fashion MNIST using Dropout regularization\n",
    "\n",
    "# Importing essential package\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Loading Fashion MNIST dataset and preprocessing\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# Scaling pixel values to the range [0, 1]\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Splitting training set into validation and training subsets\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "# Calculating mean and standard deviation of training pixel values\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "# Standardizing the input features using mean and standard deviation\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the model architecture with dropout regularization\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),  # Apply dropout to the input layer\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),  # Apply dropout to the first hidden layer\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),  # Apply dropout to the second hidden layer\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Monte Carlo (MC) Dropout\n",
    "* MC Dropout stands for Monte Carlo Dropout, which is an extension of the Dropout regularization technique commonly used in deep learning.\n",
    "\n",
    "* In traditional Dropout, during training, random neurons are temporarily \"dropped out\" of the network by setting their outputs to zero with a certain probability (typically 0.5). This helps prevent overfitting by introducing noise and redundancy, forcing the network to learn more robust features.\n",
    "\n",
    "* Monte Carlo Dropout extends this idea to the inference phase. Instead of just using Dropout during training, it's also used during inference (i.e., when making predictions). This means that when making predictions, instead of getting a single prediction for each instance, you get multiple predictions with different dropout masks applied. These predictions capture the uncertainty in the model's predictions.\n",
    "\n",
    "* MCDropout is a method where dropout is enabled during prediction, but it's not used for regularization purposes like during training. Instead, it's used to estimate uncertainty in the predictions made by the model. By performing multiple forward passes with dropout enabled and averaging the predictions, MCDropout provides a measure of uncertainty or confidence in the model's predictions.\n",
    "\n",
    "* By repeating this process multiple times (Monte Carlo sampling), you can get a distribution of predictions for each instance. You can then analyze this distribution to estimate uncertainty, such as by calculating the mean or variance of the predictions.\n",
    "\n",
    "* Monte Carlo Dropout is particularly useful for estimating uncertainty in deep learning models, which is important in various applications such as medical diagnosis, autonomous vehicles, and finance, where understanding the model's confidence in its predictions is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "Prediction of first sample input      [[0.   0.   0.   0.   0.   0.01 0.   0.08 0.   0.91]]\n",
      "Round of the predicted probabilities  [[0.   0.   0.   0.   0.   0.06 0.   0.17 0.   0.77]]\n",
      "Standard deviation of the predictions [[0.   0.   0.   0.   0.   0.08 0.   0.16 0.   0.19]]\n",
      "The predicted class labels are =  [9 2 1 ... 8 1 5]\n",
      "The accuracy of the prediction is 85.95 percent\n"
     ]
    }
   ],
   "source": [
    "# # Perform Monte Carlo Dropout by making 100 predictions with dropout activated \n",
    "# # for each instance in the test set. The `model` function is called with \n",
    "# # `training=True` to ensure that Dropout layers are active during prediction.\n",
    "# # This results in a list of 100 sets of predictions, where each set represents \n",
    "# # the model's output for the test set with slight variations due to dropout randomness.\n",
    "y_probas = np.stack([model(X_test_scaled, training=True) for sample in range(100)])\n",
    "\n",
    "# # Calculate the mean prediction across the 100 iterations along the first axis,\n",
    "# # resulting in a single prediction for each instance in the test set.\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "\n",
    "# # Calculate the standard deviation of the predictions across the 100 iterations\n",
    "# # along the first axis, resulting in a measure of uncertainty associated with each prediction.\n",
    "y_std = y_probas.std(axis=0)\n",
    "\n",
    "# Predict the output for the first sample in the scaled test dataset and round the result to two decimal places\n",
    "print('Prediction of first sample input     ',np.round(model.predict(X_test_scaled[:1,:,:]), 2))\n",
    "\n",
    "# Round the predicted probabilities for the first sample to two decimal places\n",
    "print('Round of the predicted probabilities ',np.round(y_proba[:1,:], 2))\n",
    "\n",
    "# Round the standard deviation of the predictions for the first sample to two decimal places\n",
    "print('Standard deviation of the predictions',np.round(y_std[:1], 2))\n",
    "\n",
    "# Determine the predicted class labels by selecting the index of the highest probability from the predicted probabilities\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "print('The predicted class labels are = ', y_pred)\n",
    "\n",
    "# Calculate the accuracy by comparing the predicted class labels with the true class labels and dividing by the total number of samples\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "print('The accuracy of the prediction is %2.2f percent'%(100*accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1. Consistent Layer Behavior: Training and Prediction Alignment\n",
    "\n",
    "* Ensuring consistent behavior for layers like BatchNormalization during both training and prediction phases when using dropout during training and MCDropout during prediction is very essential. This consistency is vital to ensure the model's predictions are accurate and reliable across different phases.\n",
    "\n",
    "* *Layers with Different Behavior*: Layers like BatchNormalization behave differently during training and prediction. During training, BatchNormalization layers utilize batch statistics for normalization, whereas during prediction, they use learned population statistics (mean and variance) for normalization.\n",
    "\n",
    "* *Consistency in Behavior:* To ensure consistent behavior for layers like BatchNormalization during both training and prediction, it's essential to maintain dropout during training and use dropout-like behavior during prediction, such as MCDropout.\n",
    "\n",
    "* **Custom MCDropout Class:** The custom MCDropout class is introduced to ensure consistent behavior of dropout during both training and prediction phases. This class overrides the call method to enforce dropout-like behavior during prediction, similar to what dropout does during training.\n",
    "\n",
    "* **Replacement of Dropout Layers:** In models containing layers like BatchNormalization, replacing traditional Dropout layers with MCDropout layers ensures dropout-like behavior is maintained during both training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " mc_dropout (MCDropout)      (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " mc_dropout_1 (MCDropout)    (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " mc_dropout_2 (MCDropout)    (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.16, 0.  , 0.78]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a class for class MCDropout.\n",
    "class MCDropout(keras.layers.Dropout):\n",
    "    # Override the call method to force dropout during prediction\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "# Define a class for MCAlphaDropout.\n",
    "# AlphaDropout is a dropout variant that scales activations during training\n",
    "# to maintain the mean and variance of inputs, offering improved training stability,\n",
    "#  especially in networks with ReLU activations.\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    # Override the call method to force alpha dropout during prediction\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "    \n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a new model using MCAlphaDropout or MCDropout where appropriate\n",
    "mc_model = keras.models.Sequential([\n",
    "    # Replace AlphaDropout layers with MCAlphaDropout and Dropout layers with MCDropout\n",
    "    MCDropout(layer.rate) if isinstance(layer, keras.layers.Dropout) else layer\n",
    "    for layer in model.layers\n",
    "])\n",
    "\n",
    "# Print the summary of the Monte Carlo Dropout model\n",
    "mc_model.summary()\n",
    "\n",
    "# Compile the Monte Carlo Dropout model with the same optimizer and metrics\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Set the weights of the Monte Carlo Dropout model to be the same as the original model\n",
    "mc_model.set_weights(model.get_weights())\n",
    "\n",
    "# Perform Monte Carlo dropout prediction and round the mean predictions to two decimal places\n",
    "np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Max-norm Regularization\n",
    "\n",
    "* Max-norm regularization is a popular technique in neural network regularization. It constrains the weights of each neuron's incoming connections so that their norm (magnitude) does not exceed a certain threshold, denoted as *'r'*. The norm used for this constraint is typically the $\\ell_2$ norm.\n",
    "\n",
    "* Unlike some other regularization techniques, max-norm regularization does not directly add a regularization loss term to the overall loss function. Instead, it is implemented by calculating the norm of the weights after each training step. If the norm exceeds the threshold 'r', the weights are rescaled to satisfy the constraint.\n",
    "\n",
    "* Adjusting the hyperparameter 'r' allows for controlling the amount of regularization applied. Increasing 'r' reduces the regularization effect, while decreasing it enhances regularization, helping to mitigate overfitting.\n",
    "\n",
    "* Max-norm regularization also serves to address unstable gradients, a common issue in training deep neural networks. When not using techniques like Batch Normalization, max-norm regularization can provide stability by limiting the magnitude of gradients during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
