{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Implementing PCA using Numpy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.26203346, -0.42067648],\n",
       "       [ 0.08001485,  0.35272239],\n",
       "       [-1.17545763, -0.36085729],\n",
       "       [-0.89305601,  0.30862856],\n",
       "       [-0.73016287,  0.25404049],\n",
       "       [ 1.10436914, -0.20204953],\n",
       "       [-1.27265808, -0.46781247],\n",
       "       [ 0.44933007, -0.67736663],\n",
       "       [ 1.09356195,  0.04467792],\n",
       "       [ 0.66177325,  0.28651264],\n",
       "       [-1.04466138,  0.11244353],\n",
       "       [ 1.05932502, -0.31189109],\n",
       "       [-1.13761426, -0.14576655],\n",
       "       [-1.16044117, -0.36481599],\n",
       "       [ 1.00167625, -0.39422008],\n",
       "       [-0.2750406 ,  0.34391089],\n",
       "       [ 0.45624787, -0.69707573],\n",
       "       [ 0.79706574,  0.26870969],\n",
       "       [ 0.66924929, -0.65520024],\n",
       "       [-1.30679728, -0.37671343],\n",
       "       [ 0.6626586 ,  0.32706423],\n",
       "       [-1.25387588, -0.56043928],\n",
       "       [-1.04046987,  0.08727672],\n",
       "       [-1.26047729, -0.1571074 ],\n",
       "       [ 1.09786649, -0.38643428],\n",
       "       [ 0.7130973 , -0.64941523],\n",
       "       [-0.17786909,  0.43609071],\n",
       "       [ 1.02975735, -0.33747452],\n",
       "       [-0.94552283,  0.22833268],\n",
       "       [ 0.80994916,  0.33810729],\n",
       "       [ 0.20189175,  0.3514758 ],\n",
       "       [-1.34219411, -0.42415687],\n",
       "       [ 0.13599883,  0.37258632],\n",
       "       [ 0.8206931 , -0.55120835],\n",
       "       [ 0.90818634, -0.31869127],\n",
       "       [ 0.06703671,  0.42486148],\n",
       "       [ 0.13936893,  0.41906961],\n",
       "       [-0.37356775,  0.27320849],\n",
       "       [ 0.7312441 ,  0.23441131],\n",
       "       [-0.5230355 ,  0.46621776],\n",
       "       [ 0.86146183,  0.30212526],\n",
       "       [-0.33203239,  0.47352674],\n",
       "       [-0.99467436,  0.18342807],\n",
       "       [ 1.04520043, -0.32697207],\n",
       "       [ 0.87477048,  0.18062856],\n",
       "       [ 0.30457923,  0.43904343],\n",
       "       [-0.63685997,  0.32851826],\n",
       "       [ 1.1287259 , -0.11627335],\n",
       "       [ 0.03836205,  0.49036349],\n",
       "       [-0.41386843,  0.31734423],\n",
       "       [-1.32417938, -0.1944472 ],\n",
       "       [ 0.92968677,  0.18429606],\n",
       "       [-0.40274964,  0.34154025],\n",
       "       [ 1.11480941, -0.24138847],\n",
       "       [ 0.31915065,  0.27787663],\n",
       "       [ 1.02666316, -0.34676546],\n",
       "       [-1.24145806, -0.35049349],\n",
       "       [ 0.66770361,  0.32262317],\n",
       "       [-1.16397896,  0.03648137],\n",
       "       [ 0.68326064,  0.22756871]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing NumPy library with an alias 'np'\n",
    "import numpy as np\n",
    "\n",
    "# Setting a random seed for reproducibility\n",
    "np.random.seed(4)\n",
    "\n",
    "# Setting parameters for generating synthetic data\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "# Generating random angles and creating a 3D dataset\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)\n",
    "\n",
    "# Centering the data by subtracting the mean along each feature\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# Performing Singular Value Decomposition (SVD) on the centered data\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "\n",
    "# Extracting the principal components\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]\n",
    "\n",
    "# Getting the shape of the original data\n",
    "m, n = X.shape\n",
    "\n",
    "# Creating a diagonal matrix S from the singular values\n",
    "S = np.zeros(X_centered.shape)\n",
    "S[:n, :n] = np.diag(s)\n",
    "\n",
    "# Checking if the original data can be reconstructed using SVD\n",
    "np.allclose(X_centered, U.dot(S).dot(Vt))\n",
    "\n",
    "# Extracting the first two principal components\n",
    "W2 = Vt.T[:, :2]\n",
    "\n",
    "# Projecting the centered data onto the first two principal components\n",
    "X2D = X_centered.dot(W2)\n",
    "\n",
    "# Storing the result in X2D_using_svd\n",
    "X2D_using_svd = X2D\n",
    "\n",
    "# Displaying the resulting 2D projection\n",
    "X2D_using_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Implementing PCA using Scikit-Learn library\n",
    "Scikit-Learn’s PCA class uses SVD decomposition to implement PCA, just like we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing PCA using SVD method: \n",
      " [[ 1.26203346  0.42067648]\n",
      " [-0.08001485 -0.35272239]\n",
      " [ 1.17545763  0.36085729]\n",
      " [ 0.89305601 -0.30862856]\n",
      " [ 0.73016287 -0.25404049]]\n",
      "Implementing PCA using Scikit-learn PCA class: \n",
      " [[-1.26203346 -0.42067648]\n",
      " [ 0.08001485  0.35272239]\n",
      " [-1.17545763 -0.36085729]\n",
      " [-0.89305601  0.30862856]\n",
      " [-0.73016287  0.25404049]]\n",
      "In general the only difference is that some axes may be flipped.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)\n",
    "print('Implementing PCA using SVD method: \\n',X2D[:5])\n",
    "print('Implementing PCA using Scikit-learn PCA class: \\n',X2D_using_svd[:5])\n",
    "print('In general the only difference is that some axes may be flipped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Explained Variance Ratio\n",
    "    The ratio indicates the percentage of the dataset's variance present along each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84248607 0.14631839]\n",
      "We can see that the majority proportion of the variance is in the first PC axis.\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "print('We can see that the majority proportion of the variance is in the first PC axis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Choosing the Right Number of Dimensions\n",
    "    computing the minimum number of dimensions required to preserve 95% of the training set’s variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of principal components that explain at least 95% of the variance is 2\n"
     ]
    }
   ],
   "source": [
    "# Importing PCA from scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA() # Initializing a PCA object without specifying the number of components\n",
    "pca.fit(X) # Fitting the PCA model to the data\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) # Calculating the cumulative explained variance\n",
    "# Finding the number of principal components that explain at least 95% of the variance\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "print('The number of principal components that explain at least 95% of the variance is',d)\n",
    "\n",
    "\n",
    "# Initializing a PCA object with a specified explained variance threshold (95%)\n",
    "pca = PCA(n_components=0.95)\n",
    "# Transforming the data to the reduced-dimensional space\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Reconstructing the original data\n",
    "    The inverse_transform method to reconstruct the original data (X_recovered) from the reduced-dimensional representation (X_reduced). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a PCA object with a specified explained variance threshold (95%)\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Transforming the original data (X) to the reduced-dimensional space\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# Reconstructing the original data from the reduced-dimensional representation\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Randomized PCA\n",
    "    * If the svd_solver is set to \"randomized,\" Scikit-Learn employs a stochastic algorithm called Randomized PCA.\n",
    "\n",
    "    * This results in a significantly faster computation when the number of desired principal components (d) is much smaller than the original dimensionality (n).\n",
    "\n",
    "    * By default, the svd_solver is set to \"auto\" in Scikit-Learn.\n",
    "\n",
    "    * If there is a need to force Scikit-Learn to use the full SVD approach, the svd_solver hyperparameter can be set explicitly to \"full.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=2, svd_solver=\"randomized\", random_state=42)\n",
    "X_reduced = rnd_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Incremental PCA (IPCA)\n",
    "    * Incremental PCA (IPCA) is a variant of the traditional Principal Component Analysis (PCA) that allows for incremental and memory-efficient computation of principal components.\n",
    "\n",
    "    * IPCA operates on mini-batches of data, making it memory-efficient compared to batch PCA, especially when dealing with large datasets that may not fit into memory entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 10\n",
    "inc_pca = IncrementalPCA(n_components=2)\n",
    "for X_batch in np.array_split(X, n_batches):\n",
    "    print(\".\", end=\"\") \n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_reduced = inc_pca.transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
