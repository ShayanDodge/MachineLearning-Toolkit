{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Implementing PCA using Numpy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.26203346, -0.42067648],\n",
       "       [ 0.08001485,  0.35272239],\n",
       "       [-1.17545763, -0.36085729],\n",
       "       [-0.89305601,  0.30862856],\n",
       "       [-0.73016287,  0.25404049],\n",
       "       [ 1.10436914, -0.20204953],\n",
       "       [-1.27265808, -0.46781247],\n",
       "       [ 0.44933007, -0.67736663],\n",
       "       [ 1.09356195,  0.04467792],\n",
       "       [ 0.66177325,  0.28651264],\n",
       "       [-1.04466138,  0.11244353],\n",
       "       [ 1.05932502, -0.31189109],\n",
       "       [-1.13761426, -0.14576655],\n",
       "       [-1.16044117, -0.36481599],\n",
       "       [ 1.00167625, -0.39422008],\n",
       "       [-0.2750406 ,  0.34391089],\n",
       "       [ 0.45624787, -0.69707573],\n",
       "       [ 0.79706574,  0.26870969],\n",
       "       [ 0.66924929, -0.65520024],\n",
       "       [-1.30679728, -0.37671343],\n",
       "       [ 0.6626586 ,  0.32706423],\n",
       "       [-1.25387588, -0.56043928],\n",
       "       [-1.04046987,  0.08727672],\n",
       "       [-1.26047729, -0.1571074 ],\n",
       "       [ 1.09786649, -0.38643428],\n",
       "       [ 0.7130973 , -0.64941523],\n",
       "       [-0.17786909,  0.43609071],\n",
       "       [ 1.02975735, -0.33747452],\n",
       "       [-0.94552283,  0.22833268],\n",
       "       [ 0.80994916,  0.33810729],\n",
       "       [ 0.20189175,  0.3514758 ],\n",
       "       [-1.34219411, -0.42415687],\n",
       "       [ 0.13599883,  0.37258632],\n",
       "       [ 0.8206931 , -0.55120835],\n",
       "       [ 0.90818634, -0.31869127],\n",
       "       [ 0.06703671,  0.42486148],\n",
       "       [ 0.13936893,  0.41906961],\n",
       "       [-0.37356775,  0.27320849],\n",
       "       [ 0.7312441 ,  0.23441131],\n",
       "       [-0.5230355 ,  0.46621776],\n",
       "       [ 0.86146183,  0.30212526],\n",
       "       [-0.33203239,  0.47352674],\n",
       "       [-0.99467436,  0.18342807],\n",
       "       [ 1.04520043, -0.32697207],\n",
       "       [ 0.87477048,  0.18062856],\n",
       "       [ 0.30457923,  0.43904343],\n",
       "       [-0.63685997,  0.32851826],\n",
       "       [ 1.1287259 , -0.11627335],\n",
       "       [ 0.03836205,  0.49036349],\n",
       "       [-0.41386843,  0.31734423],\n",
       "       [-1.32417938, -0.1944472 ],\n",
       "       [ 0.92968677,  0.18429606],\n",
       "       [-0.40274964,  0.34154025],\n",
       "       [ 1.11480941, -0.24138847],\n",
       "       [ 0.31915065,  0.27787663],\n",
       "       [ 1.02666316, -0.34676546],\n",
       "       [-1.24145806, -0.35049349],\n",
       "       [ 0.66770361,  0.32262317],\n",
       "       [-1.16397896,  0.03648137],\n",
       "       [ 0.68326064,  0.22756871]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing NumPy library with an alias 'np'\n",
    "import numpy as np\n",
    "\n",
    "# Setting a random seed for reproducibility\n",
    "np.random.seed(4)\n",
    "\n",
    "# Setting parameters for generating synthetic data\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "# Generating random angles and creating a 3D dataset\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)\n",
    "\n",
    "# Centering the data by subtracting the mean along each feature\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# Performing Singular Value Decomposition (SVD) on the centered data\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "\n",
    "# Extracting the principal components\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]\n",
    "\n",
    "# Getting the shape of the original data\n",
    "m, n = X.shape\n",
    "\n",
    "# Creating a diagonal matrix S from the singular values\n",
    "S = np.zeros(X_centered.shape)\n",
    "S[:n, :n] = np.diag(s)\n",
    "\n",
    "# Checking if the original data can be reconstructed using SVD\n",
    "np.allclose(X_centered, U.dot(S).dot(Vt))\n",
    "\n",
    "# Extracting the first two principal components\n",
    "W2 = Vt.T[:, :2]\n",
    "\n",
    "# Projecting the centered data onto the first two principal components\n",
    "X2D = X_centered.dot(W2)\n",
    "\n",
    "# Storing the result in X2D_using_svd\n",
    "X2D_using_svd = X2D\n",
    "\n",
    "# Displaying the resulting 2D projection\n",
    "X2D_using_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Implementing PCA using Scikit-Learn library\n",
    "    * Scikit-Learn’s PCA class uses SVD decomposition to implement PCA, just like we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing PCA using SVD method: \n",
      " [[ 1.26203346  0.42067648]\n",
      " [-0.08001485 -0.35272239]\n",
      " [ 1.17545763  0.36085729]\n",
      " [ 0.89305601 -0.30862856]\n",
      " [ 0.73016287 -0.25404049]]\n",
      "Implementing PCA using Scikit-learn PCA class: \n",
      " [[-1.26203346 -0.42067648]\n",
      " [ 0.08001485  0.35272239]\n",
      " [-1.17545763 -0.36085729]\n",
      " [-0.89305601  0.30862856]\n",
      " [-0.73016287  0.25404049]]\n",
      "In general the only difference is that some axes may be flipped.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)\n",
    "print('Implementing PCA using SVD method: \\n',X2D[:5])\n",
    "print('Implementing PCA using Scikit-learn PCA class: \\n',X2D_using_svd[:5])\n",
    "print('In general the only difference is that some axes may be flipped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Explained Variance Ratio\n",
    "    The ratio indicates the percentage of the dataset's variance present along each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84248607 0.14631839]\n",
      "We can see that the majority proportion of the variance is in the first PC axis.\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "print('We can see that the majority proportion of the variance is in the first PC axis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Choosing the Right Number of Dimensions\n",
    "    computing the minimum number of dimensions required to preserve 95% of the training set’s variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of principal components that explain at least 95% of the variance is 2\n"
     ]
    }
   ],
   "source": [
    "# Importing PCA from scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA() # Initializing a PCA object without specifying the number of components\n",
    "pca.fit(X) # Fitting the PCA model to the data\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) # Calculating the cumulative explained variance\n",
    "# Finding the number of principal components that explain at least 95% of the variance\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "print('The number of principal components that explain at least 95% of the variance is',d)\n",
    "\n",
    "\n",
    "# Initializing a PCA object with a specified explained variance threshold (95%)\n",
    "pca = PCA(n_components=0.95)\n",
    "# Transforming the data to the reduced-dimensional space\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Reconstructing the original data\n",
    "    The inverse_transform method to reconstruct the original data (X_recovered) from the reduced-dimensional representation (X_reduced). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a PCA object with a specified explained variance threshold (95%)\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Transforming the original data (X) to the reduced-dimensional space\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# Reconstructing the original data from the reduced-dimensional representation\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Randomized PCA\n",
    "    * If the svd_solver is set to \"randomized,\" Scikit-Learn employs a stochastic algorithm called Randomized PCA.\n",
    "\n",
    "    * This results in a significantly faster computation when the number of desired principal components (d) is much smaller than the original dimensionality (n).\n",
    "\n",
    "    * By default, the svd_solver is set to \"auto\" in Scikit-Learn.\n",
    "\n",
    "    * If there is a need to force Scikit-Learn to use the full SVD approach, the svd_solver hyperparameter can be set explicitly to \"full.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=2, svd_solver=\"randomized\", random_state=42)\n",
    "X_reduced = rnd_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Incremental PCA (IPCA)\n",
    "    * Incremental PCA (IPCA) is a variant of the traditional Principal Component Analysis (PCA) that allows for incremental and memory-efficient computation of principal components.\n",
    "\n",
    "    * IPCA operates on mini-batches of data, making it memory-efficient compared to batch PCA, especially when dealing with large datasets that may not fit into memory entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 10\n",
    "inc_pca = IncrementalPCA(n_components=2)\n",
    "for X_batch in np.array_split(X, n_batches):\n",
    "    print(\".\", end=\"\") \n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_reduced = inc_pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Using `memmap()`:\n",
    "    * memmap (memory-mapped file) is a feature in Python that allows you to create a memory-mapped view of a file on disk. It provides a convenient way to manipulate large datasets that do not fit into the computer's RAM (random access memory) entirely. Instead of loading the entire dataset into memory, you can map a portion of it to memory as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'my_mnist.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_mnist.data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m m, n \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m----> 4\u001b[0m X_mm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmemmap(filename, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreadonly\u001b[39m\u001b[38;5;124m\"\u001b[39m, shape\u001b[38;5;241m=\u001b[39m(m, n))\n\u001b[0;32m      6\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_batches\n\u001b[0;32m      7\u001b[0m inc_pca \u001b[38;5;241m=\u001b[39m IncrementalPCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m154\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[1;32mc:\\Users\\dodge\\anaconda3\\Lib\\site-packages\\numpy\\core\\memmap.py:229\u001b[0m, in \u001b[0;36mmemmap.__new__\u001b[1;34m(subtype, filename, dtype, mode, offset, shape, order)\u001b[0m\n\u001b[0;32m    227\u001b[0m     f_ctx \u001b[38;5;241m=\u001b[39m nullcontext(filename)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 229\u001b[0m     f_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(os_fspath(filename), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m mode)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m f_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m    232\u001b[0m     fid\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'my_mnist.data'"
     ]
    }
   ],
   "source": [
    "filename = \"my_mnist.data\"\n",
    "m, n = X.shape\n",
    "\n",
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Implementing Kernel PCA using Scikit-Learn library\n",
    "    * Kernel PCA (kPCA) is an extension of Principal Component Analysis (PCA) that employs kernel methods to perform non-linear dimensionality reduction. While traditional PCA is effective for linearly separable data, kPCA is designed to handle non-linear relationships in the data by mapping it to a higher-dimensional feature space.\n",
    "\n",
    "    * The key idea behind kPCA is to use a kernel function to implicitly map the original data into a higher-dimensional space where linear methods, such as PCA, can be applied. This allows kPCA to capture complex, non-linear patterns in the data.\n",
    "\n",
    "    * __Choose a Kernel Function__:\n",
    "\n",
    "        * The choice of the kernel function is crucial in kPCA. Commonly used kernel functions include \n",
    "            * __Polynomial kernels__\n",
    "            * __Radial Basis Function (RBF)__ kernels (also known as Gaussian kernels) $$k(x,y) = exp (-|x- y|^2/(2\\sigma^2))$$\n",
    "            * __Sigmoid kernels__$$k(x, y) = tanh(\\kappa(x • y) + \\theta))$$.\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the make_swiss_roll function from scikit-learn datasets module\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "# Generating a Swiss roll dataset with 1000 samples, noise=0.2, and setting a random state for reproducibility\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "# Importing the KernelPCA class from scikit-learn decomposition module\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Creating a KernelPCA model with a radial basis function (RBF) kernel\n",
    "# n_components=2 indicates that we want to reduce the dataset to 2 principal components\n",
    "# The kernel parameter is set to \"rbf\" for the RBF kernel\n",
    "# Gamma is a hyperparameter for the RBF kernel and controls the width of the Gaussian \"bell\"\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "\n",
    "# Transforming the original dataset (X) into the reduced-dimensional space using KernelPCA\n",
    "X_reduced = rbf_pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Selecting a Kernel and Tuning Hyperparameters using GridSearch\n",
    "    The code involves a two-step pipeline: first, it reduces dimensionality to two dimensions using kPCA (kernel Principal Component Analysis), and then applies Logistic Regression for classification. GridSearchCV is utilized to determine the optimal kernel and gamma values for kPCA, aiming to achieve the highest classification accuracy in the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "        (\"kpca\", KernelPCA(n_components=2)),\n",
    "        (\"log_reg\", LogisticRegression(solver=\"lbfgs\"))\n",
    "    ])\n",
    "\n",
    "param_grid = [{\n",
    "        \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "        \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "    }]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "y = t > 6.9\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Selecting a Kernel and Tuning Hyperparameters using lowest reconstruction error\n",
    "    Another approach is to select the kernel and hyperparameters that yield the lowest reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 32.786308795766125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an instance of KernelPCA with RBF kernel for dimensionality reduction\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
    "\n",
    "# Fit and transform the input data using RBF kernel PCA\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "\n",
    "# Reconstruct the original data from the reduced dimensions\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
    "\n",
    "# Calculate the mean squared error between the original and reconstructed data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(X, X_preimage)\n",
    "\n",
    "# Print or use the mean squared error as needed\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Implementing Locally Linear Embedding (LLE) using Scikit-Learn library\n",
    "    * Locally Linear Embedding (LLE) is indeed another powerful technique for non-linear dimensionality reduction. LLE falls under the category of manifold learning methods and aims to preserve the local relationships within the data. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the make_swiss_roll function from scikit-learn datasets module\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "# Generating Swiss roll dataset with noise\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=41)\n",
    "\n",
    "# Importing LocallyLinearEmbedding from scikit-learn manifold module\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "# Creating LocallyLinearEmbedding instance\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "\n",
    "# Transforming the data using LocallyLinearEmbedding\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
